NOTE: Redirects are currently not supported in Windows or MacOs.
/opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : ./tools/test.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 1
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:28596
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /var/folders/bl/c09z1sm91mxfb8z7mn7v4rhh0000gq/T/torchelastic_771_051r/none_4ae6t0ix
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=28596
  group_rank=0
  group_world_size=1
  local_ranks=[0]
  role_ranks=[0]
  global_ranks=[0]
  role_world_sizes=[1]
  global_world_sizes=[1]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /var/folders/bl/c09z1sm91mxfb8z7mn7v4rhh0000gq/T/torchelastic_771_051r/none_4ae6t0ix/attempt_0/0/error.json
projects.mmdet3d_plugin
======
Loading NuScenes tables for version v1.0-trainval...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 21.433 seconds.
======
Reverse indexing ...
Done reverse indexing in 6.2 seconds.
======
load checkpoint from local path: ./ckpts/uniad_base_e2e.pth
2023-07-26 00:08:37,978 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2023-07-26 00:08:37,981 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2023-07-26 00:08:37,984 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2023-07-26 00:08:37,987 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2023-07-26 00:08:37,989 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2023-07-26 00:08:37,991 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2023-07-26 00:08:37,994 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2023-07-26 00:08:37,996 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2023-07-26 00:08:37,999 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2023-07-26 00:08:38,001 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2023-07-26 00:08:38,004 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2023-07-26 00:08:38,006 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2023-07-26 00:08:38,009 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2023-07-26 00:08:38,011 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
2023-07-26 00:08:38,014 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2023-07-26 00:08:38,016 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2023-07-26 00:08:38,018 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2023-07-26 00:08:38,021 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2023-07-26 00:08:38,023 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2023-07-26 00:08:38,026 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2023-07-26 00:08:38,028 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2023-07-26 00:08:38,031 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2023-07-26 00:08:38,033 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2023-07-26 00:08:38,036 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2023-07-26 00:08:38,040 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2023-07-26 00:08:38,043 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
The model and loaded state dict do not match exactly

unexpected key in source state_dict: bbox_size_fc.weight, bbox_size_fc.bias, pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

> /Users/liangming.xu/code/UniAD/tools/test.py(230)main()
-> result = model(return_loss=False, rescale=True, **data)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(723)simple_test_track()
-> bs = img.size(0)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(658)_forward_single_frame_inference()
-> bev_embed, bev_pos = self.get_bevs(img, img_metas, prev_bev=prev_bev)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(659)_forward_single_frame_inference()
-> det_output = self.pts_bbox_head.get_detections(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(660)_forward_single_frame_inference()
-> bev_embed,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(661)_forward_single_frame_inference()
-> object_query_embeds=track_instances.query,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(662)_forward_single_frame_inference()
-> ref_points=track_instances.ref_pts,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(663)_forward_single_frame_inference()
-> img_metas=img_metas,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(659)_forward_single_frame_inference()
-> det_output = self.pts_bbox_head.get_detections(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(665)_forward_single_frame_inference()
-> output_classes = det_output["all_cls_scores"]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(666)_forward_single_frame_inference()
-> output_coords = det_output["all_bbox_preds"]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(667)_forward_single_frame_inference()
-> last_ref_pts = det_output["last_ref_points"]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(668)_forward_single_frame_inference()
-> query_feats = det_output["query_feats"]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(671)_forward_single_frame_inference()
-> "pred_logits": output_classes,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(672)_forward_single_frame_inference()
-> "pred_boxes": output_coords,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(673)_forward_single_frame_inference()
-> "ref_pts": last_ref_pts,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(674)_forward_single_frame_inference()
-> "bev_embed": bev_embed,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(675)_forward_single_frame_inference()
-> "query_embeddings": query_feats,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(676)_forward_single_frame_inference()
-> "all_past_traj_preds": det_output["all_past_traj_preds"],
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(677)_forward_single_frame_inference()
-> "bev_pos": bev_pos,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(670)_forward_single_frame_inference()
-> out = {
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(681)_forward_single_frame_inference()
-> track_scores = output_classes[-1, 0, :].sigmoid().max(dim=-1).values
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(683)_forward_single_frame_inference()
-> track_instances.scores = track_scores
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(685)_forward_single_frame_inference()
-> track_instances.pred_logits = output_classes[-1, 0]  # [300, num_cls]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(686)_forward_single_frame_inference()
-> track_instances.pred_boxes = output_coords[-1, 0]  # [300, box_dim]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(687)_forward_single_frame_inference()
-> track_instances.output_embedding = query_feats[-1][0]  # [300, feat_dim]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(688)_forward_single_frame_inference()
-> track_instances.ref_pts = last_ref_pts[0]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(690)_forward_single_frame_inference()
-> track_instances.obj_idxes[900] = -2
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(692)_forward_single_frame_inference()
-> self.track_base.update(track_instances, None)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(694)_forward_single_frame_inference()
-> active_index = (track_instances.obj_idxes>=0) & (track_instances.scores >= self.track_base.filter_score_thresh)    # filter out sleep objects
(Pdb) 0.35
(Pdb) tensor([False, False, False, False, False, False, False, False, False, False,
        False, False, False,  True, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False,  True, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False,  True, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False,  True, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
         True])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(695)_forward_single_frame_inference()
-> out.update(self.select_active_track_query(track_instances, active_index, img_metas))
(Pdb) --Call--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(480)select_active_track_query()
-> def select_active_track_query(self, track_instances, active_index, img_metas, with_mask=True):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(481)select_active_track_query()
-> result_dict = self._track_instances2results(track_instances[active_index], img_metas, with_mask=with_mask)
(Pdb) --Call--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(120)__getitem__()
-> def __getitem__(self, item: Union[int, slice, torch.BoolTensor]) -> "Instances":
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(128)__getitem__()
-> if type(item) == int:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(134)__getitem__()
-> ret = Instances(self._image_size)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(481)select_active_track_query()
-> result_dict = self._track_instances2results(track_instances[active_index], img_metas, with_mask=with_mask)
(Pdb) tensor([False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False,  True, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False,  True, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False,  True, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False])
(Pdb) torch.Size([901])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(134)__getitem__()
-> ret = Instances(self._image_size)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) 14
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) 'scores'
(Pdb) tensor([1.2134e-03, 6.2523e-02, 3.5854e-02, 2.3008e-03, 1.1493e-03, 1.1479e-02,
        1.6889e-02, 2.7017e-02, 4.1991e-03, 3.1803e-03, 1.4656e-02, 4.7814e-03,
        2.1172e-02, 3.6873e-01, 6.2139e-03, 3.8186e-03, 1.2621e-02, 4.0460e-02,
        3.7893e-02, 3.4594e-03, 4.9484e-03, 9.8611e-04, 2.1758e-02, 5.3686e-02,
        2.1225e-02, 3.4493e-02, 2.8313e-03, 1.5702e-03, 1.4362e-02, 2.7795e-02,
        4.6897e-02, 7.2637e-03, 2.7099e-02, 1.4972e-02, 3.9342e-03, 2.4330e-02,
        9.3692e-03, 6.4523e-02, 5.1957e-02, 9.3188e-03, 6.6445e-02, 8.2580e-02,
        3.8335e-03, 6.9467e-03, 3.4724e-02, 5.2116e-03, 8.9579e-02, 6.2508e-02,
        5.2185e-02, 9.1465e-03, 2.8851e-02, 5.1593e-03, 1.0974e-01, 3.9678e-03,
        2.8626e-02, 2.6586e-02, 8.3299e-03, 5.2649e-02, 5.3267e-03, 6.4704e-03,
        3.9233e-03, 1.5086e-02, 6.7448e-02, 6.6768e-03, 1.6665e-02, 6.9540e-02,
        2.5125e-02, 3.6785e-02, 5.8670e-03, 7.5140e-03, 1.2913e-02, 3.6196e-02,
        8.6278e-03, 7.7499e-03, 6.9070e-02, 3.8398e-02, 1.3969e-02, 1.1849e-03,
        8.7286e-03, 8.0405e-03, 6.5666e-03, 1.3306e-01, 4.7247e-03, 3.9266e-02,
        5.0965e-03, 7.2163e-03, 5.7265e-02, 3.8921e-02, 1.6542e-02, 1.4544e-01,
        1.4110e-01, 8.5508e-03, 3.6722e-02, 1.1033e-03, 9.6849e-04, 1.3027e-01,
        6.2588e-04, 5.8292e-03, 1.6276e-02, 5.5910e-03, 1.1736e-02, 2.3894e-02,
        5.0752e-03, 6.0627e-02, 1.2194e-02, 4.0648e-03, 2.2797e-02, 1.6678e-02,
        3.1925e-03, 4.7256e-02, 7.4789e-02, 8.4010e-01, 5.7277e-03, 2.7151e-02,
        1.0818e-01, 4.1882e-02, 1.9956e-03, 4.3114e-02, 7.3796e-03, 1.1827e-02,
        3.1510e-02, 8.1394e-02, 5.5510e-02, 2.3008e-02, 3.7469e-03, 1.6620e-02,
        1.1137e-02, 4.6571e-03, 6.4330e-03, 6.4549e-02, 1.4618e-01, 2.0940e-02,
        2.4190e-02, 6.6831e-02, 8.4846e-04, 7.3563e-03, 4.9336e-03, 1.4138e-03,
        1.2344e-03, 6.2734e-02, 8.6931e-03, 6.2626e-03, 2.2344e-02, 5.7888e-02,
        6.1706e-02, 2.5208e-02, 9.7907e-04, 2.8844e-03, 9.5125e-03, 1.2403e-02,
        1.1536e-02, 1.6938e-02, 3.4666e-02, 5.2722e-03, 2.4653e-02, 4.4650e-03,
        7.2293e-03, 1.3935e-02, 6.5360e-02, 1.1119e-03, 2.4714e-03, 1.4973e-02,
        2.6434e-03, 2.1652e-03, 1.2158e-02, 6.7462e-03, 1.9515e-02, 2.4495e-02,
        3.6428e-02, 2.1267e-02, 3.1087e-02, 1.9324e-02, 4.9264e-03, 1.2190e-02,
        3.7031e-02, 1.6408e-03, 1.8139e-02, 1.3024e-03, 1.2271e-03, 1.2923e-02,
        7.4022e-03, 5.7929e-03, 1.0363e-02, 1.2676e-03, 1.0056e-02, 1.1427e-02,
        7.5995e-02, 3.9850e-03, 6.6336e-03, 1.4071e-03, 6.7100e-03, 5.1421e-02,
        1.3440e-03, 6.4037e-02, 1.2116e-02, 4.6018e-02, 4.9342e-02, 5.1683e-03,
        1.9097e-02, 1.5111e-02, 3.0793e-02, 7.4207e-03, 2.6880e-02, 3.5225e-03,
        1.1567e-02, 1.4508e-03, 1.6973e-02, 6.1134e-03, 4.1442e-02, 1.0759e-02,
        6.6415e-02, 4.8017e-02, 6.6356e-03, 1.5206e-03, 1.2363e-03, 4.3600e-02,
        3.9149e-02, 1.9085e-02, 1.5656e-02, 2.7196e-02, 1.1147e-03, 1.6049e-01,
        5.6968e-02, 5.2893e-03, 4.0854e-02, 1.6607e-01, 3.7793e-03, 9.2531e-03,
        5.4123e-02, 1.1974e-02, 1.3448e-03, 2.4931e-02, 1.3381e-02, 1.2397e-02,
        3.0968e-02, 3.7141e-02, 4.5333e-02, 1.1147e-02, 2.6424e-02, 4.2933e-02,
        5.2535e-03, 1.1537e-02, 6.0293e-03, 3.8656e-02, 1.0428e-02, 6.7816e-03,
        5.7107e-02, 1.0390e-01, 3.6373e-03, 4.7635e-03, 6.8706e-03, 2.4473e-02,
        5.5701e-03, 9.1752e-04, 3.1969e-03, 4.2385e-03, 4.7782e-03, 1.3262e-03,
        2.7159e-02, 1.6284e-02, 1.0323e-02, 4.7396e-02, 4.2747e-02, 3.8807e-02,
        7.0219e-03, 4.5195e-02, 2.4153e-02, 5.1583e-02, 7.8921e-03, 1.0167e-02,
        1.5030e-02, 2.6144e-02, 6.5438e-03, 4.0822e-02, 1.0016e-03, 9.4240e-03,
        3.8219e-02, 8.1689e-03, 5.1255e-02, 9.5637e-03, 1.3022e-03, 2.5308e-03,
        1.0544e-02, 2.3149e-02, 8.2537e-03, 1.2807e-02, 1.0644e-02, 5.5866e-02,
        8.2002e-03, 9.8223e-02, 1.1481e-03, 3.2006e-02, 4.2788e-02, 4.7632e-03,
        9.0154e-02, 8.6585e-03, 3.0511e-03, 6.2478e-03, 8.3633e-03, 5.2223e-03,
        7.2684e-03, 1.4663e-02, 4.0191e-03, 7.4031e-03, 9.1275e-03, 1.2543e-02,
        2.3229e-02, 1.3753e-03, 5.9674e-02, 3.3736e-02, 4.8074e-02, 4.1207e-02,
        9.5094e-02, 1.2115e-02, 1.6038e-02, 8.3455e-03, 3.7825e-02, 6.7053e-03,
        1.0765e-03, 1.7989e-02, 5.3714e-03, 8.1112e-03, 3.5229e-02, 2.2709e-02,
        1.4656e-01, 1.1850e-02, 2.6655e-02, 6.5880e-03, 6.8146e-03, 6.6188e-02,
        5.6035e-03, 9.2401e-03, 4.9330e-02, 1.1847e-03, 7.3345e-02, 3.6133e-02,
        8.9987e-01, 4.9836e-02, 8.0589e-02, 4.1223e-03, 6.2678e-02, 1.2009e-01,
        3.9707e-03, 1.8302e-02, 2.6263e-01, 4.8747e-02, 3.4416e-02, 1.4222e-02,
        1.6961e-03, 8.5399e-03, 5.1251e-03, 5.7892e-02, 8.2813e-03, 1.4120e-02,
        1.2153e-03, 3.0682e-02, 5.2534e-02, 3.1185e-03, 5.3804e-03, 2.1879e-02,
        1.0233e-02, 3.4845e-02, 5.0240e-02, 6.6517e-03, 5.5033e-02, 9.9240e-02,
        5.8001e-03, 3.6543e-03, 3.5133e-02, 1.5622e-02, 7.5054e-02, 6.2472e-02,
        7.9391e-03, 6.3959e-02, 2.9504e-02, 6.9829e-02, 1.1694e-02, 9.7521e-02,
        4.1910e-02, 1.6028e-02, 1.5102e-01, 7.6756e-02, 5.6412e-02, 6.7188e-03,
        5.6265e-03, 4.2931e-03, 3.5824e-03, 1.1934e-02, 2.5243e-02, 1.5601e-02,
        9.1454e-03, 9.2640e-03, 1.2290e-03, 4.8899e-03, 1.6706e-03, 5.1655e-03,
        3.5898e-03, 1.1129e-02, 4.1541e-02, 5.8084e-03, 1.7909e-02, 5.2201e-03,
        1.2108e-02, 1.8057e-03, 4.9781e-02, 3.7611e-02, 3.1539e-02, 1.0157e-02,
        1.1005e-03, 3.7674e-02, 9.4846e-04, 1.3821e-02, 1.5579e-03, 4.8643e-02,
        7.5679e-03, 4.9013e-03, 1.1422e-03, 1.2696e-01, 1.9293e-02, 1.9225e-02,
        6.2625e-03, 6.8183e-03, 6.1910e-02, 5.0084e-03, 4.9455e-02, 5.0841e-02,
        2.5227e-02, 2.1574e-02, 3.6100e-02, 2.7357e-02, 4.2781e-02, 3.4200e-02,
        7.3570e-03, 1.1011e-02, 7.6662e-02, 1.3262e-02, 3.2418e-02, 1.4014e-02,
        9.6086e-03, 1.6986e-02, 4.6768e-02, 2.1747e-03, 3.5014e-02, 2.0059e-02,
        2.3717e-02, 2.0145e-02, 2.8579e-02, 3.5022e-03, 9.9863e-04, 4.2501e-02,
        2.5523e-02, 4.8021e-03, 1.2995e-02, 4.5505e-03, 2.9458e-02, 1.3647e-02,
        2.3899e-02, 1.0175e-02, 3.2205e-02, 3.4850e-03, 1.0634e-03, 1.1630e-02,
        7.9688e-03, 1.4233e-01, 2.2008e-02, 1.6555e-03, 4.5813e-02, 1.1434e-03,
        1.1137e-02, 1.3660e-02, 2.5831e-02, 3.4862e-02, 4.6683e-03, 2.3943e-02,
        7.4350e-03, 4.2919e-02, 2.1815e-02, 1.1046e-02, 8.6327e-03, 2.1631e-02,
        9.6153e-03, 1.0895e-01, 8.7908e-03, 1.9043e-02, 3.9042e-02, 2.6306e-02,
        3.4800e-03, 4.7643e-02, 6.1349e-03, 1.0257e-02, 5.8688e-03, 1.4184e-02,
        1.6281e-02, 7.4497e-03, 9.5034e-04, 3.2814e-02, 4.1897e-02, 2.1638e-02,
        9.9387e-03, 9.2697e-03, 5.0314e-02, 1.2052e-03, 1.4283e-02, 1.4066e-02,
        5.3169e-02, 1.4760e-02, 1.2717e-02, 2.0727e-02, 2.0729e-02, 1.3156e-02,
        1.6902e-02, 6.2680e-03, 6.2336e-03, 1.5705e-02, 7.7093e-02, 1.6454e-02,
        4.2099e-03, 7.0141e-02, 1.2030e-02, 3.5844e-02, 1.3685e-02, 4.8726e-03,
        8.7846e-03, 9.5205e-03, 3.2727e-03, 1.2507e-01, 6.3954e-03, 1.8167e-02,
        3.7289e-03, 9.2284e-03, 3.8752e-03, 2.6565e-03, 2.6234e-03, 1.2883e-03,
        7.1558e-03, 2.7902e-02, 2.3973e-02, 4.2808e-02, 2.2890e-02, 1.3514e-02,
        8.4640e-03, 5.5918e-03, 7.9391e-03, 6.8766e-03, 9.4866e-03, 8.0524e-03,
        1.4649e-02, 4.7029e-03, 4.3191e-03, 2.5143e-02, 1.0017e-02, 9.3041e-04,
        2.1764e-02, 1.6214e-02, 5.6475e-03, 5.7276e-03, 1.1524e-02, 1.2925e-02,
        1.0318e-02, 2.3928e-03, 2.6898e-03, 1.0364e-01, 8.2849e-03, 4.2610e-03,
        1.4148e-01, 3.6718e-03, 1.7754e-02, 1.3446e-02, 7.2389e-03, 8.3923e-03,
        1.3773e-02, 2.9674e-02, 2.6754e-02, 9.5184e-03, 3.2540e-03, 7.8509e-04,
        4.3543e-02, 4.2098e-02, 9.8746e-04, 1.1894e-02, 4.3983e-03, 4.8268e-02,
        1.3743e-02, 6.1215e-04, 1.3224e-02, 1.2323e-02, 4.2154e-02, 2.5367e-02,
        1.6254e-02, 1.3730e-02, 6.4320e-02, 4.1057e-03, 6.7030e-03, 1.6866e-02,
        6.2101e-02, 1.6485e-02, 2.5149e-02, 4.1828e-02, 7.5231e-03, 2.4282e-03,
        3.4044e-03, 4.8306e-03, 2.4965e-02, 3.4466e-02, 1.4917e-02, 1.3371e-02,
        2.1824e-02, 1.1339e-03, 6.6276e-03, 1.2884e-02, 4.6933e-02, 4.4922e-02,
        5.5381e-03, 1.2710e-02, 4.4529e-02, 2.3726e-02, 3.8491e-02, 1.4928e-02,
        2.8750e-02, 2.4997e-03, 1.4293e-03, 1.4469e-02, 1.2889e-02, 1.2198e-02,
        3.1700e-03, 1.2895e-03, 6.8457e-02, 2.9942e-02, 1.3678e-02, 1.1382e-02,
        3.1289e-02, 1.6485e-03, 3.9063e-02, 8.6605e-01, 2.9481e-02, 8.5971e-03,
        2.4789e-01, 7.0986e-03, 5.5425e-03, 7.1797e-03, 8.5298e-02, 4.7958e-02,
        1.8968e-02, 8.1707e-03, 5.9681e-03, 2.8397e-02, 2.0292e-02, 5.9120e-03,
        1.1771e-03, 2.0855e-02, 2.0247e-02, 8.6334e-03, 7.0641e-03, 8.1918e-03,
        9.3954e-03, 8.2024e-02, 2.2804e-03, 1.1885e-01, 2.5871e-02, 5.7722e-03,
        1.5079e-02, 5.9215e-02, 3.9772e-03, 2.1827e-02, 1.2483e-02, 7.7384e-03,
        6.8899e-02, 1.0509e-02, 1.6041e-02, 1.4781e-03, 3.0236e-02, 3.8648e-02,
        2.0800e-03, 9.3104e-03, 1.4078e-02, 2.0105e-03, 6.4797e-03, 5.8823e-02,
        2.2588e-01, 1.3489e-02, 4.4152e-02, 6.9509e-02, 3.3535e-02, 8.8430e-03,
        5.5192e-03, 6.5108e-03, 6.0700e-03, 2.3320e-02, 3.8966e-02, 2.7285e-02,
        1.6611e-03, 7.4567e-03, 3.0014e-02, 1.0790e-03, 1.0670e-02, 2.3976e-02,
        7.9866e-03, 1.9947e-02, 3.6452e-02, 2.0288e-02, 8.5655e-03, 2.3905e-03,
        7.0432e-03, 4.7897e-02, 4.0366e-02, 9.3588e-03, 5.6001e-03, 5.6697e-03,
        2.7458e-02, 1.4447e-02, 5.3912e-03, 9.4531e-03, 1.7389e-02, 1.5690e-02,
        5.1908e-03, 2.7225e-02, 7.7876e-03, 6.9681e-03, 3.2093e-03, 3.8725e-02,
        4.3201e-03, 1.0264e-02, 1.1616e-02, 8.6118e-02, 6.7074e-03, 4.5637e-03,
        9.0264e-03, 7.3751e-03, 2.8856e-02, 1.7818e-02, 3.6863e-02, 1.2164e-02,
        6.8021e-03, 9.1666e-03, 3.8805e-03, 1.2022e-02, 5.3499e-03, 2.3527e-03,
        7.1392e-03, 2.2741e-03, 1.1849e-03, 1.8058e-02, 4.9544e-03, 1.0407e-02,
        6.3434e-02, 7.7950e-03, 5.3718e-02, 8.5043e-03, 1.8742e-02, 3.7175e-03,
        3.4874e-02, 1.5555e-02, 3.5836e-02, 7.2156e-03, 4.0224e-02, 1.1971e-02,
        1.5522e-01, 1.3716e-01, 2.7829e-02, 7.1587e-03, 5.9096e-03, 3.8481e-02,
        4.1408e-02, 1.1290e-01, 1.0321e-03, 8.1784e-02, 2.3073e-03, 3.0509e-02,
        2.5704e-02, 3.5071e-02, 1.5234e-03, 5.5836e-03, 3.0906e-02, 1.9596e-03,
        1.8160e-02, 1.7222e-02, 2.1589e-03, 4.1568e-03, 1.4273e-02, 7.8894e-02,
        3.0986e-03, 2.0555e-02, 7.5576e-03, 1.7354e-02, 6.7544e-03, 1.1260e-03,
        3.3693e-02, 1.6364e-03, 2.9528e-02, 2.8759e-02, 1.3075e-02, 3.5391e-03,
        7.9627e-03, 5.4334e-02, 1.2412e-02, 4.3285e-03, 9.7642e-02, 8.2015e-03,
        2.4233e-02, 2.2819e-02, 1.0958e-03, 2.8139e-03, 5.0963e-02, 3.4218e-03,
        9.4313e-03, 3.9315e-02, 4.9605e-02, 1.0425e-01, 2.3686e-03, 5.0026e-02,
        7.7612e-03, 5.4702e-03, 7.4165e-03, 1.3946e-03, 4.2023e-02, 1.6111e-02,
        2.4136e-02, 8.0160e-03, 1.0436e-03, 4.6804e-03, 2.0107e-03, 1.7419e-02,
        8.8171e-03, 6.5201e-03, 8.7014e-03, 7.4491e-02, 3.7844e-02, 5.1229e-02,
        6.2621e-02, 7.2930e-03, 6.8501e-02, 5.1532e-02, 1.8383e-02, 1.2483e-03,
        5.3616e-02, 1.3578e-02, 2.3480e-02, 2.9433e-02, 1.5497e-03, 1.9143e-02,
        4.4572e-02, 3.2648e-02, 6.3635e-03, 5.6019e-03, 7.2052e-02, 3.3516e-02,
        1.1562e-03, 1.0537e-03, 1.0457e-02, 2.8388e-03, 1.9056e-02, 6.6741e-03,
        6.4513e-03, 5.7186e-03, 1.0832e-02, 6.3372e-02, 1.3298e-01, 1.7387e-02,
        3.6845e-02, 1.3048e-03, 8.4211e-03, 4.9586e-02, 5.9509e-02, 1.6831e-02,
        4.2773e-03, 4.2043e-02, 5.1998e-03, 1.4899e-02, 8.1630e-03, 2.6650e-02,
        4.8746e-03, 6.2005e-03, 4.2179e-02, 6.4319e-03, 2.8676e-02, 6.1934e-03,
        5.1957e-03, 3.4885e-02, 3.0818e-02, 2.3451e-02, 1.1838e-02, 9.3603e-03,
        6.1641e-02, 6.0797e-03, 3.9582e-03, 5.2156e-02, 2.8057e-03, 2.6650e-02,
        1.9928e-03, 2.5077e-02, 7.7074e-04, 9.9252e-04, 1.8732e-02, 1.1438e-03,
        1.2189e-01, 5.4555e-03, 1.0569e-01, 2.4203e-03, 6.2680e-03, 1.5034e-02,
        4.0580e-01])
(Pdb) torch.Size([901])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(148)__getitem__()
-> return ret
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(148)__getitem__()->Instances(num...0., 0., 0.])])
-> return ret
(Pdb) *** AttributeError: Cannot find field 'shape' in the given Instances!
(Pdb) Instances(num_instances=3, image_height=1, image_width=1, fields=[ref_pts: tensor([[-0.1737, -0.4143, -0.4898],
        [-0.0294, -1.0461, -0.6533],
        [-0.4897,  1.7039,  0.0219]]), query: tensor([[-0.5876,  1.6050, -2.0626,  ..., -0.8195,  2.2342,  1.4749],
        [-0.5229, -0.1827, -0.5886,  ..., -0.1446, -0.1752, -0.6182],
        [-0.1590,  0.5117,  0.0389,  ..., -0.4621, -1.1121, -0.6192]]), output_embedding: tensor([[ 7.2694e-01,  3.1695e-01, -3.3184e-01,  1.1483e+00,  1.5224e+00,
         -5.9879e-01,  7.3964e-02, -6.6572e-01, -3.5503e-01, -9.8590e-02,
          1.1947e+00, -2.6883e-01, -4.5232e-01,  5.6389e-01,  5.5222e-01,
         -3.0688e-01,  1.5410e-01,  1.3478e+00,  1.5335e-02, -3.0108e-01,
         -1.9055e-01,  6.2463e-02,  4.9710e-01,  6.6413e-01, -1.2193e+00,
         -7.5995e-01,  1.1914e+00, -5.0798e-03, -7.6076e-03, -5.7233e-01,
         -5.7443e-01,  4.2273e-01,  7.1485e-02, -4.1189e-01,  3.5460e-01,
         -6.7798e-01, -1.2680e+00, -6.5361e-01,  7.6835e-01,  8.9665e-01,
         -5.0035e-01,  1.0631e-01,  8.2272e-01,  1.8325e+00,  8.0111e-01,
         -2.4224e-01, -4.4750e-01, -4.3873e-01,  3.5413e-01,  9.6047e-01,
         -4.4461e-01, -4.5955e-01,  4.4361e-01,  7.5620e-01, -3.0593e-01,
         -2.6154e-01,  5.9923e-01,  2.2267e-01, -8.2645e-01,  2.1011e-01,
          9.4936e-01, -3.6755e-01, -1.9930e-01,  4.3162e-01, -1.0218e+00,
          9.6713e-01,  1.6494e+00,  2.4206e-01, -3.9320e-01, -3.2703e-01,
          8.2183e-01,  4.3381e-01,  3.0251e-02,  8.4313e-01, -1.7448e+00,
          3.9759e-01,  3.6216e-01, -1.1021e+00,  2.3605e-01, -3.1720e-01,
         -2.0022e+00, -2.3310e+00,  9.9953e-01, -8.5519e-01, -2.0730e+00,
         -1.6531e+00,  7.7287e-01,  3.1795e-01, -9.9628e-01,  6.0382e-01,
         -5.1290e-01, -4.6506e-01, -3.4594e-01, -7.4537e-01, -7.5967e-02,
         -3.6659e-01,  3.7468e-01,  8.3924e-01, -3.1773e-01, -1.0420e+00,
          4.9966e-01, -6.6588e-01, -2.5750e-01, -2.9709e+00,  1.2574e+00,
         -3.5731e-01, -4.8655e-01, -4.8249e-01, -1.1307e+00, -2.9750e-02,
         -1.5870e+00,  5.9637e-02, -1.7909e-01,  6.1030e-01,  6.5910e-01,
         -4.5963e-01, -5.2525e-02,  7.4953e-01, -7.2478e-03, -5.4064e-01,
          9.7136e-02,  8.7130e-01, -4.1656e-01, -7.1769e-01, -5.4666e-02,
          2.0464e+00,  1.4173e+00, -1.2885e-01,  1.1901e-01, -1.0678e+00,
          5.4064e-01,  4.9471e-01, -6.6469e-01, -2.9270e-01,  8.7610e-01,
         -4.9644e-01, -3.8483e-01, -5.8687e-02,  1.3609e-01, -1.5492e-01,
          1.0125e+00, -6.2630e-01,  7.6383e-01, -2.7331e-01, -7.6137e-01,
          9.1030e-01,  1.1017e+00,  3.3450e-01,  5.8950e-01,  6.5643e-01,
          4.0939e-01,  2.9196e-01, -2.5082e-01,  1.7208e+00,  3.1487e-01,
          3.2712e-01,  5.8992e-01,  8.9243e-01, -2.4007e-01,  4.6560e-01,
         -5.8472e-01, -1.2109e+00, -6.9805e-01, -2.2465e-01,  6.3318e-01,
          1.7725e-01, -7.3745e-01,  1.7641e+00,  9.6645e-02,  6.4937e-03,
         -1.1891e-01,  4.8954e-01,  1.5640e-01,  1.2476e-01,  1.5282e+00,
          3.8717e-01,  4.3892e-01, -5.4581e-02, -2.9244e-01, -1.5993e+00,
          8.2941e-01, -1.8160e+00, -8.9961e-01,  2.7121e-01, -5.5043e-01,
         -4.4373e-01, -5.4099e-01, -5.7787e-01,  1.8372e+00, -4.5881e-01,
         -7.1424e-01, -1.0763e+00, -3.4648e-01,  1.4664e-02,  4.0678e-01,
         -8.4456e-01, -2.5652e-01,  4.8034e-01, -6.5535e-01, -4.0944e-02,
          3.5053e-01,  1.4080e+00,  1.0351e+00, -1.8653e-01,  4.6054e-01,
         -3.7751e-02, -6.9706e-01, -1.7872e-01, -1.8899e+00,  7.4991e-01,
          1.4765e+00, -3.4634e-01,  8.3284e-02, -5.2292e-01, -4.0594e-01,
          5.8082e-01, -6.6763e-01,  4.4188e-01, -5.0132e-01,  1.2265e+00,
          1.4121e-01,  2.9375e-01,  1.8764e+00,  5.4309e-01, -1.4453e-01,
         -2.8427e-01, -1.9991e-01, -2.1194e-01,  5.5494e-01, -1.2598e+00,
         -8.5878e-01, -3.2394e-01, -4.0180e-01, -1.9010e-01,  4.2423e-01,
         -5.4407e-01, -1.1354e+00,  1.1981e+00,  9.5252e-01, -6.6023e-01,
          7.0692e-01, -1.2207e-01, -5.5430e-04,  1.5154e-01, -8.5813e-01,
         -1.4299e+00,  2.5044e-01, -4.7223e-01,  9.8596e-01,  8.9382e-01,
          1.7118e-01, -4.6269e-01,  4.0933e-01,  5.5163e-01, -5.7390e-01,
          2.6532e-01],
        [ 3.5175e-01, -3.5604e-01, -9.7067e-02,  1.8338e-01, -1.4753e-01,
          3.8563e-02, -1.3317e-01, -1.9324e-01, -5.1011e-02,  3.1631e-02,
          2.6219e-01,  2.0009e+00, -5.2623e-01,  6.2971e-01,  4.1340e-01,
         -1.5432e+00, -2.3047e-01, -3.2278e-01, -6.5874e-01, -1.4835e-01,
         -2.8788e-01,  1.3388e+00, -6.7844e-02, -3.3641e-01, -1.7970e+00,
         -7.2196e-02,  1.1957e+00,  5.1432e-01, -4.5699e-01, -4.7573e-01,
          5.3592e-01,  5.5428e-02, -2.0762e-01, -9.9296e-01, -4.0767e-01,
         -8.0755e-01, -4.1640e-01,  8.4198e-01,  5.7631e-02,  4.2826e-01,
         -1.1275e+00, -7.6334e-01,  3.1749e-01,  1.5643e+00,  6.7910e-01,
         -4.4407e-01, -4.6601e-02, -2.9539e-02, -2.1661e-01,  7.3895e-02,
         -4.0169e-01,  6.1455e-01,  3.0145e-01,  3.3305e-01,  3.2219e-02,
          2.2168e-01,  1.1359e+00,  3.0313e-01,  1.0197e+00, -7.0802e-01,
          1.8707e-01,  1.6715e-01, -1.4158e+00,  4.1564e-01, -7.4775e-01,
          5.1793e-01, -1.7391e-01,  7.0185e-01,  4.6832e-01, -6.5214e-01,
         -1.6975e-01, -9.6631e-02,  2.2942e-01,  1.1551e-01, -6.6892e-01,
          1.1216e-01,  4.1095e-01, -6.5190e-01, -7.0968e-01, -5.0184e-01,
         -5.7705e-01, -1.4119e+00, -7.5816e-02, -6.7831e-01, -5.5205e-01,
         -1.1766e+00,  3.5930e-01, -4.2231e-01, -1.4423e-01,  2.0953e-01,
         -3.6977e-01,  4.3729e-01,  3.0353e-01, -5.6471e-01,  4.9197e-02,
         -6.1611e-01, -3.1157e-01,  6.7363e-01, -4.2133e-01, -1.9759e-01,
         -9.6557e-01, -5.6422e-01, -1.6308e-02, -1.5764e+00,  3.1522e-01,
         -5.1310e-01, -7.4084e-01, -8.7649e-02, -2.4856e+00, -1.4375e+00,
         -2.0956e+00,  6.6836e-01, -1.7447e-01,  4.7054e-01,  4.5285e-01,
         -6.6924e-01,  1.3629e+00,  8.7438e-01, -7.0519e-01, -2.2270e-01,
         -6.2955e-01,  1.9143e+00,  2.5757e-02, -4.5248e-01, -3.0948e-01,
          4.8609e-01,  1.0507e+00, -5.8151e-01,  9.5013e-02, -6.8459e-01,
          1.5353e+00, -2.1641e-01, -1.0664e+00,  3.2953e-01, -6.4453e-01,
         -9.4313e-01, -2.5347e-01, -3.0617e-01, -9.2170e-01,  4.2102e-01,
          6.5960e-01, -2.1843e-02, -4.5122e-02, -6.4178e-01, -4.7056e-02,
          6.9719e-01, -1.1707e-01, -6.0526e-01, -2.7057e-01,  9.4504e-01,
          5.4400e-01,  1.1060e-01, -9.5943e-01,  4.7167e+00, -2.9433e-01,
          2.9216e-03, -5.2925e-01,  5.3396e-01,  1.3864e-01, -4.1258e-01,
         -6.4463e-01, -1.8232e-01, -1.2657e+00,  1.3741e+00, -1.5489e-01,
          6.2267e-01, -1.0823e+00,  2.0617e+00,  3.7629e-01, -6.1748e-01,
          1.0548e+00,  6.3961e-01,  2.6850e-01,  5.9275e-01,  1.3276e+00,
          1.0104e-01,  4.1088e-01, -3.1435e-01,  3.2603e-01, -3.3215e-01,
          3.2694e+00, -2.2436e-01,  6.9167e-02,  3.2299e-01, -9.0835e-01,
         -1.2739e+00, -1.0349e+00,  7.6532e-01,  7.2499e-01,  3.0560e-01,
         -3.1952e-01, -2.2890e-01,  6.3951e-01,  7.7289e-02, -1.0208e-01,
         -8.7849e-01,  1.1870e+00,  7.8377e-01, -1.8962e-01,  1.4940e-01,
         -3.0071e-01,  5.9864e-01, -1.5389e-01, -5.4247e-01, -3.4934e-01,
         -2.3072e-01,  6.7218e-01,  2.0196e-01, -1.3372e+00,  3.6862e-01,
         -4.0789e-01,  4.2295e-01, -3.4080e-01,  1.9408e-01,  4.9969e-01,
          5.5824e-01, -4.7848e-01,  2.0658e+00, -2.3163e-02, -2.8768e-01,
          1.1680e+00,  5.1867e-01,  9.1732e-01, -4.0654e-01,  1.3632e-01,
         -2.6899e-01, -3.9827e-02, -1.7757e-01, -2.6666e-01, -1.4132e+00,
         -7.0022e-01,  1.1015e-01,  6.1303e-01, -1.5691e-01,  3.0138e-01,
          3.2780e-01,  9.2632e-01,  8.9005e-01,  4.8667e-01,  6.1024e-01,
          2.7990e-01,  5.8607e-01,  3.9382e-01,  1.1831e-01, -3.3819e-01,
         -1.9828e+00, -8.4637e-02,  9.1902e-02,  7.1806e-01,  9.8545e-01,
          7.8920e-01, -2.2768e-01, -1.0494e+00,  4.9757e-01, -8.4579e-01,
          1.1767e-01],
        [-8.5410e-01, -1.3653e-02, -5.9994e-01, -3.9260e-01,  8.6629e-01,
          4.4574e-01,  5.8994e-02, -4.4518e-01,  1.6066e-01,  2.8581e-01,
          9.2751e-01,  3.6498e-01,  5.9080e-01,  5.9024e-01,  3.6513e-01,
         -1.9271e-01, -1.1359e+00,  1.1296e+00, -5.9634e-01,  1.7707e-01,
          7.0479e-01,  2.9443e-01,  3.4823e-01, -6.5629e-03, -7.7836e-01,
         -3.9578e-01,  4.6806e-01,  8.2694e-01, -3.4464e-01, -1.6236e+00,
         -6.6032e-02,  3.7356e-01,  1.7833e-01, -5.4454e-01, -8.0832e-01,
         -2.1392e-01, -2.9915e-01, -2.5563e-02,  8.7833e-01,  6.8929e-01,
         -1.0603e+00,  1.8376e+00,  4.4792e-01, -3.0744e+00,  9.8138e-01,
          5.3489e-01,  7.1467e-01, -1.1210e-01,  2.1846e-02,  4.5077e-01,
          2.4371e+00,  1.9617e-01, -1.5992e-01, -3.7091e-01, -8.4181e-01,
          5.7519e-01,  9.3733e-01,  7.2198e-01,  9.3957e-01, -6.8650e-01,
         -2.9789e-01, -2.9121e-01, -7.9973e-01,  5.4766e-01,  3.1243e-01,
          6.3460e-01,  4.6200e-01, -8.5588e-01, -2.1389e-01, -2.5874e-01,
         -1.9357e-02, -1.5903e-01, -7.0800e-01, -2.8155e-01,  6.7967e-01,
          2.8297e-03, -1.0327e+00, -4.2943e-01,  3.8248e-03, -1.8320e-01,
          1.4094e+00, -1.4613e+00, -7.3006e-01,  3.1213e-01, -8.8545e-01,
         -1.0902e+00, -5.3080e-01, -2.5854e-02,  4.6226e-01, -1.9763e-01,
         -1.4980e-02,  5.7094e-01,  3.7040e-01,  3.1193e-01,  5.4305e-01,
          5.5403e-02,  4.3927e-01, -8.1247e-01, -7.4195e-01,  6.5472e-01,
          3.2516e-01,  1.7001e-01,  5.6941e-01, -9.2623e-01, -1.5805e-01,
         -5.5653e-01, -1.5697e+00,  9.5220e-01, -1.7465e+00, -6.4865e-01,
          1.0835e+00,  1.2268e+00, -2.2257e-01, -1.3739e+00, -3.7203e-01,
         -4.4517e-02, -6.1463e-01, -5.5013e-01, -1.7509e-01,  1.3676e-01,
          6.9227e-01,  3.8846e-01,  2.7001e-01,  3.6216e-01,  6.5690e-01,
         -7.4120e-01,  8.6808e-02,  1.2616e+00,  7.2347e-01,  6.1484e-01,
          1.3079e+00, -1.5750e-01, -1.1012e+00,  4.8019e-01, -2.9056e-01,
         -6.3401e-02, -9.3528e-02, -1.4350e+00, -5.2750e-01,  8.2331e-01,
          2.1115e-01,  2.8066e-01,  6.4499e-01,  6.8153e-01,  7.0916e-01,
          1.3536e+00, -5.2912e-01, -2.0708e-01,  1.3237e+00,  3.5245e-01,
          3.7896e-01,  3.6975e-01, -8.0600e-01,  2.7276e+00,  1.6323e+00,
          9.3643e-01, -3.3077e-01,  3.8006e-02,  6.4981e-01,  5.3370e-02,
         -8.6701e-01, -4.8912e-01,  1.3947e-03, -7.4726e-01, -6.1742e-01,
         -9.0252e-02, -5.9249e-01,  1.1394e+00,  8.5479e-01, -5.6317e-01,
         -1.2356e-02, -7.6427e-01, -8.4514e-01, -1.0813e-01,  1.3639e+00,
          1.5924e-01, -8.5771e-01, -3.2660e-01,  7.5353e-01,  2.6395e-01,
         -3.4856e-01,  3.0623e-01, -1.4393e+00,  9.4517e-01,  7.5477e-02,
         -2.2159e+00,  6.3105e-02,  5.3979e-01,  2.9668e-01, -1.2851e+00,
          8.6741e-01, -7.2136e-01,  7.5946e-02, -1.0501e-01, -6.0374e-01,
         -8.5043e-01, -2.0430e-01,  1.4407e+00,  5.0037e-01, -4.4212e-01,
          4.4270e-01,  1.1354e+00, -4.4780e-01,  3.9675e-01,  6.3906e-02,
          2.4338e-01, -3.6491e-01,  5.4376e-01, -7.8556e-01,  2.9817e-01,
          3.8700e-01, -9.5799e-02, -1.2757e+00, -1.0806e+00,  6.3144e-01,
          1.3459e+00,  2.8040e-01, -1.4387e+00,  1.4059e-01, -2.2057e-01,
         -1.2620e+00, -9.9024e-02, -1.3269e+00, -8.0128e-01,  3.2628e-02,
         -3.9028e-02,  4.2750e-01,  2.6276e-01,  3.2275e-01, -3.5256e+00,
          6.9590e-01,  4.7463e-01,  3.0509e-02, -8.9338e-01,  1.1479e+00,
         -1.1521e+00, -1.6299e+00, -7.0114e-01, -8.3033e-01, -6.0437e-02,
         -1.2732e-01,  1.6454e-01,  4.5053e-01,  1.5171e+00, -4.1756e-02,
          2.3419e-01,  2.5649e-01, -5.3035e-01, -2.1445e-01,  9.9852e-01,
          4.9138e-01,  1.1628e-01, -7.1855e-01,  9.3856e-02, -4.6179e-01,
          3.9456e-01]]), obj_idxes: tensor([0, 1, 2]), matched_gt_idxes: tensor([-1, -1, -1]), disappear_time: tensor([0, 0, 0]), iou: tensor([0., 0., 0.]), scores: tensor([0.8401, 0.8999, 0.8660]), track_scores: tensor([0., 0., 0.]), pred_boxes: tensor([[-4.4362e+00, -1.0457e+01, -5.7496e-01,  5.4815e-01, -1.9604e+00,
          8.2555e-02,  2.6890e-02,  9.9731e-01, -1.2011e-03, -9.9059e-04],
        [-7.5262e-01, -2.4579e+01,  6.6525e-01,  1.5121e+00, -2.2621e+00,
          4.3397e-01, -1.8351e-02, -9.8844e-01,  8.8571e-02,  5.5166e+00],
        [-1.2292e+01,  3.5435e+01, -3.3134e-01, -2.0525e-01, -9.5619e-01,
          5.5846e-01,  1.1792e-01, -9.8280e-01, -2.9149e-01,  1.2141e+00]]), pred_logits: tensor([[ -6.4774,  -8.0849,  -7.5573, -11.3790,  -6.8656,  -5.9882,  -2.8453,
           1.6590,  -5.5511,  -4.6095],
        [  2.1958,  -6.2505,  -7.5734,  -7.5439, -13.5229,  -6.9315,  -7.1278,
          -9.5285,  -7.1040,  -6.2322],
        [ -8.0861,  -7.3184,  -7.0678,  -7.8600,  -6.4306,  -4.7218,  -6.1443,
          -5.5766,   1.8664,  -5.3478]]), mem_bank: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), mem_padding_mask: tensor([[True, True, True, True],
        [True, True, True, True],
        [True, True, True, True]]), save_period: tensor([0., 0., 0.])])
(Pdb) torch.Size([3, 512])
(Pdb) torch.Size([3, 3])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(481)select_active_track_query()
-> result_dict = self._track_instances2results(track_instances[active_index], img_metas, with_mask=with_mask)
(Pdb) tensor([False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False,  True, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False,  True, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False,  True, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False])
(Pdb) tensor([False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False,  True, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False,  True, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False,  True, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False])
(Pdb) torch.Size([901])
(Pdb) tensor([False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False,  True, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False,  True, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False,  True, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False])
(Pdb) tensor([False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False,  True, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False,  True, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False,  True, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(148)__getitem__()->Instances(num...0., 0., 0.])])
-> return ret
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(481)select_active_track_query()
-> result_dict = self._track_instances2results(track_instances[active_index], img_metas, with_mask=with_mask)
(Pdb) Instances(num_instances=3, image_height=1, image_width=1, fields=[ref_pts: tensor([[-0.1737, -0.4143, -0.4898],
        [-0.0294, -1.0461, -0.6533],
        [-0.4897,  1.7039,  0.0219]]), query: tensor([[-0.5876,  1.6050, -2.0626,  ..., -0.8195,  2.2342,  1.4749],
        [-0.5229, -0.1827, -0.5886,  ..., -0.1446, -0.1752, -0.6182],
        [-0.1590,  0.5117,  0.0389,  ..., -0.4621, -1.1121, -0.6192]]), output_embedding: tensor([[ 7.2694e-01,  3.1695e-01, -3.3184e-01,  1.1483e+00,  1.5224e+00,
         -5.9879e-01,  7.3964e-02, -6.6572e-01, -3.5503e-01, -9.8590e-02,
          1.1947e+00, -2.6883e-01, -4.5232e-01,  5.6389e-01,  5.5222e-01,
         -3.0688e-01,  1.5410e-01,  1.3478e+00,  1.5335e-02, -3.0108e-01,
         -1.9055e-01,  6.2463e-02,  4.9710e-01,  6.6413e-01, -1.2193e+00,
         -7.5995e-01,  1.1914e+00, -5.0798e-03, -7.6076e-03, -5.7233e-01,
         -5.7443e-01,  4.2273e-01,  7.1485e-02, -4.1189e-01,  3.5460e-01,
         -6.7798e-01, -1.2680e+00, -6.5361e-01,  7.6835e-01,  8.9665e-01,
         -5.0035e-01,  1.0631e-01,  8.2272e-01,  1.8325e+00,  8.0111e-01,
         -2.4224e-01, -4.4750e-01, -4.3873e-01,  3.5413e-01,  9.6047e-01,
         -4.4461e-01, -4.5955e-01,  4.4361e-01,  7.5620e-01, -3.0593e-01,
         -2.6154e-01,  5.9923e-01,  2.2267e-01, -8.2645e-01,  2.1011e-01,
          9.4936e-01, -3.6755e-01, -1.9930e-01,  4.3162e-01, -1.0218e+00,
          9.6713e-01,  1.6494e+00,  2.4206e-01, -3.9320e-01, -3.2703e-01,
          8.2183e-01,  4.3381e-01,  3.0251e-02,  8.4313e-01, -1.7448e+00,
          3.9759e-01,  3.6216e-01, -1.1021e+00,  2.3605e-01, -3.1720e-01,
         -2.0022e+00, -2.3310e+00,  9.9953e-01, -8.5519e-01, -2.0730e+00,
         -1.6531e+00,  7.7287e-01,  3.1795e-01, -9.9628e-01,  6.0382e-01,
         -5.1290e-01, -4.6506e-01, -3.4594e-01, -7.4537e-01, -7.5967e-02,
         -3.6659e-01,  3.7468e-01,  8.3924e-01, -3.1773e-01, -1.0420e+00,
          4.9966e-01, -6.6588e-01, -2.5750e-01, -2.9709e+00,  1.2574e+00,
         -3.5731e-01, -4.8655e-01, -4.8249e-01, -1.1307e+00, -2.9750e-02,
         -1.5870e+00,  5.9637e-02, -1.7909e-01,  6.1030e-01,  6.5910e-01,
         -4.5963e-01, -5.2525e-02,  7.4953e-01, -7.2478e-03, -5.4064e-01,
          9.7136e-02,  8.7130e-01, -4.1656e-01, -7.1769e-01, -5.4666e-02,
          2.0464e+00,  1.4173e+00, -1.2885e-01,  1.1901e-01, -1.0678e+00,
          5.4064e-01,  4.9471e-01, -6.6469e-01, -2.9270e-01,  8.7610e-01,
         -4.9644e-01, -3.8483e-01, -5.8687e-02,  1.3609e-01, -1.5492e-01,
          1.0125e+00, -6.2630e-01,  7.6383e-01, -2.7331e-01, -7.6137e-01,
          9.1030e-01,  1.1017e+00,  3.3450e-01,  5.8950e-01,  6.5643e-01,
          4.0939e-01,  2.9196e-01, -2.5082e-01,  1.7208e+00,  3.1487e-01,
          3.2712e-01,  5.8992e-01,  8.9243e-01, -2.4007e-01,  4.6560e-01,
         -5.8472e-01, -1.2109e+00, -6.9805e-01, -2.2465e-01,  6.3318e-01,
          1.7725e-01, -7.3745e-01,  1.7641e+00,  9.6645e-02,  6.4937e-03,
         -1.1891e-01,  4.8954e-01,  1.5640e-01,  1.2476e-01,  1.5282e+00,
          3.8717e-01,  4.3892e-01, -5.4581e-02, -2.9244e-01, -1.5993e+00,
          8.2941e-01, -1.8160e+00, -8.9961e-01,  2.7121e-01, -5.5043e-01,
         -4.4373e-01, -5.4099e-01, -5.7787e-01,  1.8372e+00, -4.5881e-01,
         -7.1424e-01, -1.0763e+00, -3.4648e-01,  1.4664e-02,  4.0678e-01,
         -8.4456e-01, -2.5652e-01,  4.8034e-01, -6.5535e-01, -4.0944e-02,
          3.5053e-01,  1.4080e+00,  1.0351e+00, -1.8653e-01,  4.6054e-01,
         -3.7751e-02, -6.9706e-01, -1.7872e-01, -1.8899e+00,  7.4991e-01,
          1.4765e+00, -3.4634e-01,  8.3284e-02, -5.2292e-01, -4.0594e-01,
          5.8082e-01, -6.6763e-01,  4.4188e-01, -5.0132e-01,  1.2265e+00,
          1.4121e-01,  2.9375e-01,  1.8764e+00,  5.4309e-01, -1.4453e-01,
         -2.8427e-01, -1.9991e-01, -2.1194e-01,  5.5494e-01, -1.2598e+00,
         -8.5878e-01, -3.2394e-01, -4.0180e-01, -1.9010e-01,  4.2423e-01,
         -5.4407e-01, -1.1354e+00,  1.1981e+00,  9.5252e-01, -6.6023e-01,
          7.0692e-01, -1.2207e-01, -5.5430e-04,  1.5154e-01, -8.5813e-01,
         -1.4299e+00,  2.5044e-01, -4.7223e-01,  9.8596e-01,  8.9382e-01,
          1.7118e-01, -4.6269e-01,  4.0933e-01,  5.5163e-01, -5.7390e-01,
          2.6532e-01],
        [ 3.5175e-01, -3.5604e-01, -9.7067e-02,  1.8338e-01, -1.4753e-01,
          3.8563e-02, -1.3317e-01, -1.9324e-01, -5.1011e-02,  3.1631e-02,
          2.6219e-01,  2.0009e+00, -5.2623e-01,  6.2971e-01,  4.1340e-01,
         -1.5432e+00, -2.3047e-01, -3.2278e-01, -6.5874e-01, -1.4835e-01,
         -2.8788e-01,  1.3388e+00, -6.7844e-02, -3.3641e-01, -1.7970e+00,
         -7.2196e-02,  1.1957e+00,  5.1432e-01, -4.5699e-01, -4.7573e-01,
          5.3592e-01,  5.5428e-02, -2.0762e-01, -9.9296e-01, -4.0767e-01,
         -8.0755e-01, -4.1640e-01,  8.4198e-01,  5.7631e-02,  4.2826e-01,
         -1.1275e+00, -7.6334e-01,  3.1749e-01,  1.5643e+00,  6.7910e-01,
         -4.4407e-01, -4.6601e-02, -2.9539e-02, -2.1661e-01,  7.3895e-02,
         -4.0169e-01,  6.1455e-01,  3.0145e-01,  3.3305e-01,  3.2219e-02,
          2.2168e-01,  1.1359e+00,  3.0313e-01,  1.0197e+00, -7.0802e-01,
          1.8707e-01,  1.6715e-01, -1.4158e+00,  4.1564e-01, -7.4775e-01,
          5.1793e-01, -1.7391e-01,  7.0185e-01,  4.6832e-01, -6.5214e-01,
         -1.6975e-01, -9.6631e-02,  2.2942e-01,  1.1551e-01, -6.6892e-01,
          1.1216e-01,  4.1095e-01, -6.5190e-01, -7.0968e-01, -5.0184e-01,
         -5.7705e-01, -1.4119e+00, -7.5816e-02, -6.7831e-01, -5.5205e-01,
         -1.1766e+00,  3.5930e-01, -4.2231e-01, -1.4423e-01,  2.0953e-01,
         -3.6977e-01,  4.3729e-01,  3.0353e-01, -5.6471e-01,  4.9197e-02,
         -6.1611e-01, -3.1157e-01,  6.7363e-01, -4.2133e-01, -1.9759e-01,
         -9.6557e-01, -5.6422e-01, -1.6308e-02, -1.5764e+00,  3.1522e-01,
         -5.1310e-01, -7.4084e-01, -8.7649e-02, -2.4856e+00, -1.4375e+00,
         -2.0956e+00,  6.6836e-01, -1.7447e-01,  4.7054e-01,  4.5285e-01,
         -6.6924e-01,  1.3629e+00,  8.7438e-01, -7.0519e-01, -2.2270e-01,
         -6.2955e-01,  1.9143e+00,  2.5757e-02, -4.5248e-01, -3.0948e-01,
          4.8609e-01,  1.0507e+00, -5.8151e-01,  9.5013e-02, -6.8459e-01,
          1.5353e+00, -2.1641e-01, -1.0664e+00,  3.2953e-01, -6.4453e-01,
         -9.4313e-01, -2.5347e-01, -3.0617e-01, -9.2170e-01,  4.2102e-01,
          6.5960e-01, -2.1843e-02, -4.5122e-02, -6.4178e-01, -4.7056e-02,
          6.9719e-01, -1.1707e-01, -6.0526e-01, -2.7057e-01,  9.4504e-01,
          5.4400e-01,  1.1060e-01, -9.5943e-01,  4.7167e+00, -2.9433e-01,
          2.9216e-03, -5.2925e-01,  5.3396e-01,  1.3864e-01, -4.1258e-01,
         -6.4463e-01, -1.8232e-01, -1.2657e+00,  1.3741e+00, -1.5489e-01,
          6.2267e-01, -1.0823e+00,  2.0617e+00,  3.7629e-01, -6.1748e-01,
          1.0548e+00,  6.3961e-01,  2.6850e-01,  5.9275e-01,  1.3276e+00,
          1.0104e-01,  4.1088e-01, -3.1435e-01,  3.2603e-01, -3.3215e-01,
          3.2694e+00, -2.2436e-01,  6.9167e-02,  3.2299e-01, -9.0835e-01,
         -1.2739e+00, -1.0349e+00,  7.6532e-01,  7.2499e-01,  3.0560e-01,
         -3.1952e-01, -2.2890e-01,  6.3951e-01,  7.7289e-02, -1.0208e-01,
         -8.7849e-01,  1.1870e+00,  7.8377e-01, -1.8962e-01,  1.4940e-01,
         -3.0071e-01,  5.9864e-01, -1.5389e-01, -5.4247e-01, -3.4934e-01,
         -2.3072e-01,  6.7218e-01,  2.0196e-01, -1.3372e+00,  3.6862e-01,
         -4.0789e-01,  4.2295e-01, -3.4080e-01,  1.9408e-01,  4.9969e-01,
          5.5824e-01, -4.7848e-01,  2.0658e+00, -2.3163e-02, -2.8768e-01,
          1.1680e+00,  5.1867e-01,  9.1732e-01, -4.0654e-01,  1.3632e-01,
         -2.6899e-01, -3.9827e-02, -1.7757e-01, -2.6666e-01, -1.4132e+00,
         -7.0022e-01,  1.1015e-01,  6.1303e-01, -1.5691e-01,  3.0138e-01,
          3.2780e-01,  9.2632e-01,  8.9005e-01,  4.8667e-01,  6.1024e-01,
          2.7990e-01,  5.8607e-01,  3.9382e-01,  1.1831e-01, -3.3819e-01,
         -1.9828e+00, -8.4637e-02,  9.1902e-02,  7.1806e-01,  9.8545e-01,
          7.8920e-01, -2.2768e-01, -1.0494e+00,  4.9757e-01, -8.4579e-01,
          1.1767e-01],
        [-8.5410e-01, -1.3653e-02, -5.9994e-01, -3.9260e-01,  8.6629e-01,
          4.4574e-01,  5.8994e-02, -4.4518e-01,  1.6066e-01,  2.8581e-01,
          9.2751e-01,  3.6498e-01,  5.9080e-01,  5.9024e-01,  3.6513e-01,
         -1.9271e-01, -1.1359e+00,  1.1296e+00, -5.9634e-01,  1.7707e-01,
          7.0479e-01,  2.9443e-01,  3.4823e-01, -6.5629e-03, -7.7836e-01,
         -3.9578e-01,  4.6806e-01,  8.2694e-01, -3.4464e-01, -1.6236e+00,
         -6.6032e-02,  3.7356e-01,  1.7833e-01, -5.4454e-01, -8.0832e-01,
         -2.1392e-01, -2.9915e-01, -2.5563e-02,  8.7833e-01,  6.8929e-01,
         -1.0603e+00,  1.8376e+00,  4.4792e-01, -3.0744e+00,  9.8138e-01,
          5.3489e-01,  7.1467e-01, -1.1210e-01,  2.1846e-02,  4.5077e-01,
          2.4371e+00,  1.9617e-01, -1.5992e-01, -3.7091e-01, -8.4181e-01,
          5.7519e-01,  9.3733e-01,  7.2198e-01,  9.3957e-01, -6.8650e-01,
         -2.9789e-01, -2.9121e-01, -7.9973e-01,  5.4766e-01,  3.1243e-01,
          6.3460e-01,  4.6200e-01, -8.5588e-01, -2.1389e-01, -2.5874e-01,
         -1.9357e-02, -1.5903e-01, -7.0800e-01, -2.8155e-01,  6.7967e-01,
          2.8297e-03, -1.0327e+00, -4.2943e-01,  3.8248e-03, -1.8320e-01,
          1.4094e+00, -1.4613e+00, -7.3006e-01,  3.1213e-01, -8.8545e-01,
         -1.0902e+00, -5.3080e-01, -2.5854e-02,  4.6226e-01, -1.9763e-01,
         -1.4980e-02,  5.7094e-01,  3.7040e-01,  3.1193e-01,  5.4305e-01,
          5.5403e-02,  4.3927e-01, -8.1247e-01, -7.4195e-01,  6.5472e-01,
          3.2516e-01,  1.7001e-01,  5.6941e-01, -9.2623e-01, -1.5805e-01,
         -5.5653e-01, -1.5697e+00,  9.5220e-01, -1.7465e+00, -6.4865e-01,
          1.0835e+00,  1.2268e+00, -2.2257e-01, -1.3739e+00, -3.7203e-01,
         -4.4517e-02, -6.1463e-01, -5.5013e-01, -1.7509e-01,  1.3676e-01,
          6.9227e-01,  3.8846e-01,  2.7001e-01,  3.6216e-01,  6.5690e-01,
         -7.4120e-01,  8.6808e-02,  1.2616e+00,  7.2347e-01,  6.1484e-01,
          1.3079e+00, -1.5750e-01, -1.1012e+00,  4.8019e-01, -2.9056e-01,
         -6.3401e-02, -9.3528e-02, -1.4350e+00, -5.2750e-01,  8.2331e-01,
          2.1115e-01,  2.8066e-01,  6.4499e-01,  6.8153e-01,  7.0916e-01,
          1.3536e+00, -5.2912e-01, -2.0708e-01,  1.3237e+00,  3.5245e-01,
          3.7896e-01,  3.6975e-01, -8.0600e-01,  2.7276e+00,  1.6323e+00,
          9.3643e-01, -3.3077e-01,  3.8006e-02,  6.4981e-01,  5.3370e-02,
         -8.6701e-01, -4.8912e-01,  1.3947e-03, -7.4726e-01, -6.1742e-01,
         -9.0252e-02, -5.9249e-01,  1.1394e+00,  8.5479e-01, -5.6317e-01,
         -1.2356e-02, -7.6427e-01, -8.4514e-01, -1.0813e-01,  1.3639e+00,
          1.5924e-01, -8.5771e-01, -3.2660e-01,  7.5353e-01,  2.6395e-01,
         -3.4856e-01,  3.0623e-01, -1.4393e+00,  9.4517e-01,  7.5477e-02,
         -2.2159e+00,  6.3105e-02,  5.3979e-01,  2.9668e-01, -1.2851e+00,
          8.6741e-01, -7.2136e-01,  7.5946e-02, -1.0501e-01, -6.0374e-01,
         -8.5043e-01, -2.0430e-01,  1.4407e+00,  5.0037e-01, -4.4212e-01,
          4.4270e-01,  1.1354e+00, -4.4780e-01,  3.9675e-01,  6.3906e-02,
          2.4338e-01, -3.6491e-01,  5.4376e-01, -7.8556e-01,  2.9817e-01,
          3.8700e-01, -9.5799e-02, -1.2757e+00, -1.0806e+00,  6.3144e-01,
          1.3459e+00,  2.8040e-01, -1.4387e+00,  1.4059e-01, -2.2057e-01,
         -1.2620e+00, -9.9024e-02, -1.3269e+00, -8.0128e-01,  3.2628e-02,
         -3.9028e-02,  4.2750e-01,  2.6276e-01,  3.2275e-01, -3.5256e+00,
          6.9590e-01,  4.7463e-01,  3.0509e-02, -8.9338e-01,  1.1479e+00,
         -1.1521e+00, -1.6299e+00, -7.0114e-01, -8.3033e-01, -6.0437e-02,
         -1.2732e-01,  1.6454e-01,  4.5053e-01,  1.5171e+00, -4.1756e-02,
          2.3419e-01,  2.5649e-01, -5.3035e-01, -2.1445e-01,  9.9852e-01,
          4.9138e-01,  1.1628e-01, -7.1855e-01,  9.3856e-02, -4.6179e-01,
          3.9456e-01]]), obj_idxes: tensor([0, 1, 2]), matched_gt_idxes: tensor([-1, -1, -1]), disappear_time: tensor([0, 0, 0]), iou: tensor([0., 0., 0.]), scores: tensor([0.8401, 0.8999, 0.8660]), track_scores: tensor([0., 0., 0.]), pred_boxes: tensor([[-4.4362e+00, -1.0457e+01, -5.7496e-01,  5.4815e-01, -1.9604e+00,
          8.2555e-02,  2.6890e-02,  9.9731e-01, -1.2011e-03, -9.9059e-04],
        [-7.5262e-01, -2.4579e+01,  6.6525e-01,  1.5121e+00, -2.2621e+00,
          4.3397e-01, -1.8351e-02, -9.8844e-01,  8.8571e-02,  5.5166e+00],
        [-1.2292e+01,  3.5435e+01, -3.3134e-01, -2.0525e-01, -9.5619e-01,
          5.5846e-01,  1.1792e-01, -9.8280e-01, -2.9149e-01,  1.2141e+00]]), pred_logits: tensor([[ -6.4774,  -8.0849,  -7.5573, -11.3790,  -6.8656,  -5.9882,  -2.8453,
           1.6590,  -5.5511,  -4.6095],
        [  2.1958,  -6.2505,  -7.5734,  -7.5439, -13.5229,  -6.9315,  -7.1278,
          -9.5285,  -7.1040,  -6.2322],
        [ -8.0861,  -7.3184,  -7.0678,  -7.8600,  -6.4306,  -4.7218,  -6.1443,
          -5.5766,   1.8664,  -5.3478]]), mem_bank: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), mem_padding_mask: tensor([[True, True, True, True],
        [True, True, True, True],
        [True, True, True, True]]), save_period: tensor([0., 0., 0.])])
(Pdb) *** AttributeError: Cannot find field 'fields' in the given Instances!
(Pdb) {'ref_pts': tensor([[-0.1737, -0.4143, -0.4898],
        [-0.0294, -1.0461, -0.6533],
        [-0.4897,  1.7039,  0.0219]]), 'query': tensor([[-0.5876,  1.6050, -2.0626,  ..., -0.8195,  2.2342,  1.4749],
        [-0.5229, -0.1827, -0.5886,  ..., -0.1446, -0.1752, -0.6182],
        [-0.1590,  0.5117,  0.0389,  ..., -0.4621, -1.1121, -0.6192]]), 'output_embedding': tensor([[ 7.2694e-01,  3.1695e-01, -3.3184e-01,  1.1483e+00,  1.5224e+00,
         -5.9879e-01,  7.3964e-02, -6.6572e-01, -3.5503e-01, -9.8590e-02,
          1.1947e+00, -2.6883e-01, -4.5232e-01,  5.6389e-01,  5.5222e-01,
         -3.0688e-01,  1.5410e-01,  1.3478e+00,  1.5335e-02, -3.0108e-01,
         -1.9055e-01,  6.2463e-02,  4.9710e-01,  6.6413e-01, -1.2193e+00,
         -7.5995e-01,  1.1914e+00, -5.0798e-03, -7.6076e-03, -5.7233e-01,
         -5.7443e-01,  4.2273e-01,  7.1485e-02, -4.1189e-01,  3.5460e-01,
         -6.7798e-01, -1.2680e+00, -6.5361e-01,  7.6835e-01,  8.9665e-01,
         -5.0035e-01,  1.0631e-01,  8.2272e-01,  1.8325e+00,  8.0111e-01,
         -2.4224e-01, -4.4750e-01, -4.3873e-01,  3.5413e-01,  9.6047e-01,
         -4.4461e-01, -4.5955e-01,  4.4361e-01,  7.5620e-01, -3.0593e-01,
         -2.6154e-01,  5.9923e-01,  2.2267e-01, -8.2645e-01,  2.1011e-01,
          9.4936e-01, -3.6755e-01, -1.9930e-01,  4.3162e-01, -1.0218e+00,
          9.6713e-01,  1.6494e+00,  2.4206e-01, -3.9320e-01, -3.2703e-01,
          8.2183e-01,  4.3381e-01,  3.0251e-02,  8.4313e-01, -1.7448e+00,
          3.9759e-01,  3.6216e-01, -1.1021e+00,  2.3605e-01, -3.1720e-01,
         -2.0022e+00, -2.3310e+00,  9.9953e-01, -8.5519e-01, -2.0730e+00,
         -1.6531e+00,  7.7287e-01,  3.1795e-01, -9.9628e-01,  6.0382e-01,
         -5.1290e-01, -4.6506e-01, -3.4594e-01, -7.4537e-01, -7.5967e-02,
         -3.6659e-01,  3.7468e-01,  8.3924e-01, -3.1773e-01, -1.0420e+00,
          4.9966e-01, -6.6588e-01, -2.5750e-01, -2.9709e+00,  1.2574e+00,
         -3.5731e-01, -4.8655e-01, -4.8249e-01, -1.1307e+00, -2.9750e-02,
         -1.5870e+00,  5.9637e-02, -1.7909e-01,  6.1030e-01,  6.5910e-01,
         -4.5963e-01, -5.2525e-02,  7.4953e-01, -7.2478e-03, -5.4064e-01,
          9.7136e-02,  8.7130e-01, -4.1656e-01, -7.1769e-01, -5.4666e-02,
          2.0464e+00,  1.4173e+00, -1.2885e-01,  1.1901e-01, -1.0678e+00,
          5.4064e-01,  4.9471e-01, -6.6469e-01, -2.9270e-01,  8.7610e-01,
         -4.9644e-01, -3.8483e-01, -5.8687e-02,  1.3609e-01, -1.5492e-01,
          1.0125e+00, -6.2630e-01,  7.6383e-01, -2.7331e-01, -7.6137e-01,
          9.1030e-01,  1.1017e+00,  3.3450e-01,  5.8950e-01,  6.5643e-01,
          4.0939e-01,  2.9196e-01, -2.5082e-01,  1.7208e+00,  3.1487e-01,
          3.2712e-01,  5.8992e-01,  8.9243e-01, -2.4007e-01,  4.6560e-01,
         -5.8472e-01, -1.2109e+00, -6.9805e-01, -2.2465e-01,  6.3318e-01,
          1.7725e-01, -7.3745e-01,  1.7641e+00,  9.6645e-02,  6.4937e-03,
         -1.1891e-01,  4.8954e-01,  1.5640e-01,  1.2476e-01,  1.5282e+00,
          3.8717e-01,  4.3892e-01, -5.4581e-02, -2.9244e-01, -1.5993e+00,
          8.2941e-01, -1.8160e+00, -8.9961e-01,  2.7121e-01, -5.5043e-01,
         -4.4373e-01, -5.4099e-01, -5.7787e-01,  1.8372e+00, -4.5881e-01,
         -7.1424e-01, -1.0763e+00, -3.4648e-01,  1.4664e-02,  4.0678e-01,
         -8.4456e-01, -2.5652e-01,  4.8034e-01, -6.5535e-01, -4.0944e-02,
          3.5053e-01,  1.4080e+00,  1.0351e+00, -1.8653e-01,  4.6054e-01,
         -3.7751e-02, -6.9706e-01, -1.7872e-01, -1.8899e+00,  7.4991e-01,
          1.4765e+00, -3.4634e-01,  8.3284e-02, -5.2292e-01, -4.0594e-01,
          5.8082e-01, -6.6763e-01,  4.4188e-01, -5.0132e-01,  1.2265e+00,
          1.4121e-01,  2.9375e-01,  1.8764e+00,  5.4309e-01, -1.4453e-01,
         -2.8427e-01, -1.9991e-01, -2.1194e-01,  5.5494e-01, -1.2598e+00,
         -8.5878e-01, -3.2394e-01, -4.0180e-01, -1.9010e-01,  4.2423e-01,
         -5.4407e-01, -1.1354e+00,  1.1981e+00,  9.5252e-01, -6.6023e-01,
          7.0692e-01, -1.2207e-01, -5.5430e-04,  1.5154e-01, -8.5813e-01,
         -1.4299e+00,  2.5044e-01, -4.7223e-01,  9.8596e-01,  8.9382e-01,
          1.7118e-01, -4.6269e-01,  4.0933e-01,  5.5163e-01, -5.7390e-01,
          2.6532e-01],
        [ 3.5175e-01, -3.5604e-01, -9.7067e-02,  1.8338e-01, -1.4753e-01,
          3.8563e-02, -1.3317e-01, -1.9324e-01, -5.1011e-02,  3.1631e-02,
          2.6219e-01,  2.0009e+00, -5.2623e-01,  6.2971e-01,  4.1340e-01,
         -1.5432e+00, -2.3047e-01, -3.2278e-01, -6.5874e-01, -1.4835e-01,
         -2.8788e-01,  1.3388e+00, -6.7844e-02, -3.3641e-01, -1.7970e+00,
         -7.2196e-02,  1.1957e+00,  5.1432e-01, -4.5699e-01, -4.7573e-01,
          5.3592e-01,  5.5428e-02, -2.0762e-01, -9.9296e-01, -4.0767e-01,
         -8.0755e-01, -4.1640e-01,  8.4198e-01,  5.7631e-02,  4.2826e-01,
         -1.1275e+00, -7.6334e-01,  3.1749e-01,  1.5643e+00,  6.7910e-01,
         -4.4407e-01, -4.6601e-02, -2.9539e-02, -2.1661e-01,  7.3895e-02,
         -4.0169e-01,  6.1455e-01,  3.0145e-01,  3.3305e-01,  3.2219e-02,
          2.2168e-01,  1.1359e+00,  3.0313e-01,  1.0197e+00, -7.0802e-01,
          1.8707e-01,  1.6715e-01, -1.4158e+00,  4.1564e-01, -7.4775e-01,
          5.1793e-01, -1.7391e-01,  7.0185e-01,  4.6832e-01, -6.5214e-01,
         -1.6975e-01, -9.6631e-02,  2.2942e-01,  1.1551e-01, -6.6892e-01,
          1.1216e-01,  4.1095e-01, -6.5190e-01, -7.0968e-01, -5.0184e-01,
         -5.7705e-01, -1.4119e+00, -7.5816e-02, -6.7831e-01, -5.5205e-01,
         -1.1766e+00,  3.5930e-01, -4.2231e-01, -1.4423e-01,  2.0953e-01,
         -3.6977e-01,  4.3729e-01,  3.0353e-01, -5.6471e-01,  4.9197e-02,
         -6.1611e-01, -3.1157e-01,  6.7363e-01, -4.2133e-01, -1.9759e-01,
         -9.6557e-01, -5.6422e-01, -1.6308e-02, -1.5764e+00,  3.1522e-01,
         -5.1310e-01, -7.4084e-01, -8.7649e-02, -2.4856e+00, -1.4375e+00,
         -2.0956e+00,  6.6836e-01, -1.7447e-01,  4.7054e-01,  4.5285e-01,
         -6.6924e-01,  1.3629e+00,  8.7438e-01, -7.0519e-01, -2.2270e-01,
         -6.2955e-01,  1.9143e+00,  2.5757e-02, -4.5248e-01, -3.0948e-01,
          4.8609e-01,  1.0507e+00, -5.8151e-01,  9.5013e-02, -6.8459e-01,
          1.5353e+00, -2.1641e-01, -1.0664e+00,  3.2953e-01, -6.4453e-01,
         -9.4313e-01, -2.5347e-01, -3.0617e-01, -9.2170e-01,  4.2102e-01,
          6.5960e-01, -2.1843e-02, -4.5122e-02, -6.4178e-01, -4.7056e-02,
          6.9719e-01, -1.1707e-01, -6.0526e-01, -2.7057e-01,  9.4504e-01,
          5.4400e-01,  1.1060e-01, -9.5943e-01,  4.7167e+00, -2.9433e-01,
          2.9216e-03, -5.2925e-01,  5.3396e-01,  1.3864e-01, -4.1258e-01,
         -6.4463e-01, -1.8232e-01, -1.2657e+00,  1.3741e+00, -1.5489e-01,
          6.2267e-01, -1.0823e+00,  2.0617e+00,  3.7629e-01, -6.1748e-01,
          1.0548e+00,  6.3961e-01,  2.6850e-01,  5.9275e-01,  1.3276e+00,
          1.0104e-01,  4.1088e-01, -3.1435e-01,  3.2603e-01, -3.3215e-01,
          3.2694e+00, -2.2436e-01,  6.9167e-02,  3.2299e-01, -9.0835e-01,
         -1.2739e+00, -1.0349e+00,  7.6532e-01,  7.2499e-01,  3.0560e-01,
         -3.1952e-01, -2.2890e-01,  6.3951e-01,  7.7289e-02, -1.0208e-01,
         -8.7849e-01,  1.1870e+00,  7.8377e-01, -1.8962e-01,  1.4940e-01,
         -3.0071e-01,  5.9864e-01, -1.5389e-01, -5.4247e-01, -3.4934e-01,
         -2.3072e-01,  6.7218e-01,  2.0196e-01, -1.3372e+00,  3.6862e-01,
         -4.0789e-01,  4.2295e-01, -3.4080e-01,  1.9408e-01,  4.9969e-01,
          5.5824e-01, -4.7848e-01,  2.0658e+00, -2.3163e-02, -2.8768e-01,
          1.1680e+00,  5.1867e-01,  9.1732e-01, -4.0654e-01,  1.3632e-01,
         -2.6899e-01, -3.9827e-02, -1.7757e-01, -2.6666e-01, -1.4132e+00,
         -7.0022e-01,  1.1015e-01,  6.1303e-01, -1.5691e-01,  3.0138e-01,
          3.2780e-01,  9.2632e-01,  8.9005e-01,  4.8667e-01,  6.1024e-01,
          2.7990e-01,  5.8607e-01,  3.9382e-01,  1.1831e-01, -3.3819e-01,
         -1.9828e+00, -8.4637e-02,  9.1902e-02,  7.1806e-01,  9.8545e-01,
          7.8920e-01, -2.2768e-01, -1.0494e+00,  4.9757e-01, -8.4579e-01,
          1.1767e-01],
        [-8.5410e-01, -1.3653e-02, -5.9994e-01, -3.9260e-01,  8.6629e-01,
          4.4574e-01,  5.8994e-02, -4.4518e-01,  1.6066e-01,  2.8581e-01,
          9.2751e-01,  3.6498e-01,  5.9080e-01,  5.9024e-01,  3.6513e-01,
         -1.9271e-01, -1.1359e+00,  1.1296e+00, -5.9634e-01,  1.7707e-01,
          7.0479e-01,  2.9443e-01,  3.4823e-01, -6.5629e-03, -7.7836e-01,
         -3.9578e-01,  4.6806e-01,  8.2694e-01, -3.4464e-01, -1.6236e+00,
         -6.6032e-02,  3.7356e-01,  1.7833e-01, -5.4454e-01, -8.0832e-01,
         -2.1392e-01, -2.9915e-01, -2.5563e-02,  8.7833e-01,  6.8929e-01,
         -1.0603e+00,  1.8376e+00,  4.4792e-01, -3.0744e+00,  9.8138e-01,
          5.3489e-01,  7.1467e-01, -1.1210e-01,  2.1846e-02,  4.5077e-01,
          2.4371e+00,  1.9617e-01, -1.5992e-01, -3.7091e-01, -8.4181e-01,
          5.7519e-01,  9.3733e-01,  7.2198e-01,  9.3957e-01, -6.8650e-01,
         -2.9789e-01, -2.9121e-01, -7.9973e-01,  5.4766e-01,  3.1243e-01,
          6.3460e-01,  4.6200e-01, -8.5588e-01, -2.1389e-01, -2.5874e-01,
         -1.9357e-02, -1.5903e-01, -7.0800e-01, -2.8155e-01,  6.7967e-01,
          2.8297e-03, -1.0327e+00, -4.2943e-01,  3.8248e-03, -1.8320e-01,
          1.4094e+00, -1.4613e+00, -7.3006e-01,  3.1213e-01, -8.8545e-01,
         -1.0902e+00, -5.3080e-01, -2.5854e-02,  4.6226e-01, -1.9763e-01,
         -1.4980e-02,  5.7094e-01,  3.7040e-01,  3.1193e-01,  5.4305e-01,
          5.5403e-02,  4.3927e-01, -8.1247e-01, -7.4195e-01,  6.5472e-01,
          3.2516e-01,  1.7001e-01,  5.6941e-01, -9.2623e-01, -1.5805e-01,
         -5.5653e-01, -1.5697e+00,  9.5220e-01, -1.7465e+00, -6.4865e-01,
          1.0835e+00,  1.2268e+00, -2.2257e-01, -1.3739e+00, -3.7203e-01,
         -4.4517e-02, -6.1463e-01, -5.5013e-01, -1.7509e-01,  1.3676e-01,
          6.9227e-01,  3.8846e-01,  2.7001e-01,  3.6216e-01,  6.5690e-01,
         -7.4120e-01,  8.6808e-02,  1.2616e+00,  7.2347e-01,  6.1484e-01,
          1.3079e+00, -1.5750e-01, -1.1012e+00,  4.8019e-01, -2.9056e-01,
         -6.3401e-02, -9.3528e-02, -1.4350e+00, -5.2750e-01,  8.2331e-01,
          2.1115e-01,  2.8066e-01,  6.4499e-01,  6.8153e-01,  7.0916e-01,
          1.3536e+00, -5.2912e-01, -2.0708e-01,  1.3237e+00,  3.5245e-01,
          3.7896e-01,  3.6975e-01, -8.0600e-01,  2.7276e+00,  1.6323e+00,
          9.3643e-01, -3.3077e-01,  3.8006e-02,  6.4981e-01,  5.3370e-02,
         -8.6701e-01, -4.8912e-01,  1.3947e-03, -7.4726e-01, -6.1742e-01,
         -9.0252e-02, -5.9249e-01,  1.1394e+00,  8.5479e-01, -5.6317e-01,
         -1.2356e-02, -7.6427e-01, -8.4514e-01, -1.0813e-01,  1.3639e+00,
          1.5924e-01, -8.5771e-01, -3.2660e-01,  7.5353e-01,  2.6395e-01,
         -3.4856e-01,  3.0623e-01, -1.4393e+00,  9.4517e-01,  7.5477e-02,
         -2.2159e+00,  6.3105e-02,  5.3979e-01,  2.9668e-01, -1.2851e+00,
          8.6741e-01, -7.2136e-01,  7.5946e-02, -1.0501e-01, -6.0374e-01,
         -8.5043e-01, -2.0430e-01,  1.4407e+00,  5.0037e-01, -4.4212e-01,
          4.4270e-01,  1.1354e+00, -4.4780e-01,  3.9675e-01,  6.3906e-02,
          2.4338e-01, -3.6491e-01,  5.4376e-01, -7.8556e-01,  2.9817e-01,
          3.8700e-01, -9.5799e-02, -1.2757e+00, -1.0806e+00,  6.3144e-01,
          1.3459e+00,  2.8040e-01, -1.4387e+00,  1.4059e-01, -2.2057e-01,
         -1.2620e+00, -9.9024e-02, -1.3269e+00, -8.0128e-01,  3.2628e-02,
         -3.9028e-02,  4.2750e-01,  2.6276e-01,  3.2275e-01, -3.5256e+00,
          6.9590e-01,  4.7463e-01,  3.0509e-02, -8.9338e-01,  1.1479e+00,
         -1.1521e+00, -1.6299e+00, -7.0114e-01, -8.3033e-01, -6.0437e-02,
         -1.2732e-01,  1.6454e-01,  4.5053e-01,  1.5171e+00, -4.1756e-02,
          2.3419e-01,  2.5649e-01, -5.3035e-01, -2.1445e-01,  9.9852e-01,
          4.9138e-01,  1.1628e-01, -7.1855e-01,  9.3856e-02, -4.6179e-01,
          3.9456e-01]]), 'obj_idxes': tensor([0, 1, 2]), 'matched_gt_idxes': tensor([-1, -1, -1]), 'disappear_time': tensor([0, 0, 0]), 'iou': tensor([0., 0., 0.]), 'scores': tensor([0.8401, 0.8999, 0.8660]), 'track_scores': tensor([0., 0., 0.]), 'pred_boxes': tensor([[-4.4362e+00, -1.0457e+01, -5.7496e-01,  5.4815e-01, -1.9604e+00,
          8.2555e-02,  2.6890e-02,  9.9731e-01, -1.2011e-03, -9.9059e-04],
        [-7.5262e-01, -2.4579e+01,  6.6525e-01,  1.5121e+00, -2.2621e+00,
          4.3397e-01, -1.8351e-02, -9.8844e-01,  8.8571e-02,  5.5166e+00],
        [-1.2292e+01,  3.5435e+01, -3.3134e-01, -2.0525e-01, -9.5619e-01,
          5.5846e-01,  1.1792e-01, -9.8280e-01, -2.9149e-01,  1.2141e+00]]), 'pred_logits': tensor([[ -6.4774,  -8.0849,  -7.5573, -11.3790,  -6.8656,  -5.9882,  -2.8453,
           1.6590,  -5.5511,  -4.6095],
        [  2.1958,  -6.2505,  -7.5734,  -7.5439, -13.5229,  -6.9315,  -7.1278,
          -9.5285,  -7.1040,  -6.2322],
        [ -8.0861,  -7.3184,  -7.0678,  -7.8600,  -6.4306,  -4.7218,  -6.1443,
          -5.5766,   1.8664,  -5.3478]]), 'mem_bank': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), 'mem_padding_mask': tensor([[True, True, True, True],
        [True, True, True, True],
        [True, True, True, True]]), 'save_period': tensor([0., 0., 0.])}
(Pdb) dict_keys(['ref_pts', 'query', 'output_embedding', 'obj_idxes', 'matched_gt_idxes', 'disappear_time', 'iou', 'scores', 'track_scores', 'pred_boxes', 'pred_logits', 'mem_bank', 'mem_padding_mask', 'save_period'])
(Pdb) tensor([0, 1, 2])
(Pdb) tensor([0.8401, 0.8999, 0.8660])
(Pdb) tensor([0., 0., 0.])
(Pdb) tensor([0., 0., 0.])
(Pdb) tensor([[-4.4362e+00, -1.0457e+01, -5.7496e-01,  5.4815e-01, -1.9604e+00,
          8.2555e-02,  2.6890e-02,  9.9731e-01, -1.2011e-03, -9.9059e-04],
        [-7.5262e-01, -2.4579e+01,  6.6525e-01,  1.5121e+00, -2.2621e+00,
          4.3397e-01, -1.8351e-02, -9.8844e-01,  8.8571e-02,  5.5166e+00],
        [-1.2292e+01,  3.5435e+01, -3.3134e-01, -2.0525e-01, -9.5619e-01,
          5.5846e-01,  1.1792e-01, -9.8280e-01, -2.9149e-01,  1.2141e+00]])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(148)__getitem__()->Instances(num...0., 0., 0.])])
-> return ret
(Pdb) *** NameError: name 'ret_list' is not defined
(Pdb) --Call--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(780)_track_instances2results()
-> def _track_instances2results(self, track_instances, img_metas, with_mask=True):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(781)_track_instances2results()
-> bbox_dict = dict(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(782)_track_instances2results()
-> cls_scores=track_instances.pred_logits,
(Pdb) torch.Size([3, 10])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(783)_track_instances2results()
-> bbox_preds=track_instances.pred_boxes,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(784)_track_instances2results()
-> track_scores=track_instances.scores,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(785)_track_instances2results()
-> obj_idxes=track_instances.obj_idxes,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(781)_track_instances2results()
-> bbox_dict = dict(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(788)_track_instances2results()
-> bboxes_dict = self.bbox_coder.decode(bbox_dict, with_mask=with_mask, img_metas=img_metas)[0]
(Pdb) --Call--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(129)decode()
-> def decode(self, preds_dicts, with_mask=True, img_metas=None):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(143)decode()
-> all_cls_scores = preds_dicts['cls_scores']
(Pdb) False
(Pdb) torch.Size([3, 10])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(788)_track_instances2results()
-> bboxes_dict = self.bbox_coder.decode(bbox_dict, with_mask=with_mask, img_metas=img_metas)[0]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(481)select_active_track_query()
-> result_dict = self._track_instances2results(track_instances[active_index], img_metas, with_mask=with_mask)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(788)_track_instances2results()
-> bboxes_dict = self.bbox_coder.decode(bbox_dict, with_mask=with_mask, img_metas=img_metas)[0]
(Pdb) 783  	            bbox_preds=track_instances.pred_boxes,
784  	            track_scores=track_instances.scores,
785  	            obj_idxes=track_instances.obj_idxes,
786  	        )
787  	        # bboxes_dict = self.bbox_coder.decode(bbox_dict, with_mask=with_mask)[0]
788  ->	        bboxes_dict = self.bbox_coder.decode(bbox_dict, with_mask=with_mask, img_metas=img_metas)[0]
789  	        bboxes = bboxes_dict["bboxes"]
790  	        # bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 5] * 0.5
791  	        bboxes = img_metas[0]["box_type_3d"](bboxes, 9)
792  	        labels = bboxes_dict["labels"]
793  	        scores = bboxes_dict["scores"]
(Pdb) dict_keys(['cls_scores', 'bbox_preds', 'track_scores', 'obj_idxes'])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(143)decode()
-> all_cls_scores = preds_dicts['cls_scores']
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(144)decode()
-> all_bbox_preds = preds_dicts['bbox_preds']
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(145)decode()
-> track_scores = preds_dicts['track_scores']
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(146)decode()
-> obj_idxes = preds_dicts['obj_idxes']
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(148)decode()
-> batch_size = all_cls_scores.size()[0]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(149)decode()
-> predictions_list = []
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(151)decode()
-> predictions_list.append(self.decode_single(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(152)decode()
-> all_cls_scores, all_bbox_preds,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(153)decode()
-> track_scores, obj_idxes, with_mask, img_metas))
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(151)decode()
-> predictions_list.append(self.decode_single(
(Pdb) --Call--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(42)decode_single()
-> def decode_single(self, cls_scores, bbox_preds,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(56)decode_single()
-> max_num = self.max_num
(Pdb) 300
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(57)decode_single()
-> max_num = min(cls_scores.size(0), self.max_num)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(59)decode_single()
-> cls_scores = cls_scores.sigmoid()
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(60)decode_single()
-> _, indexs = cls_scores.max(dim=-1)
(Pdb) torch.Size([3, 10])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(61)decode_single()
-> labels = indexs % self.num_classes
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(63)decode_single()
-> _, bbox_index = track_scores.topk(max_num)
(Pdb) torch.Size([3])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(65)decode_single()
-> labels = labels[bbox_index]
(Pdb) tensor([1, 2, 0])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(66)decode_single()
-> bbox_preds = bbox_preds[bbox_index]
(Pdb) torch.Size([3])
(Pdb) tensor([0, 8, 7])
(Pdb) tensor([[-4.4362e+00, -1.0457e+01, -5.7496e-01,  5.4815e-01, -1.9604e+00,
          8.2555e-02,  2.6890e-02,  9.9731e-01, -1.2011e-03, -9.9059e-04],
        [-7.5262e-01, -2.4579e+01,  6.6525e-01,  1.5121e+00, -2.2621e+00,
          4.3397e-01, -1.8351e-02, -9.8844e-01,  8.8571e-02,  5.5166e+00],
        [-1.2292e+01,  3.5435e+01, -3.3134e-01, -2.0525e-01, -9.5619e-01,
          5.5846e-01,  1.1792e-01, -9.8280e-01, -2.9149e-01,  1.2141e+00]])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(67)decode_single()
-> track_scores = track_scores[bbox_index]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(68)decode_single()
-> obj_idxes = obj_idxes[bbox_index]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(70)decode_single()
-> scores = track_scores
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(72)decode_single()
-> final_box_preds = denormalize_bbox(bbox_preds, self.pc_range)
(Pdb) --Call--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/util.py(27)denormalize_bbox()
-> def denormalize_bbox(normalized_bboxes, pc_range):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/util.py(29)denormalize_bbox()
-> rot_sine = normalized_bboxes[..., 6:7]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/util.py(31)denormalize_bbox()
-> rot_cosine = normalized_bboxes[..., 7:8]
(Pdb) torch.Size([3, 10])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/util.py(32)denormalize_bbox()
-> rot = torch.atan2(rot_sine, rot_cosine)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/util.py(35)denormalize_bbox()
-> cx = normalized_bboxes[..., 0:1]
(Pdb) tensor([[-3.1230],
        [ 3.0222],
        [ 0.0270]])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/util.py(36)denormalize_bbox()
-> cy = normalized_bboxes[..., 1:2]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/util.py(37)denormalize_bbox()
-> cz = normalized_bboxes[..., 4:5]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/util.py(40)denormalize_bbox()
-> w = normalized_bboxes[..., 2:3]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/util.py(41)denormalize_bbox()
-> l = normalized_bboxes[..., 3:4]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/util.py(42)denormalize_bbox()
-> h = normalized_bboxes[..., 5:6]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/util.py(44)denormalize_bbox()
-> w = w.exp()
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/util.py(45)denormalize_bbox()
-> l = l.exp()
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/util.py(46)denormalize_bbox()
-> h = h.exp()
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/util.py(47)denormalize_bbox()
-> if normalized_bboxes.size(-1) > 8:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/util.py(49)denormalize_bbox()
-> vx = normalized_bboxes[:, 8:9]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/util.py(50)denormalize_bbox()
-> vy = normalized_bboxes[:, 9:10]
(Pdb) tensor([[ 0.0886],
        [-0.2915],
        [-0.0012]])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/util.py(51)denormalize_bbox()
-> denormalized_bboxes = torch.cat([cx, cy, cz, w, l, h, rot, vx, vy], dim=-1)
(Pdb) tensor([[ 5.5166e+00],
        [ 1.2141e+00],
        [-9.9059e-04]])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/util.py(54)denormalize_bbox()
-> return denormalized_bboxes
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/util.py(54)denormalize_bbox()->tensor([[-7.5...-9.9059e-04]])
-> return denormalized_bboxes
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(73)decode_single()
-> final_scores = track_scores
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(74)decode_single()
-> final_preds = labels
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(77)decode_single()
-> if self.score_threshold is not None:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(78)decode_single()
-> thresh_mask = final_scores > self.score_threshold
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(80)decode_single()
-> if self.with_nms:
(Pdb) tensor([True, True, True])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(93)decode_single()
-> if self.post_center_range is not None:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(94)decode_single()
-> self.post_center_range = torch.tensor(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(95)decode_single()
-> self.post_center_range, device=scores.device)
(Pdb) [-61.2, -61.2, -10.0, 61.2, 61.2, 10.0]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(94)decode_single()
-> self.post_center_range = torch.tensor(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(96)decode_single()
-> mask = (final_box_preds[..., :3] >=
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(97)decode_single()
-> self.post_center_range[:3]).all(1)
(Pdb) *** NameError: name 'mask' is not defined
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(96)decode_single()
-> mask = (final_box_preds[..., :3] >=
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(97)decode_single()
-> self.post_center_range[:3]).all(1)
(Pdb) *** NameError: name 'mask' is not defined
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(96)decode_single()
-> mask = (final_box_preds[..., :3] >=
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(98)decode_single()
-> mask &= (final_box_preds[..., :3] <=
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(99)decode_single()
-> self.post_center_range[3:]).all(1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(98)decode_single()
-> mask &= (final_box_preds[..., :3] <=
(Pdb) tensor([True, True, True])
(Pdb) *** NameError: name 'nn' is not defined
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(99)decode_single()
-> self.post_center_range[3:]).all(1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(98)decode_single()
-> mask &= (final_box_preds[..., :3] <=
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(101)decode_single()
-> if self.score_threshold:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(103)decode_single()
-> if not with_mask:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(105)decode_single()
-> if self.with_nms:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(108)decode_single()
-> boxes3d = final_box_preds[mask]
(Pdb) tensor([True, True, True])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(109)decode_single()
-> scores = final_scores[mask]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(110)decode_single()
-> labels = final_preds[mask]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(111)decode_single()
-> track_scores = track_scores[mask]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(112)decode_single()
-> obj_idxes = obj_idxes[mask]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(114)decode_single()
-> 'bboxes': boxes3d,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(115)decode_single()
-> 'scores': scores,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(116)decode_single()
-> 'labels': labels,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(117)decode_single()
-> 'track_scores': track_scores,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(118)decode_single()
-> 'obj_idxes': obj_idxes,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(119)decode_single()
-> 'bbox_index': bbox_index,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(120)decode_single()
-> 'mask': mask
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(113)decode_single()
-> predictions_dict = {
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(127)decode_single()
-> return predictions_dict
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(127)decode_single()->{'bbox_index': tensor([1, 2, 0]), 'bboxes': tensor([[-7.5...-9.9059e-04]]), 'labels': tensor([0, 8, 7]), 'mask': tensor([True, True, True]), ...}
-> return predictions_dict
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(156)decode()
-> return predictions_list
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(156)decode()->[{'bbox_index': tensor([1, 2, 0]), 'bboxes': tensor([[-7.5...-9.9059e-04]]), 'labels': tensor([0, 8, 7]), 'mask': tensor([True, True, True]), ...}]
-> return predictions_list
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(789)_track_instances2results()
-> bboxes = bboxes_dict["bboxes"]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(791)_track_instances2results()
-> bboxes = img_metas[0]["box_type_3d"](bboxes, 9)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(792)_track_instances2results()
-> labels = bboxes_dict["labels"]
(Pdb) LiDARInstance3DBoxes(
    tensor([[-7.5262e-01, -2.4579e+01, -2.2621e+00,  1.9450e+00,  4.5364e+00,
          1.5434e+00, -3.1230e+00,  8.8571e-02,  5.5166e+00],
        [-1.2292e+01,  3.5435e+01, -9.5619e-01,  7.1796e-01,  8.1444e-01,
          1.7480e+00,  3.0222e+00, -2.9149e-01,  1.2141e+00],
        [-4.4362e+00, -1.0457e+01, -1.9604e+00,  5.6273e-01,  1.7301e+00,
          1.0861e+00,  2.6956e-02, -1.2011e-03, -9.9059e-04]]))
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(793)_track_instances2results()
-> scores = bboxes_dict["scores"]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(794)_track_instances2results()
-> bbox_index = bboxes_dict["bbox_index"]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(796)_track_instances2results()
-> track_scores = bboxes_dict["track_scores"]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(797)_track_instances2results()
-> obj_idxes = bboxes_dict["obj_idxes"]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(798)_track_instances2results()
-> result_dict = dict(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(799)_track_instances2results()
-> boxes_3d=bboxes.to("cpu"),
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(800)_track_instances2results()
-> scores_3d=scores.cpu(),
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(801)_track_instances2results()
-> labels_3d=labels.cpu(),
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(802)_track_instances2results()
-> track_scores=track_scores.cpu(),
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(803)_track_instances2results()
-> bbox_index=bbox_index.cpu(),
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(804)_track_instances2results()
-> track_ids=obj_idxes.cpu(),
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(805)_track_instances2results()
-> mask=bboxes_dict["mask"].cpu(),
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(806)_track_instances2results()
-> track_bbox_results=[[bboxes.to("cpu"), scores.cpu(), labels.cpu(), bbox_index.cpu(), bboxes_dict["mask"].cpu()]]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(798)_track_instances2results()
-> result_dict = dict(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(808)_track_instances2results()
-> return result_dict
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(808)_track_instances2results()->{'bbox_index': tensor([1, 2, 0]), 'boxes_3d': LiDARInstance...9.9059e-04]])), 'labels_3d': tensor([0, 8, 7]), 'mask': tensor([True, True, True]), ...}
-> return result_dict
(Pdb) LiDARInstance3DBoxes(
    tensor([[-7.5262e-01, -2.4579e+01, -2.2621e+00,  1.9450e+00,  4.5364e+00,
          1.5434e+00, -3.1230e+00,  8.8571e-02,  5.5166e+00],
        [-1.2292e+01,  3.5435e+01, -9.5619e-01,  7.1796e-01,  8.1444e-01,
          1.7480e+00,  3.0222e+00, -2.9149e-01,  1.2141e+00],
        [-4.4362e+00, -1.0457e+01, -1.9604e+00,  5.6273e-01,  1.7301e+00,
          1.0861e+00,  2.6956e-02, -1.2011e-03, -9.9059e-04]]))
(Pdb) <class 'projects.mmdet3d_plugin.core.bbox.lidar_box3d.LiDARInstance3DBoxes'>
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(482)select_active_track_query()
-> result_dict["track_query_embeddings"] = track_instances.output_embedding[active_index][result_dict['bbox_index']][result_dict['mask']]
(Pdb) 477  	        out["track_instances"] = out_track_instances
478  	        return out
479  	
480  	    def select_active_track_query(self, track_instances, active_index, img_metas, with_mask=True):
481  	        result_dict = self._track_instances2results(track_instances[active_index], img_metas, with_mask=with_mask)
482  ->	        result_dict["track_query_embeddings"] = track_instances.output_embedding[active_index][result_dict['bbox_index']][result_dict['mask']]
483  	        result_dict["track_query_matched_idxes"] = track_instances.matched_gt_idxes[active_index][result_dict['bbox_index']][result_dict['mask']]
484  	        return result_dict
485  	
486  	    def select_sdc_track_query(self, sdc_instance, img_metas):
487  	        out = dict()
(Pdb) torch.Size([901, 256])
(Pdb) tensor([True, True, True])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(483)select_active_track_query()
-> result_dict["track_query_matched_idxes"] = track_instances.matched_gt_idxes[active_index][result_dict['bbox_index']][result_dict['mask']]
(Pdb) torch.Size([3, 256])
(Pdb) torch.Size([901])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(484)select_active_track_query()
-> return result_dict
(Pdb) torch.Size([3])
(Pdb) tensor([-1, -1, -1])
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(484)select_active_track_query()->{'bbox_index': tensor([1, 2, 0]), 'boxes_3d': LiDARInstance...9.9059e-04]])), 'labels_3d': tensor([0, 8, 7]), 'mask': tensor([True, True, True]), ...}
-> return result_dict
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(696)_forward_single_frame_inference()
-> out.update(self.select_sdc_track_query(track_instances[track_instances.obj_idxes==-2], img_metas))
(Pdb) --Call--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(54)__getattr__()
-> def __getattr__(self, name: str) -> Any:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(55)__getattr__()
-> if name == "_fields" or name not in self._fields:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(57)__getattr__()
-> return self._fields[name]
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(57)__getattr__()->tensor([-1, -...,
        -2])
-> return self._fields[name]
(Pdb) --Call--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(120)__getitem__()
-> def __getitem__(self, item: Union[int, slice, torch.BoolTensor]) -> "Instances":
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(128)__getitem__()
-> if type(item) == int:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(134)__getitem__()
-> ret = Instances(self._image_size)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) Instances(num_instances=1, image_height=1, image_width=1, fields=[ref_pts: tensor([[ 0.0012, -0.0024,  0.1136]]), query: tensor([[ 1.1220e+00, -1.8478e+00,  7.0555e-01, -3.1391e-01,  3.5123e-01,
          6.1337e-01, -4.0798e-01, -7.4832e-01,  4.8113e-01, -6.3264e-01,
         -7.1372e-01, -2.1515e+00,  1.3269e+00, -9.0078e-01, -2.3208e-01,
          3.9646e-01,  2.1189e-01, -2.1993e-02,  6.5558e-01,  6.2625e-02,
          3.6630e-01,  9.6408e-02, -1.0213e+00, -2.2153e-01,  1.5129e+00,
          6.5254e-01,  4.9937e-01, -1.0994e+00,  1.4195e+00, -1.7417e+00,
          4.5437e-01,  1.6214e+00,  1.6305e+00, -1.1432e+00, -6.6659e-01,
         -1.9926e-02,  1.5728e+00, -5.9254e-01, -2.3218e-01,  5.4649e-01,
         -2.3996e-01,  2.5129e-01,  1.8442e-01, -6.1771e-01, -1.0216e-01,
          1.3565e+00,  6.9384e-01, -1.2760e+00, -5.5984e-01, -2.3471e+00,
          4.6810e-01, -1.0278e+00, -1.0436e+00,  1.2780e+00, -3.0014e-01,
          2.6047e-01, -1.0130e+00,  3.9432e-01,  1.1288e+00,  1.3000e+00,
         -3.4322e-01,  3.1912e-01, -3.2916e-01, -6.5754e-01,  1.3322e-01,
          1.4872e+00,  3.3406e-01, -8.5896e-01,  5.6429e-02, -1.2782e+00,
          1.4471e+00, -6.7698e-01,  2.7346e-01, -1.6382e+00,  2.2102e-02,
         -4.6719e-01, -5.7070e-01,  1.7668e+00,  5.8053e-01, -3.6928e-01,
         -7.9056e-01, -1.0566e-01,  2.3034e-01, -1.5379e+00, -5.3714e-02,
         -2.0413e+00, -3.2396e-02,  7.0850e-01,  3.4245e-01, -2.0712e-01,
          9.6050e-02, -2.5181e-01, -9.6619e-01, -5.9346e-01,  2.6988e-01,
         -1.6355e+00, -8.9870e-01,  1.7788e+00, -1.1164e-01,  9.1774e-01,
         -1.2001e+00, -1.5393e+00, -1.5818e-01, -1.8646e+00, -9.3066e-01,
          7.6793e-01,  9.9613e-01, -1.3722e+00, -1.3500e+00,  6.8103e-01,
          4.1190e-01, -1.8488e+00,  7.0303e-01, -9.0148e-01,  8.5878e-01,
          8.7450e-01, -4.8325e-02, -5.2769e-02, -7.8688e-02, -5.2092e-01,
         -1.4312e-01, -2.0614e+00, -3.7743e-01, -1.3410e-01, -3.3978e-01,
          1.2877e+00,  3.4951e-01,  6.0755e-01,  1.9015e-01, -1.8678e+00,
          7.5514e-01, -1.8589e-01, -4.5788e-01,  1.4513e+00,  1.7880e+00,
         -1.2963e-01, -9.5089e-01, -1.2412e-01,  4.7011e-01,  1.9193e+00,
         -1.1242e+00, -1.0321e+00,  5.5186e-01, -1.6518e-01, -9.9210e-01,
         -8.8201e-01,  2.5959e-01, -1.4807e+00,  1.1453e+00, -5.7856e-01,
         -5.5256e-01,  1.3698e+00, -7.8244e-01, -6.0015e-01, -1.5487e+00,
          1.0693e+00,  3.3249e-01, -8.6826e-01,  1.0084e+00, -4.5956e-01,
         -1.6148e+00, -1.7401e-01, -1.9187e+00,  2.4855e-01,  5.5284e-01,
          1.4573e+00,  3.2613e-02, -1.0722e+00,  4.5457e-01,  6.7663e-01,
          9.5070e-01,  1.9298e+00,  1.4436e-03,  4.6063e-01, -1.5001e+00,
          7.4149e-01, -6.0796e-01,  7.3927e-01, -8.2667e-01, -4.9717e-01,
         -2.3809e-01, -1.3273e+00,  6.6118e-01,  1.3817e+00,  8.1525e-01,
         -8.8447e-01,  1.3811e-01, -6.8249e-01, -4.1958e-01, -2.3332e+00,
         -1.1105e+00,  9.4233e-01,  5.3776e-01,  5.3802e-01, -7.0783e-01,
          6.5302e-01,  3.8073e-01,  2.6281e-01, -6.2956e-01,  1.1421e+00,
         -1.3057e+00, -6.9222e-01,  1.0418e+00,  1.5223e+00, -3.0123e-01,
         -9.7432e-01,  1.5428e+00,  5.6129e-01, -1.1917e-01, -7.8261e-01,
         -8.7698e-01,  1.5460e+00,  8.2454e-01, -1.5860e+00, -8.0340e-01,
          8.1813e-01,  1.3133e+00,  1.1080e+00,  1.0372e+00,  2.2219e-01,
         -2.0161e-01, -1.4155e-01,  5.3216e-01, -9.6410e-02,  1.7948e+00,
         -1.7062e-01, -1.4997e-03, -1.3744e+00,  5.2533e-01,  1.1247e+00,
         -1.0739e+00, -5.3848e-01, -1.0358e+00, -4.1634e-01, -1.0192e+00,
          8.0389e-01, -2.3628e-01,  3.1780e-02, -2.0364e-01,  2.5836e-01,
         -1.9524e+00, -1.1520e+00, -1.2636e+00,  7.4585e-01,  1.3198e-01,
         -1.4250e+00,  1.8459e+00, -5.7148e-01, -1.1567e+00, -4.0067e-01,
         -6.2902e-01, -1.1415e+00,  5.4868e-01,  7.8087e-01,  7.5292e-02,
         -2.9564e-01,  6.5217e-01, -4.6224e-01,  1.5101e-01,  8.5021e-01,
          2.9873e-01, -1.1355e+00,  1.6372e+00, -3.1092e-02,  5.3752e-01,
          1.4604e+00, -1.4555e+00,  1.1979e+00, -7.8614e-01,  6.2306e-01,
          2.0207e+00, -2.2010e-01,  1.0438e-01, -7.3968e-02, -8.1986e-01,
          1.3701e+00,  6.1866e-02,  1.5265e+00, -4.5773e-01, -3.1278e-01,
         -3.8263e-01, -1.7958e+00,  5.7462e-02, -1.0191e+00,  3.0819e-01,
          4.2824e-02, -2.1576e+00, -8.2004e-02, -2.0147e-01,  7.1641e-01,
          4.3848e-01,  7.3543e-01,  5.8282e-01,  2.3244e+00, -9.7639e-01,
          1.9591e-01,  4.4259e-01,  8.9500e-02, -1.0641e+00,  1.2207e+00,
          7.0560e-02, -5.7831e-01,  9.0717e-01, -3.2226e-01, -1.4384e+00,
         -7.6588e-01,  8.8581e-01,  5.0879e-01, -7.4768e-01,  8.3173e-01,
         -1.5212e+00,  1.4653e+00, -8.7602e-02, -2.3463e-01, -1.0277e+00,
          4.7738e-01,  5.1060e-01, -1.9068e+00,  3.2738e-01, -3.8155e-01,
          6.8094e-01,  1.1442e+00, -2.3083e-01,  2.7956e-02, -2.0416e+00,
         -4.2927e-01,  3.0954e-01, -2.0644e+00,  4.3077e-01, -1.9746e-01,
          2.7377e-01,  8.6149e-01, -1.4293e+00, -1.6919e+00, -9.3549e-01,
         -7.0867e-01, -2.6552e-02,  2.1637e-01,  1.1796e+00, -1.0206e-01,
          1.2719e-01,  3.3238e-01, -8.9301e-02,  9.3436e-01, -1.1423e-01,
          2.1710e+00,  1.6724e+00,  1.4248e+00, -7.0311e-02, -2.6874e-01,
          7.8724e-01, -1.2645e-01,  2.6997e-01,  2.2734e-02,  3.6460e-01,
         -3.5762e-02, -7.9331e-01, -1.1521e+00, -8.4928e-02, -8.8407e-02,
         -5.4222e-01, -1.0364e+00, -3.2266e-01,  1.0855e+00, -8.1322e-02,
          1.3894e+00, -7.4104e-03, -8.3995e-01,  1.8148e+00,  2.1664e-01,
          8.2385e-01,  3.0077e-01, -1.3889e+00, -1.2663e-01,  5.2896e-01,
          5.8481e-01,  2.0698e+00,  1.8139e-01, -5.1590e-01,  1.8764e-01,
         -7.9362e-01, -4.2889e-02, -3.6977e-01,  5.3196e-01,  9.1258e-01,
          3.3203e-01,  2.6107e-01,  1.1895e+00,  5.8136e-01,  7.8030e-01,
         -4.4771e-01,  2.5248e-01, -1.2125e+00,  8.5201e-01,  6.2615e-01,
         -4.6464e-01, -4.6168e-01,  6.3012e-01, -1.3001e+00, -4.7600e-01,
         -1.8405e+00,  1.5082e+00, -1.2966e+00, -1.2550e+00,  6.6496e-02,
         -5.9417e-01, -1.2807e-01,  1.5602e+00, -1.0811e-01,  8.9084e-01,
         -3.6435e-01,  1.1567e+00, -1.5760e+00, -3.8553e-02, -4.8042e-01,
         -2.4266e-01, -2.8390e-01,  6.0504e-01,  4.9040e-01,  3.4005e-02,
         -1.5020e-01, -5.3679e-01, -5.2114e-02,  1.0685e-02,  6.2674e-01,
         -3.6176e-02, -8.4974e-01, -8.6500e-02,  6.7058e-01, -1.6868e+00,
          1.2242e-01,  1.6896e+00,  5.2232e-01,  9.0551e-01, -1.3200e+00,
          4.3703e-01,  1.8856e+00,  2.6453e-01,  1.9155e+00,  7.3929e-01,
          2.0273e-01,  1.6746e-01, -1.1358e+00, -1.5308e+00,  1.7469e-01,
          1.5851e+00, -8.5155e-01, -8.3011e-01,  6.5800e-01,  6.4055e-01,
         -1.0305e+00,  1.7110e+00, -1.0026e+00,  6.8704e-02,  2.1984e-01,
          7.1450e-01,  1.0368e+00, -8.3135e-01,  1.9949e-01, -8.9561e-01,
          6.0898e-02,  5.6567e-01,  1.4009e+00, -1.6122e+00,  9.0249e-01,
          9.6149e-01, -2.6620e-01,  4.4834e-01, -1.0726e+00, -1.3024e+00,
          1.9978e+00,  7.9052e-01, -6.9631e-01, -1.1775e-01, -5.4189e-01,
          1.3945e-01, -6.1785e-01,  1.8642e-01, -4.2044e-01,  8.3601e-01,
          2.2372e+00, -5.9374e-01,  1.1521e+00,  1.0014e+00,  2.3731e-01,
         -9.2328e-01, -6.1568e-01,  6.7766e-01, -1.2004e+00,  2.7637e-01,
          9.3837e-01,  3.1301e-01,  1.9548e+00, -6.0744e-01,  5.1339e-01,
          1.1864e+00,  1.1217e-01,  6.8392e-01,  2.1476e+00,  1.0936e+00,
         -4.4918e-01,  6.7367e-01,  1.9326e+00, -9.5147e-01,  1.4895e+00,
          1.3305e-01,  8.8955e-01, -6.4028e-01, -2.0680e+00,  2.9896e+00,
          3.2782e-01,  1.9336e+00]]), output_embedding: tensor([[-1.8255e-01, -4.0788e-01,  1.6076e-01, -2.5198e-01, -8.8826e-01,
          1.4533e-01, -7.3903e-01, -4.7802e-01, -6.8017e-01,  5.4484e-01,
         -8.3872e-01,  1.7665e+00,  8.4522e-02,  2.3097e-02,  6.8516e-01,
         -7.0446e-01, -3.8165e-01,  6.5782e-01,  8.0665e-02,  3.4668e-01,
         -5.3021e-01, -1.6320e-01, -1.1719e-01,  5.9696e-01, -1.2922e+00,
          3.3372e-01, -3.2954e-02,  7.2119e-01,  1.8851e-01, -7.6756e-01,
         -2.7816e-01, -4.3089e-01, -3.2760e-01, -2.6845e-01, -3.7992e-01,
         -8.8505e-01,  9.3222e-03,  3.9554e-01,  1.3592e+00,  4.8343e-01,
         -7.1763e-01,  4.7244e-01,  4.9921e-01, -8.5018e-01, -4.7007e-01,
         -3.1709e-01, -5.5385e-01,  1.5694e-01, -9.4155e-01,  3.2117e-01,
          3.5412e-01, -1.5624e-01,  9.4912e-01, -7.1007e-01,  4.5652e-02,
          4.1129e-01,  1.2175e+00,  2.9892e-02,  2.5776e-01, -6.1032e-01,
         -5.2767e-01,  1.1862e-01,  4.0411e-01, -5.6977e-02, -3.4231e-01,
         -6.8259e-01,  1.1710e-01,  9.0227e-01,  5.8681e-01, -3.4236e-01,
          3.0796e-01,  8.2736e-02,  5.7098e-01,  8.1015e-01, -8.4607e-01,
         -4.2826e-01, -1.0027e+00, -7.9148e-01, -2.0352e-01, -4.7955e-02,
          3.9877e-01, -3.4746e-01,  3.5791e-01, -9.9465e-01, -4.6455e-01,
         -6.2758e-01,  6.1228e-01,  4.8205e-01,  2.0930e+00,  1.0554e+00,
          1.9314e-01,  3.8599e-01,  5.0611e-01,  7.6208e-01, -8.8878e-02,
         -3.4405e-01,  8.6539e-01, -3.6863e-01,  4.0994e-01, -2.5762e-01,
          5.3220e-01, -1.4100e-01,  3.8227e-01,  1.5783e+00, -4.5233e-01,
          2.7012e-01, -9.5246e-01,  1.8134e+00, -2.8061e+00, -7.0744e-03,
          1.1461e+00,  5.9902e-01, -9.3717e-01, -5.3895e-01,  5.4251e-01,
         -4.4150e-01, -5.2048e-03,  8.6503e-01,  6.3065e-01,  7.6411e-01,
         -2.4348e-01, -6.1991e-01, -6.2541e-02,  9.0314e-01,  9.2742e-01,
         -1.5580e+00, -9.9327e-01,  1.1429e+00, -2.7526e-01, -4.5104e-01,
          5.5858e-01,  1.2172e-01, -7.3936e-01,  1.6622e-01,  4.3890e-01,
         -4.2702e-01, -4.4180e-01, -1.3739e+00, -1.3896e-01, -5.9950e-02,
          1.1110e+00,  1.0591e+00, -7.2583e-02,  1.0202e+00,  1.3183e+00,
          3.0808e-01, -1.8869e-01,  2.0228e-01, -1.4433e+00, -4.4621e-01,
         -1.7324e-01, -5.8355e-01, -1.2615e+00,  3.3693e+00,  3.4422e-01,
          6.7722e-01, -1.6634e-01,  1.6876e-01, -9.7153e-01, -1.0274e+00,
         -4.4053e-01,  2.0086e-01, -7.2813e-01, -2.3412e-01, -7.8844e-01,
         -4.1144e-01, -9.2376e-01,  1.1608e+00, -1.9629e-01,  2.7943e-01,
          2.0589e-01, -2.6516e-01, -4.3682e-01,  4.6792e-01,  3.6933e-01,
         -3.0362e-01,  1.2336e+00, -7.5077e-02,  1.6869e-01,  2.9298e-01,
          3.8768e+00, -2.6374e+00, -2.9200e-02,  2.9550e-01, -6.4243e-01,
         -1.1066e+00, -6.0562e-01,  8.7127e-01,  9.5174e-01, -1.1741e+00,
          8.3490e-01,  2.8753e-02,  3.6931e-01,  2.0665e-01,  2.6432e-01,
         -2.3390e-01, -1.5537e+00,  5.9100e-01,  2.3830e-01, -4.1695e-01,
          1.0490e-01,  5.8710e-01, -2.6665e-01, -1.6326e-01, -2.9972e+00,
          7.8470e-01,  4.9015e-01, -1.4249e-01, -5.4056e-01,  1.1162e-01,
         -2.7971e-01, -2.1713e-01,  3.0590e-01, -1.0733e+00,  9.5212e-01,
         -9.0251e-01,  4.4069e-01,  1.6808e+00, -9.2984e-01,  8.1391e-01,
         -1.0787e-01, -2.6859e-01,  2.9374e-01, -9.1996e-01, -2.3097e-02,
          6.7745e-01, -2.8411e-01,  4.5438e-01,  1.5170e-01, -2.0155e+00,
         -7.1402e-01,  1.2454e-01, -3.1142e-01, -9.0900e-02,  3.8416e-01,
         -3.3357e-01,  3.1400e-01, -9.7673e-01, -5.5772e-01, -7.0417e-01,
         -2.3285e-01,  6.2322e-01,  2.2156e-01,  9.1579e-01,  3.5369e-03,
         -1.3154e+00,  5.6757e-03,  1.0759e-01,  1.2531e+00,  6.3730e-01,
          2.3391e-02, -2.1760e-01, -1.1546e-01,  2.5269e-01, -3.3619e-01,
          8.2279e-01]]), obj_idxes: tensor([-2]), matched_gt_idxes: tensor([-1])])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) 'pred_boxes'
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(138)__getitem__()
-> if k == 'kalman_models' and isinstance(item, torch.Tensor):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(147)__getitem__()
-> ret.set(k, v[item])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(135)__getitem__()
-> for k, v in self._fields.items():
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(148)__getitem__()
-> return ret
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/track_instance.py(148)__getitem__()->Instances(num...tensor([0.])])
-> return ret
(Pdb) Instances(num_instances=1, image_height=1, image_width=1, fields=[ref_pts: tensor([[ 0.0012, -0.0024,  0.1136]]), query: tensor([[ 1.1220e+00, -1.8478e+00,  7.0555e-01, -3.1391e-01,  3.5123e-01,
          6.1337e-01, -4.0798e-01, -7.4832e-01,  4.8113e-01, -6.3264e-01,
         -7.1372e-01, -2.1515e+00,  1.3269e+00, -9.0078e-01, -2.3208e-01,
          3.9646e-01,  2.1189e-01, -2.1993e-02,  6.5558e-01,  6.2625e-02,
          3.6630e-01,  9.6408e-02, -1.0213e+00, -2.2153e-01,  1.5129e+00,
          6.5254e-01,  4.9937e-01, -1.0994e+00,  1.4195e+00, -1.7417e+00,
          4.5437e-01,  1.6214e+00,  1.6305e+00, -1.1432e+00, -6.6659e-01,
         -1.9926e-02,  1.5728e+00, -5.9254e-01, -2.3218e-01,  5.4649e-01,
         -2.3996e-01,  2.5129e-01,  1.8442e-01, -6.1771e-01, -1.0216e-01,
          1.3565e+00,  6.9384e-01, -1.2760e+00, -5.5984e-01, -2.3471e+00,
          4.6810e-01, -1.0278e+00, -1.0436e+00,  1.2780e+00, -3.0014e-01,
          2.6047e-01, -1.0130e+00,  3.9432e-01,  1.1288e+00,  1.3000e+00,
         -3.4322e-01,  3.1912e-01, -3.2916e-01, -6.5754e-01,  1.3322e-01,
          1.4872e+00,  3.3406e-01, -8.5896e-01,  5.6429e-02, -1.2782e+00,
          1.4471e+00, -6.7698e-01,  2.7346e-01, -1.6382e+00,  2.2102e-02,
         -4.6719e-01, -5.7070e-01,  1.7668e+00,  5.8053e-01, -3.6928e-01,
         -7.9056e-01, -1.0566e-01,  2.3034e-01, -1.5379e+00, -5.3714e-02,
         -2.0413e+00, -3.2396e-02,  7.0850e-01,  3.4245e-01, -2.0712e-01,
          9.6050e-02, -2.5181e-01, -9.6619e-01, -5.9346e-01,  2.6988e-01,
         -1.6355e+00, -8.9870e-01,  1.7788e+00, -1.1164e-01,  9.1774e-01,
         -1.2001e+00, -1.5393e+00, -1.5818e-01, -1.8646e+00, -9.3066e-01,
          7.6793e-01,  9.9613e-01, -1.3722e+00, -1.3500e+00,  6.8103e-01,
          4.1190e-01, -1.8488e+00,  7.0303e-01, -9.0148e-01,  8.5878e-01,
          8.7450e-01, -4.8325e-02, -5.2769e-02, -7.8688e-02, -5.2092e-01,
         -1.4312e-01, -2.0614e+00, -3.7743e-01, -1.3410e-01, -3.3978e-01,
          1.2877e+00,  3.4951e-01,  6.0755e-01,  1.9015e-01, -1.8678e+00,
          7.5514e-01, -1.8589e-01, -4.5788e-01,  1.4513e+00,  1.7880e+00,
         -1.2963e-01, -9.5089e-01, -1.2412e-01,  4.7011e-01,  1.9193e+00,
         -1.1242e+00, -1.0321e+00,  5.5186e-01, -1.6518e-01, -9.9210e-01,
         -8.8201e-01,  2.5959e-01, -1.4807e+00,  1.1453e+00, -5.7856e-01,
         -5.5256e-01,  1.3698e+00, -7.8244e-01, -6.0015e-01, -1.5487e+00,
          1.0693e+00,  3.3249e-01, -8.6826e-01,  1.0084e+00, -4.5956e-01,
         -1.6148e+00, -1.7401e-01, -1.9187e+00,  2.4855e-01,  5.5284e-01,
          1.4573e+00,  3.2613e-02, -1.0722e+00,  4.5457e-01,  6.7663e-01,
          9.5070e-01,  1.9298e+00,  1.4436e-03,  4.6063e-01, -1.5001e+00,
          7.4149e-01, -6.0796e-01,  7.3927e-01, -8.2667e-01, -4.9717e-01,
         -2.3809e-01, -1.3273e+00,  6.6118e-01,  1.3817e+00,  8.1525e-01,
         -8.8447e-01,  1.3811e-01, -6.8249e-01, -4.1958e-01, -2.3332e+00,
         -1.1105e+00,  9.4233e-01,  5.3776e-01,  5.3802e-01, -7.0783e-01,
          6.5302e-01,  3.8073e-01,  2.6281e-01, -6.2956e-01,  1.1421e+00,
         -1.3057e+00, -6.9222e-01,  1.0418e+00,  1.5223e+00, -3.0123e-01,
         -9.7432e-01,  1.5428e+00,  5.6129e-01, -1.1917e-01, -7.8261e-01,
         -8.7698e-01,  1.5460e+00,  8.2454e-01, -1.5860e+00, -8.0340e-01,
          8.1813e-01,  1.3133e+00,  1.1080e+00,  1.0372e+00,  2.2219e-01,
         -2.0161e-01, -1.4155e-01,  5.3216e-01, -9.6410e-02,  1.7948e+00,
         -1.7062e-01, -1.4997e-03, -1.3744e+00,  5.2533e-01,  1.1247e+00,
         -1.0739e+00, -5.3848e-01, -1.0358e+00, -4.1634e-01, -1.0192e+00,
          8.0389e-01, -2.3628e-01,  3.1780e-02, -2.0364e-01,  2.5836e-01,
         -1.9524e+00, -1.1520e+00, -1.2636e+00,  7.4585e-01,  1.3198e-01,
         -1.4250e+00,  1.8459e+00, -5.7148e-01, -1.1567e+00, -4.0067e-01,
         -6.2902e-01, -1.1415e+00,  5.4868e-01,  7.8087e-01,  7.5292e-02,
         -2.9564e-01,  6.5217e-01, -4.6224e-01,  1.5101e-01,  8.5021e-01,
          2.9873e-01, -1.1355e+00,  1.6372e+00, -3.1092e-02,  5.3752e-01,
          1.4604e+00, -1.4555e+00,  1.1979e+00, -7.8614e-01,  6.2306e-01,
          2.0207e+00, -2.2010e-01,  1.0438e-01, -7.3968e-02, -8.1986e-01,
          1.3701e+00,  6.1866e-02,  1.5265e+00, -4.5773e-01, -3.1278e-01,
         -3.8263e-01, -1.7958e+00,  5.7462e-02, -1.0191e+00,  3.0819e-01,
          4.2824e-02, -2.1576e+00, -8.2004e-02, -2.0147e-01,  7.1641e-01,
          4.3848e-01,  7.3543e-01,  5.8282e-01,  2.3244e+00, -9.7639e-01,
          1.9591e-01,  4.4259e-01,  8.9500e-02, -1.0641e+00,  1.2207e+00,
          7.0560e-02, -5.7831e-01,  9.0717e-01, -3.2226e-01, -1.4384e+00,
         -7.6588e-01,  8.8581e-01,  5.0879e-01, -7.4768e-01,  8.3173e-01,
         -1.5212e+00,  1.4653e+00, -8.7602e-02, -2.3463e-01, -1.0277e+00,
          4.7738e-01,  5.1060e-01, -1.9068e+00,  3.2738e-01, -3.8155e-01,
          6.8094e-01,  1.1442e+00, -2.3083e-01,  2.7956e-02, -2.0416e+00,
         -4.2927e-01,  3.0954e-01, -2.0644e+00,  4.3077e-01, -1.9746e-01,
          2.7377e-01,  8.6149e-01, -1.4293e+00, -1.6919e+00, -9.3549e-01,
         -7.0867e-01, -2.6552e-02,  2.1637e-01,  1.1796e+00, -1.0206e-01,
          1.2719e-01,  3.3238e-01, -8.9301e-02,  9.3436e-01, -1.1423e-01,
          2.1710e+00,  1.6724e+00,  1.4248e+00, -7.0311e-02, -2.6874e-01,
          7.8724e-01, -1.2645e-01,  2.6997e-01,  2.2734e-02,  3.6460e-01,
         -3.5762e-02, -7.9331e-01, -1.1521e+00, -8.4928e-02, -8.8407e-02,
         -5.4222e-01, -1.0364e+00, -3.2266e-01,  1.0855e+00, -8.1322e-02,
          1.3894e+00, -7.4104e-03, -8.3995e-01,  1.8148e+00,  2.1664e-01,
          8.2385e-01,  3.0077e-01, -1.3889e+00, -1.2663e-01,  5.2896e-01,
          5.8481e-01,  2.0698e+00,  1.8139e-01, -5.1590e-01,  1.8764e-01,
         -7.9362e-01, -4.2889e-02, -3.6977e-01,  5.3196e-01,  9.1258e-01,
          3.3203e-01,  2.6107e-01,  1.1895e+00,  5.8136e-01,  7.8030e-01,
         -4.4771e-01,  2.5248e-01, -1.2125e+00,  8.5201e-01,  6.2615e-01,
         -4.6464e-01, -4.6168e-01,  6.3012e-01, -1.3001e+00, -4.7600e-01,
         -1.8405e+00,  1.5082e+00, -1.2966e+00, -1.2550e+00,  6.6496e-02,
         -5.9417e-01, -1.2807e-01,  1.5602e+00, -1.0811e-01,  8.9084e-01,
         -3.6435e-01,  1.1567e+00, -1.5760e+00, -3.8553e-02, -4.8042e-01,
         -2.4266e-01, -2.8390e-01,  6.0504e-01,  4.9040e-01,  3.4005e-02,
         -1.5020e-01, -5.3679e-01, -5.2114e-02,  1.0685e-02,  6.2674e-01,
         -3.6176e-02, -8.4974e-01, -8.6500e-02,  6.7058e-01, -1.6868e+00,
          1.2242e-01,  1.6896e+00,  5.2232e-01,  9.0551e-01, -1.3200e+00,
          4.3703e-01,  1.8856e+00,  2.6453e-01,  1.9155e+00,  7.3929e-01,
          2.0273e-01,  1.6746e-01, -1.1358e+00, -1.5308e+00,  1.7469e-01,
          1.5851e+00, -8.5155e-01, -8.3011e-01,  6.5800e-01,  6.4055e-01,
         -1.0305e+00,  1.7110e+00, -1.0026e+00,  6.8704e-02,  2.1984e-01,
          7.1450e-01,  1.0368e+00, -8.3135e-01,  1.9949e-01, -8.9561e-01,
          6.0898e-02,  5.6567e-01,  1.4009e+00, -1.6122e+00,  9.0249e-01,
          9.6149e-01, -2.6620e-01,  4.4834e-01, -1.0726e+00, -1.3024e+00,
          1.9978e+00,  7.9052e-01, -6.9631e-01, -1.1775e-01, -5.4189e-01,
          1.3945e-01, -6.1785e-01,  1.8642e-01, -4.2044e-01,  8.3601e-01,
          2.2372e+00, -5.9374e-01,  1.1521e+00,  1.0014e+00,  2.3731e-01,
         -9.2328e-01, -6.1568e-01,  6.7766e-01, -1.2004e+00,  2.7637e-01,
          9.3837e-01,  3.1301e-01,  1.9548e+00, -6.0744e-01,  5.1339e-01,
          1.1864e+00,  1.1217e-01,  6.8392e-01,  2.1476e+00,  1.0936e+00,
         -4.4918e-01,  6.7367e-01,  1.9326e+00, -9.5147e-01,  1.4895e+00,
          1.3305e-01,  8.8955e-01, -6.4028e-01, -2.0680e+00,  2.9896e+00,
          3.2782e-01,  1.9336e+00]]), output_embedding: tensor([[-1.8255e-01, -4.0788e-01,  1.6076e-01, -2.5198e-01, -8.8826e-01,
          1.4533e-01, -7.3903e-01, -4.7802e-01, -6.8017e-01,  5.4484e-01,
         -8.3872e-01,  1.7665e+00,  8.4522e-02,  2.3097e-02,  6.8516e-01,
         -7.0446e-01, -3.8165e-01,  6.5782e-01,  8.0665e-02,  3.4668e-01,
         -5.3021e-01, -1.6320e-01, -1.1719e-01,  5.9696e-01, -1.2922e+00,
          3.3372e-01, -3.2954e-02,  7.2119e-01,  1.8851e-01, -7.6756e-01,
         -2.7816e-01, -4.3089e-01, -3.2760e-01, -2.6845e-01, -3.7992e-01,
         -8.8505e-01,  9.3222e-03,  3.9554e-01,  1.3592e+00,  4.8343e-01,
         -7.1763e-01,  4.7244e-01,  4.9921e-01, -8.5018e-01, -4.7007e-01,
         -3.1709e-01, -5.5385e-01,  1.5694e-01, -9.4155e-01,  3.2117e-01,
          3.5412e-01, -1.5624e-01,  9.4912e-01, -7.1007e-01,  4.5652e-02,
          4.1129e-01,  1.2175e+00,  2.9892e-02,  2.5776e-01, -6.1032e-01,
         -5.2767e-01,  1.1862e-01,  4.0411e-01, -5.6977e-02, -3.4231e-01,
         -6.8259e-01,  1.1710e-01,  9.0227e-01,  5.8681e-01, -3.4236e-01,
          3.0796e-01,  8.2736e-02,  5.7098e-01,  8.1015e-01, -8.4607e-01,
         -4.2826e-01, -1.0027e+00, -7.9148e-01, -2.0352e-01, -4.7955e-02,
          3.9877e-01, -3.4746e-01,  3.5791e-01, -9.9465e-01, -4.6455e-01,
         -6.2758e-01,  6.1228e-01,  4.8205e-01,  2.0930e+00,  1.0554e+00,
          1.9314e-01,  3.8599e-01,  5.0611e-01,  7.6208e-01, -8.8878e-02,
         -3.4405e-01,  8.6539e-01, -3.6863e-01,  4.0994e-01, -2.5762e-01,
          5.3220e-01, -1.4100e-01,  3.8227e-01,  1.5783e+00, -4.5233e-01,
          2.7012e-01, -9.5246e-01,  1.8134e+00, -2.8061e+00, -7.0744e-03,
          1.1461e+00,  5.9902e-01, -9.3717e-01, -5.3895e-01,  5.4251e-01,
         -4.4150e-01, -5.2048e-03,  8.6503e-01,  6.3065e-01,  7.6411e-01,
         -2.4348e-01, -6.1991e-01, -6.2541e-02,  9.0314e-01,  9.2742e-01,
         -1.5580e+00, -9.9327e-01,  1.1429e+00, -2.7526e-01, -4.5104e-01,
          5.5858e-01,  1.2172e-01, -7.3936e-01,  1.6622e-01,  4.3890e-01,
         -4.2702e-01, -4.4180e-01, -1.3739e+00, -1.3896e-01, -5.9950e-02,
          1.1110e+00,  1.0591e+00, -7.2583e-02,  1.0202e+00,  1.3183e+00,
          3.0808e-01, -1.8869e-01,  2.0228e-01, -1.4433e+00, -4.4621e-01,
         -1.7324e-01, -5.8355e-01, -1.2615e+00,  3.3693e+00,  3.4422e-01,
          6.7722e-01, -1.6634e-01,  1.6876e-01, -9.7153e-01, -1.0274e+00,
         -4.4053e-01,  2.0086e-01, -7.2813e-01, -2.3412e-01, -7.8844e-01,
         -4.1144e-01, -9.2376e-01,  1.1608e+00, -1.9629e-01,  2.7943e-01,
          2.0589e-01, -2.6516e-01, -4.3682e-01,  4.6792e-01,  3.6933e-01,
         -3.0362e-01,  1.2336e+00, -7.5077e-02,  1.6869e-01,  2.9298e-01,
          3.8768e+00, -2.6374e+00, -2.9200e-02,  2.9550e-01, -6.4243e-01,
         -1.1066e+00, -6.0562e-01,  8.7127e-01,  9.5174e-01, -1.1741e+00,
          8.3490e-01,  2.8753e-02,  3.6931e-01,  2.0665e-01,  2.6432e-01,
         -2.3390e-01, -1.5537e+00,  5.9100e-01,  2.3830e-01, -4.1695e-01,
          1.0490e-01,  5.8710e-01, -2.6665e-01, -1.6326e-01, -2.9972e+00,
          7.8470e-01,  4.9015e-01, -1.4249e-01, -5.4056e-01,  1.1162e-01,
         -2.7971e-01, -2.1713e-01,  3.0590e-01, -1.0733e+00,  9.5212e-01,
         -9.0251e-01,  4.4069e-01,  1.6808e+00, -9.2984e-01,  8.1391e-01,
         -1.0787e-01, -2.6859e-01,  2.9374e-01, -9.1996e-01, -2.3097e-02,
          6.7745e-01, -2.8411e-01,  4.5438e-01,  1.5170e-01, -2.0155e+00,
         -7.1402e-01,  1.2454e-01, -3.1142e-01, -9.0900e-02,  3.8416e-01,
         -3.3357e-01,  3.1400e-01, -9.7673e-01, -5.5772e-01, -7.0417e-01,
         -2.3285e-01,  6.2322e-01,  2.2156e-01,  9.1579e-01,  3.5369e-03,
         -1.3154e+00,  5.6757e-03,  1.0759e-01,  1.2531e+00,  6.3730e-01,
          2.3391e-02, -2.1760e-01, -1.1546e-01,  2.5269e-01, -3.3619e-01,
          8.2279e-01]]), obj_idxes: tensor([-2]), matched_gt_idxes: tensor([-1]), disappear_time: tensor([0]), iou: tensor([0.]), scores: tensor([0.4058]), track_scores: tensor([0.]), pred_boxes: tensor([[ 3.1940e-02, -6.2492e-02,  5.5435e-01,  1.4224e+00, -7.7301e-01,
          4.5185e-01, -1.5757e-03, -9.9405e-01, -1.7210e-01,  6.9688e+00]]), pred_logits: tensor([[-0.3814, -5.0945, -5.5865, -5.2074, -5.6574, -5.5663, -5.1352, -7.4560,
         -4.3447, -5.7929]]), mem_bank: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), mem_padding_mask: tensor([[True, True, True, True]]), save_period: tensor([0.])])
(Pdb) 143  	                        ret_list.append(self.kalman_models[i])
144  	                ret.set(k, ret_list)
145  	
146  	            else:
147  	                ret.set(k, v[item])
148  ->	        return ret
149  	
150  	    def __len__(self) -> int:
151  	        for v in self._fields.values():
152  	            # use __len__ because len() has to be int and is not friendly to tracing
153  	            return v.__len__()
(Pdb) --Call--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(486)select_sdc_track_query()
-> def select_sdc_track_query(self, sdc_instance, img_metas):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(487)select_sdc_track_query()
-> out = dict()
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(488)select_sdc_track_query()
-> result_dict = self._track_instances2results(sdc_instance, img_metas, with_mask=False)
(Pdb) --Call--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(780)_track_instances2results()
-> def _track_instances2results(self, track_instances, img_metas, with_mask=True):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(781)_track_instances2results()
-> bbox_dict = dict(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(782)_track_instances2results()
-> cls_scores=track_instances.pred_logits,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(783)_track_instances2results()
-> bbox_preds=track_instances.pred_boxes,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(784)_track_instances2results()
-> track_scores=track_instances.scores,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(785)_track_instances2results()
-> obj_idxes=track_instances.obj_idxes,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(781)_track_instances2results()
-> bbox_dict = dict(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(788)_track_instances2results()
-> bboxes_dict = self.bbox_coder.decode(bbox_dict, with_mask=with_mask, img_metas=img_metas)[0]
(Pdb) --Call--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(129)decode()
-> def decode(self, preds_dicts, with_mask=True, img_metas=None):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(143)decode()
-> all_cls_scores = preds_dicts['cls_scores']
(Pdb) tensor([[-0.3814, -5.0945, -5.5865, -5.2074, -5.6574, -5.5663, -5.1352, -7.4560,
         -4.3447, -5.7929]])
(Pdb) {'cls_scores': tensor([[-0.3814, -5.0945, -5.5865, -5.2074, -5.6574, -5.5663, -5.1352, -7.4560,
         -4.3447, -5.7929]]), 'bbox_preds': tensor([[ 3.1940e-02, -6.2492e-02,  5.5435e-01,  1.4224e+00, -7.7301e-01,
          4.5185e-01, -1.5757e-03, -9.9405e-01, -1.7210e-01,  6.9688e+00]]), 'track_scores': tensor([0.4058]), 'obj_idxes': tensor([-2])}
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(144)decode()
-> all_bbox_preds = preds_dicts['bbox_preds']
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(145)decode()
-> track_scores = preds_dicts['track_scores']
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(146)decode()
-> obj_idxes = preds_dicts['obj_idxes']
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(148)decode()
-> batch_size = all_cls_scores.size()[0]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(149)decode()
-> predictions_list = []
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(151)decode()
-> predictions_list.append(self.decode_single(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(152)decode()
-> all_cls_scores, all_bbox_preds,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(153)decode()
-> track_scores, obj_idxes, with_mask, img_metas))
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(151)decode()
-> predictions_list.append(self.decode_single(
(Pdb) --Call--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(42)decode_single()
-> def decode_single(self, cls_scores, bbox_preds,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(56)decode_single()
-> max_num = self.max_num
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(57)decode_single()
-> max_num = min(cls_scores.size(0), self.max_num)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(59)decode_single()
-> cls_scores = cls_scores.sigmoid()
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(60)decode_single()
-> _, indexs = cls_scores.max(dim=-1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(61)decode_single()
-> labels = indexs % self.num_classes
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(63)decode_single()
-> _, bbox_index = track_scores.topk(max_num)
(Pdb) 1
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(65)decode_single()
-> labels = labels[bbox_index]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(66)decode_single()
-> bbox_preds = bbox_preds[bbox_index]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(67)decode_single()
-> track_scores = track_scores[bbox_index]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(68)decode_single()
-> obj_idxes = obj_idxes[bbox_index]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(70)decode_single()
-> scores = track_scores
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(72)decode_single()
-> final_box_preds = denormalize_bbox(bbox_preds, self.pc_range)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(73)decode_single()
-> final_scores = track_scores
(Pdb) tensor([[ 0.0319, -0.0625, -0.7730,  1.7408,  4.1469,  1.5712, -3.1400, -0.1721,
          6.9688]])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(74)decode_single()
-> final_preds = labels
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(77)decode_single()
-> if self.score_threshold is not None:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(78)decode_single()
-> thresh_mask = final_scores > self.score_threshold
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(80)decode_single()
-> if self.with_nms:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(93)decode_single()
-> if self.post_center_range is not None:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(94)decode_single()
-> self.post_center_range = torch.tensor(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(95)decode_single()
-> self.post_center_range, device=scores.device)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(94)decode_single()
-> self.post_center_range = torch.tensor(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(96)decode_single()
-> mask = (final_box_preds[..., :3] >=
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(97)decode_single()
-> self.post_center_range[:3]).all(1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(96)decode_single()
-> mask = (final_box_preds[..., :3] >=
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(97)decode_single()
-> self.post_center_range[:3]).all(1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(96)decode_single()
-> mask = (final_box_preds[..., :3] >=
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(98)decode_single()
-> mask &= (final_box_preds[..., :3] <=
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(99)decode_single()
-> self.post_center_range[3:]).all(1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(98)decode_single()
-> mask &= (final_box_preds[..., :3] <=
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(99)decode_single()
-> self.post_center_range[3:]).all(1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(98)decode_single()
-> mask &= (final_box_preds[..., :3] <=
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(101)decode_single()
-> if self.score_threshold:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(103)decode_single()
-> if not with_mask:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(104)decode_single()
-> mask = torch.ones_like(mask) > 0
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(105)decode_single()
-> if self.with_nms:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(108)decode_single()
-> boxes3d = final_box_preds[mask]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(109)decode_single()
-> scores = final_scores[mask]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(110)decode_single()
-> labels = final_preds[mask]
(Pdb) tensor([[ 0.0319, -0.0625, -0.7730,  1.7408,  4.1469,  1.5712, -3.1400, -0.1721,
          6.9688]])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(111)decode_single()
-> track_scores = track_scores[mask]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(112)decode_single()
-> obj_idxes = obj_idxes[mask]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(114)decode_single()
-> 'bboxes': boxes3d,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(115)decode_single()
-> 'scores': scores,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(116)decode_single()
-> 'labels': labels,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(117)decode_single()
-> 'track_scores': track_scores,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(118)decode_single()
-> 'obj_idxes': obj_idxes,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(119)decode_single()
-> 'bbox_index': bbox_index,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(120)decode_single()
-> 'mask': mask
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(113)decode_single()
-> predictions_dict = {
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(127)decode_single()
-> return predictions_dict
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(127)decode_single()->{'bbox_index': tensor([0]), 'bboxes': tensor([[ 0.0...     6.9688]]), 'labels': tensor([0]), 'mask': tensor([True]), ...}
-> return predictions_dict
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(156)decode()
-> return predictions_list
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/core/bbox/coders/detr3d_track_coder.py(156)decode()->[{'bbox_index': tensor([0]), 'bboxes': tensor([[ 0.0...     6.9688]]), 'labels': tensor([0]), 'mask': tensor([True]), ...}]
-> return predictions_list
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(789)_track_instances2results()
-> bboxes = bboxes_dict["bboxes"]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(791)_track_instances2results()
-> bboxes = img_metas[0]["box_type_3d"](bboxes, 9)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(792)_track_instances2results()
-> labels = bboxes_dict["labels"]
(Pdb) LiDARInstance3DBoxes(
    tensor([[ 0.0319, -0.0625, -0.7730,  1.7408,  4.1469,  1.5712, -3.1400, -0.1721,
          6.9688]]))
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(793)_track_instances2results()
-> scores = bboxes_dict["scores"]
(Pdb) tensor([0])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(794)_track_instances2results()
-> bbox_index = bboxes_dict["bbox_index"]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(796)_track_instances2results()
-> track_scores = bboxes_dict["track_scores"]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(797)_track_instances2results()
-> obj_idxes = bboxes_dict["obj_idxes"]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(798)_track_instances2results()
-> result_dict = dict(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(799)_track_instances2results()
-> boxes_3d=bboxes.to("cpu"),
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(800)_track_instances2results()
-> scores_3d=scores.cpu(),
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(801)_track_instances2results()
-> labels_3d=labels.cpu(),
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(802)_track_instances2results()
-> track_scores=track_scores.cpu(),
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(803)_track_instances2results()
-> bbox_index=bbox_index.cpu(),
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(804)_track_instances2results()
-> track_ids=obj_idxes.cpu(),
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(805)_track_instances2results()
-> mask=bboxes_dict["mask"].cpu(),
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(806)_track_instances2results()
-> track_bbox_results=[[bboxes.to("cpu"), scores.cpu(), labels.cpu(), bbox_index.cpu(), bboxes_dict["mask"].cpu()]]
(Pdb) tensor([-2])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(798)_track_instances2results()
-> result_dict = dict(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(808)_track_instances2results()
-> return result_dict
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(808)_track_instances2results()->{'bbox_index': tensor([0]), 'boxes_3d': LiDARInstance...    6.9688]])), 'labels_3d': tensor([0]), 'mask': tensor([True]), ...}
-> return result_dict
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(489)select_sdc_track_query()
-> out["sdc_boxes_3d"] = result_dict['boxes_3d']
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(490)select_sdc_track_query()
-> out["sdc_scores_3d"] = result_dict['scores_3d']
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(491)select_sdc_track_query()
-> out["sdc_track_scores"] = result_dict['track_scores']
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(492)select_sdc_track_query()
-> out["sdc_track_bbox_results"] = result_dict['track_bbox_results']
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(493)select_sdc_track_query()
-> out["sdc_embedding"] = sdc_instance.output_embedding[0]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(494)select_sdc_track_query()
-> return out
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(494)select_sdc_track_query()->{'sdc_boxes_3d': LiDARInstance...    6.9688]])), 'sdc_embedding': tensor([-1.82...  8.2279e-01]), 'sdc_scores_3d': tensor([0.4058]), 'sdc_track_bbox_results': [[LiDARInstance...    6.9688]])), tensor([0.4058]), tensor([0]), tensor([0]), tensor([True])]], ...}
-> return out
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(699)_forward_single_frame_inference()
-> if self.memory_bank is not None:
(Pdb) 694  	        active_index = (track_instances.obj_idxes>=0) & (track_instances.scores >= self.track_base.filter_score_thresh)    # filter out sleep objects
695  	        out.update(self.select_active_track_query(track_instances, active_index, img_metas))
696  	        out.update(self.select_sdc_track_query(track_instances[track_instances.obj_idxes==-2], img_metas))
697  	
698  	        """ update with memory_bank """
699  ->	        if self.memory_bank is not None:
700  	            track_instances = self.memory_bank(track_instances)
701  	
702  	        """  Update track instances using matcher """
703  	        tmp = {}
704  	        tmp["init_track_instances"] = self._generate_empty_tracks()
(Pdb) tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1,  2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -2])
(Pdb) 705  	        tmp["track_instances"] = track_instances
706  	        out_track_instances = self.query_interact(tmp)
707  	        out["track_instances_fordet"] = track_instances
708  	        out["track_instances"] = out_track_instances
709  	        out["track_obj_idxes"] = track_instances.obj_idxes
710  	        return out
711  	
712  	    def simple_test_track(
713  	        self,
714  	        img=None,
715  	        l2g_t=None,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(700)_forward_single_frame_inference()
-> track_instances = self.memory_bank(track_instances)
(Pdb) *** AttributeError: Cannot find field 'keys' in the given Instances!
(Pdb) --Call--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1117)__getattr__()
-> def __getattr__(self, name: str) -> Union[Tensor, 'Module']:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1118)__getattr__()
-> if '_parameters' in self.__dict__:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1119)__getattr__()
-> _parameters = self.__dict__['_parameters']
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1120)__getattr__()
-> if name in _parameters:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1122)__getattr__()
-> if '_buffers' in self.__dict__:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1123)__getattr__()
-> _buffers = self.__dict__['_buffers']
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1124)__getattr__()
-> if name in _buffers:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1126)__getattr__()
-> if '_modules' in self.__dict__:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1127)__getattr__()
-> modules = self.__dict__['_modules']
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1128)__getattr__()
-> if name in modules:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1129)__getattr__()
-> return modules[name]
(Pdb) --Return--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1129)__getattr__()->MemoryBank(
 ...affine=True)
)
-> return modules[name]
(Pdb) --Call--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1045)_call_impl()
-> def _call_impl(self, *input, **kwargs):
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1046)_call_impl()
-> forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1049)_call_impl()
-> if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1050)_call_impl()
-> or _global_forward_hooks or _global_forward_pre_hooks):
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1049)_call_impl()
-> if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1050)_call_impl()
-> or _global_forward_hooks or _global_forward_pre_hooks):
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1049)_call_impl()
-> if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) --Call--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(85)forward()
-> def forward(self, track_instances: Instances, update_bank=True) -> Instances:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(86)forward()
-> track_instances = self._forward_temporal_attn(track_instances)
(Pdb) *** NameError: name 'ss' is not defined
(Pdb) --Call--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(55)_forward_temporal_attn()
-> def _forward_temporal_attn(self, track_instances):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(56)_forward_temporal_attn()
-> if len(track_instances) == 0:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(59)_forward_temporal_attn()
-> key_padding_mask = track_instances.mem_padding_mask  # [n_, memory_bank_len]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(61)_forward_temporal_attn()
-> valid_idxes = key_padding_mask[:, -1] == 0
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(62)_forward_temporal_attn()
-> embed = track_instances.output_embedding[valid_idxes]  # (n, 256)
(Pdb) tensor([False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(64)_forward_temporal_attn()
-> if len(embed) > 0:
(Pdb) torch.Size([0, 256])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(80)_forward_temporal_attn()
-> return track_instances
(Pdb) tensor([[True, True, True, True],
        [True, True, True, True],
        [True, True, True, True],
        ...,
        [True, True, True, True],
        [True, True, True, True],
        [True, True, True, True]])
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(80)_forward_temporal_attn()->Instances(num...0., 0., 0.])])
-> return track_instances
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(87)forward()
-> if update_bank:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(88)forward()
-> self.update(track_instances)
(Pdb) --Call--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(32)update()
-> def update(self, track_instances):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(33)update()
-> embed = track_instances.output_embedding[:, None]  #( N, 1, 256)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(34)update()
-> scores = track_instances.scores
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(35)update()
-> mem_padding_mask = track_instances.mem_padding_mask
(Pdb) torch.Size([901])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(36)update()
-> device = embed.device
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(38)update()
-> save_period = track_instances.save_period
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(39)update()
-> if self.training:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(42)update()
-> saved_idxes = (save_period == 0) & (scores > self.save_thresh)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(44)update()
-> save_period[save_period > 0] -= 1
(Pdb) tensor([True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True])
(Pdb) 0.0
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(45)update()
-> save_period[saved_idxes] = self.save_period
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(47)update()
-> saved_embed = embed[saved_idxes]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(48)update()
-> if len(saved_embed) > 0:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(49)update()
-> prev_embed = track_instances.mem_bank[saved_idxes]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(50)update()
-> save_embed = self.save_proj(saved_embed)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(51)update()
-> mem_padding_mask[saved_idxes] = torch.cat([mem_padding_mask[saved_idxes, 1:], torch.zeros((len(saved_embed), 1), dtype=torch.bool, device=device)], dim=1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(52)update()
-> track_instances.mem_bank = track_instances.mem_bank.clone()
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(53)update()
-> track_instances.mem_bank[saved_idxes] = torch.cat([prev_embed[:, 1:], save_embed], dim=1)
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(53)update()->None
-> track_instances.mem_bank[saved_idxes] = torch.cat([prev_embed[:, 1:], save_embed], dim=1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(89)forward()
-> return track_instances
(Pdb) Instances(num_instances=901, image_height=1, image_width=1, fields=[ref_pts: tensor([[-1.8111e-02,  6.4056e-01, -1.6428e-01],
        [-1.1195e+00,  8.4606e-01,  4.2058e-01],
        [ 1.3552e+00,  2.4231e-01, -2.9146e-01],
        ...,
        [-9.4047e-02,  1.0546e+00, -2.0764e-01],
        [ 1.2377e+00,  3.4943e+00,  3.4574e-01],
        [ 1.2477e-03, -2.4410e-03,  1.1362e-01]]), query: tensor([[ 0.9794,  1.4678,  2.2049,  ..., -0.8909, -0.0528, -0.0377],
        [-0.1857,  0.0648, -0.3255,  ...,  0.8429,  0.9830,  1.0372],
        [ 0.9206, -2.1922, -1.2307,  ..., -0.6641, -0.4396,  1.1959],
        ...,
        [-0.4122,  1.3001, -0.3364,  ..., -0.0273,  0.7269,  0.7551],
        [ 0.5784, -1.9046, -0.5663,  ...,  0.1317, -0.8664,  1.2704],
        [ 1.1220, -1.8478,  0.7056,  ...,  2.9896,  0.3278,  1.9336]]), output_embedding: tensor([[-0.0888,  1.1860,  1.2986,  ...,  0.4291, -0.7264,  1.0962],
        [-0.0456, -0.0433, -0.3571,  ...,  0.0131,  0.3755,  0.2974],
        [-0.4183,  0.1344, -0.5244,  ..., -0.0383,  0.3907,  0.1019],
        ...,
        [ 0.4592, -0.0442, -0.3416,  ...,  0.6930, -0.1248,  1.1393],
        [-1.1838,  0.5513,  0.2055,  ..., -0.4299, -0.7415, -0.0049],
        [-0.1825, -0.4079,  0.1608,  ...,  0.2527, -0.3362,  0.8228]]), obj_idxes: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1,  2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -2]), matched_gt_idxes: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1]), disappear_time: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), iou: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), scores: tensor([1.2134e-03, 6.2523e-02, 3.5854e-02, 2.3008e-03, 1.1493e-03, 1.1479e-02,
        1.6889e-02, 2.7017e-02, 4.1991e-03, 3.1803e-03, 1.4656e-02, 4.7814e-03,
        2.1172e-02, 3.6873e-01, 6.2139e-03, 3.8186e-03, 1.2621e-02, 4.0460e-02,
        3.7893e-02, 3.4594e-03, 4.9484e-03, 9.8611e-04, 2.1758e-02, 5.3686e-02,
        2.1225e-02, 3.4493e-02, 2.8313e-03, 1.5702e-03, 1.4362e-02, 2.7795e-02,
        4.6897e-02, 7.2637e-03, 2.7099e-02, 1.4972e-02, 3.9342e-03, 2.4330e-02,
        9.3692e-03, 6.4523e-02, 5.1957e-02, 9.3188e-03, 6.6445e-02, 8.2580e-02,
        3.8335e-03, 6.9467e-03, 3.4724e-02, 5.2116e-03, 8.9579e-02, 6.2508e-02,
        5.2185e-02, 9.1465e-03, 2.8851e-02, 5.1593e-03, 1.0974e-01, 3.9678e-03,
        2.8626e-02, 2.6586e-02, 8.3299e-03, 5.2649e-02, 5.3267e-03, 6.4704e-03,
        3.9233e-03, 1.5086e-02, 6.7448e-02, 6.6768e-03, 1.6665e-02, 6.9540e-02,
        2.5125e-02, 3.6785e-02, 5.8670e-03, 7.5140e-03, 1.2913e-02, 3.6196e-02,
        8.6278e-03, 7.7499e-03, 6.9070e-02, 3.8398e-02, 1.3969e-02, 1.1849e-03,
        8.7286e-03, 8.0405e-03, 6.5666e-03, 1.3306e-01, 4.7247e-03, 3.9266e-02,
        5.0965e-03, 7.2163e-03, 5.7265e-02, 3.8921e-02, 1.6542e-02, 1.4544e-01,
        1.4110e-01, 8.5508e-03, 3.6722e-02, 1.1033e-03, 9.6849e-04, 1.3027e-01,
        6.2588e-04, 5.8292e-03, 1.6276e-02, 5.5910e-03, 1.1736e-02, 2.3894e-02,
        5.0752e-03, 6.0627e-02, 1.2194e-02, 4.0648e-03, 2.2797e-02, 1.6678e-02,
        3.1925e-03, 4.7256e-02, 7.4789e-02, 8.4010e-01, 5.7277e-03, 2.7151e-02,
        1.0818e-01, 4.1882e-02, 1.9956e-03, 4.3114e-02, 7.3796e-03, 1.1827e-02,
        3.1510e-02, 8.1394e-02, 5.5510e-02, 2.3008e-02, 3.7469e-03, 1.6620e-02,
        1.1137e-02, 4.6571e-03, 6.4330e-03, 6.4549e-02, 1.4618e-01, 2.0940e-02,
        2.4190e-02, 6.6831e-02, 8.4846e-04, 7.3563e-03, 4.9336e-03, 1.4138e-03,
        1.2344e-03, 6.2734e-02, 8.6931e-03, 6.2626e-03, 2.2344e-02, 5.7888e-02,
        6.1706e-02, 2.5208e-02, 9.7907e-04, 2.8844e-03, 9.5125e-03, 1.2403e-02,
        1.1536e-02, 1.6938e-02, 3.4666e-02, 5.2722e-03, 2.4653e-02, 4.4650e-03,
        7.2293e-03, 1.3935e-02, 6.5360e-02, 1.1119e-03, 2.4714e-03, 1.4973e-02,
        2.6434e-03, 2.1652e-03, 1.2158e-02, 6.7462e-03, 1.9515e-02, 2.4495e-02,
        3.6428e-02, 2.1267e-02, 3.1087e-02, 1.9324e-02, 4.9264e-03, 1.2190e-02,
        3.7031e-02, 1.6408e-03, 1.8139e-02, 1.3024e-03, 1.2271e-03, 1.2923e-02,
        7.4022e-03, 5.7929e-03, 1.0363e-02, 1.2676e-03, 1.0056e-02, 1.1427e-02,
        7.5995e-02, 3.9850e-03, 6.6336e-03, 1.4071e-03, 6.7100e-03, 5.1421e-02,
        1.3440e-03, 6.4037e-02, 1.2116e-02, 4.6018e-02, 4.9342e-02, 5.1683e-03,
        1.9097e-02, 1.5111e-02, 3.0793e-02, 7.4207e-03, 2.6880e-02, 3.5225e-03,
        1.1567e-02, 1.4508e-03, 1.6973e-02, 6.1134e-03, 4.1442e-02, 1.0759e-02,
        6.6415e-02, 4.8017e-02, 6.6356e-03, 1.5206e-03, 1.2363e-03, 4.3600e-02,
        3.9149e-02, 1.9085e-02, 1.5656e-02, 2.7196e-02, 1.1147e-03, 1.6049e-01,
        5.6968e-02, 5.2893e-03, 4.0854e-02, 1.6607e-01, 3.7793e-03, 9.2531e-03,
        5.4123e-02, 1.1974e-02, 1.3448e-03, 2.4931e-02, 1.3381e-02, 1.2397e-02,
        3.0968e-02, 3.7141e-02, 4.5333e-02, 1.1147e-02, 2.6424e-02, 4.2933e-02,
        5.2535e-03, 1.1537e-02, 6.0293e-03, 3.8656e-02, 1.0428e-02, 6.7816e-03,
        5.7107e-02, 1.0390e-01, 3.6373e-03, 4.7635e-03, 6.8706e-03, 2.4473e-02,
        5.5701e-03, 9.1752e-04, 3.1969e-03, 4.2385e-03, 4.7782e-03, 1.3262e-03,
        2.7159e-02, 1.6284e-02, 1.0323e-02, 4.7396e-02, 4.2747e-02, 3.8807e-02,
        7.0219e-03, 4.5195e-02, 2.4153e-02, 5.1583e-02, 7.8921e-03, 1.0167e-02,
        1.5030e-02, 2.6144e-02, 6.5438e-03, 4.0822e-02, 1.0016e-03, 9.4240e-03,
        3.8219e-02, 8.1689e-03, 5.1255e-02, 9.5637e-03, 1.3022e-03, 2.5308e-03,
        1.0544e-02, 2.3149e-02, 8.2537e-03, 1.2807e-02, 1.0644e-02, 5.5866e-02,
        8.2002e-03, 9.8223e-02, 1.1481e-03, 3.2006e-02, 4.2788e-02, 4.7632e-03,
        9.0154e-02, 8.6585e-03, 3.0511e-03, 6.2478e-03, 8.3633e-03, 5.2223e-03,
        7.2684e-03, 1.4663e-02, 4.0191e-03, 7.4031e-03, 9.1275e-03, 1.2543e-02,
        2.3229e-02, 1.3753e-03, 5.9674e-02, 3.3736e-02, 4.8074e-02, 4.1207e-02,
        9.5094e-02, 1.2115e-02, 1.6038e-02, 8.3455e-03, 3.7825e-02, 6.7053e-03,
        1.0765e-03, 1.7989e-02, 5.3714e-03, 8.1112e-03, 3.5229e-02, 2.2709e-02,
        1.4656e-01, 1.1850e-02, 2.6655e-02, 6.5880e-03, 6.8146e-03, 6.6188e-02,
        5.6035e-03, 9.2401e-03, 4.9330e-02, 1.1847e-03, 7.3345e-02, 3.6133e-02,
        8.9987e-01, 4.9836e-02, 8.0589e-02, 4.1223e-03, 6.2678e-02, 1.2009e-01,
        3.9707e-03, 1.8302e-02, 2.6263e-01, 4.8747e-02, 3.4416e-02, 1.4222e-02,
        1.6961e-03, 8.5399e-03, 5.1251e-03, 5.7892e-02, 8.2813e-03, 1.4120e-02,
        1.2153e-03, 3.0682e-02, 5.2534e-02, 3.1185e-03, 5.3804e-03, 2.1879e-02,
        1.0233e-02, 3.4845e-02, 5.0240e-02, 6.6517e-03, 5.5033e-02, 9.9240e-02,
        5.8001e-03, 3.6543e-03, 3.5133e-02, 1.5622e-02, 7.5054e-02, 6.2472e-02,
        7.9391e-03, 6.3959e-02, 2.9504e-02, 6.9829e-02, 1.1694e-02, 9.7521e-02,
        4.1910e-02, 1.6028e-02, 1.5102e-01, 7.6756e-02, 5.6412e-02, 6.7188e-03,
        5.6265e-03, 4.2931e-03, 3.5824e-03, 1.1934e-02, 2.5243e-02, 1.5601e-02,
        9.1454e-03, 9.2640e-03, 1.2290e-03, 4.8899e-03, 1.6706e-03, 5.1655e-03,
        3.5898e-03, 1.1129e-02, 4.1541e-02, 5.8084e-03, 1.7909e-02, 5.2201e-03,
        1.2108e-02, 1.8057e-03, 4.9781e-02, 3.7611e-02, 3.1539e-02, 1.0157e-02,
        1.1005e-03, 3.7674e-02, 9.4846e-04, 1.3821e-02, 1.5579e-03, 4.8643e-02,
        7.5679e-03, 4.9013e-03, 1.1422e-03, 1.2696e-01, 1.9293e-02, 1.9225e-02,
        6.2625e-03, 6.8183e-03, 6.1910e-02, 5.0084e-03, 4.9455e-02, 5.0841e-02,
        2.5227e-02, 2.1574e-02, 3.6100e-02, 2.7357e-02, 4.2781e-02, 3.4200e-02,
        7.3570e-03, 1.1011e-02, 7.6662e-02, 1.3262e-02, 3.2418e-02, 1.4014e-02,
        9.6086e-03, 1.6986e-02, 4.6768e-02, 2.1747e-03, 3.5014e-02, 2.0059e-02,
        2.3717e-02, 2.0145e-02, 2.8579e-02, 3.5022e-03, 9.9863e-04, 4.2501e-02,
        2.5523e-02, 4.8021e-03, 1.2995e-02, 4.5505e-03, 2.9458e-02, 1.3647e-02,
        2.3899e-02, 1.0175e-02, 3.2205e-02, 3.4850e-03, 1.0634e-03, 1.1630e-02,
        7.9688e-03, 1.4233e-01, 2.2008e-02, 1.6555e-03, 4.5813e-02, 1.1434e-03,
        1.1137e-02, 1.3660e-02, 2.5831e-02, 3.4862e-02, 4.6683e-03, 2.3943e-02,
        7.4350e-03, 4.2919e-02, 2.1815e-02, 1.1046e-02, 8.6327e-03, 2.1631e-02,
        9.6153e-03, 1.0895e-01, 8.7908e-03, 1.9043e-02, 3.9042e-02, 2.6306e-02,
        3.4800e-03, 4.7643e-02, 6.1349e-03, 1.0257e-02, 5.8688e-03, 1.4184e-02,
        1.6281e-02, 7.4497e-03, 9.5034e-04, 3.2814e-02, 4.1897e-02, 2.1638e-02,
        9.9387e-03, 9.2697e-03, 5.0314e-02, 1.2052e-03, 1.4283e-02, 1.4066e-02,
        5.3169e-02, 1.4760e-02, 1.2717e-02, 2.0727e-02, 2.0729e-02, 1.3156e-02,
        1.6902e-02, 6.2680e-03, 6.2336e-03, 1.5705e-02, 7.7093e-02, 1.6454e-02,
        4.2099e-03, 7.0141e-02, 1.2030e-02, 3.5844e-02, 1.3685e-02, 4.8726e-03,
        8.7846e-03, 9.5205e-03, 3.2727e-03, 1.2507e-01, 6.3954e-03, 1.8167e-02,
        3.7289e-03, 9.2284e-03, 3.8752e-03, 2.6565e-03, 2.6234e-03, 1.2883e-03,
        7.1558e-03, 2.7902e-02, 2.3973e-02, 4.2808e-02, 2.2890e-02, 1.3514e-02,
        8.4640e-03, 5.5918e-03, 7.9391e-03, 6.8766e-03, 9.4866e-03, 8.0524e-03,
        1.4649e-02, 4.7029e-03, 4.3191e-03, 2.5143e-02, 1.0017e-02, 9.3041e-04,
        2.1764e-02, 1.6214e-02, 5.6475e-03, 5.7276e-03, 1.1524e-02, 1.2925e-02,
        1.0318e-02, 2.3928e-03, 2.6898e-03, 1.0364e-01, 8.2849e-03, 4.2610e-03,
        1.4148e-01, 3.6718e-03, 1.7754e-02, 1.3446e-02, 7.2389e-03, 8.3923e-03,
        1.3773e-02, 2.9674e-02, 2.6754e-02, 9.5184e-03, 3.2540e-03, 7.8509e-04,
        4.3543e-02, 4.2098e-02, 9.8746e-04, 1.1894e-02, 4.3983e-03, 4.8268e-02,
        1.3743e-02, 6.1215e-04, 1.3224e-02, 1.2323e-02, 4.2154e-02, 2.5367e-02,
        1.6254e-02, 1.3730e-02, 6.4320e-02, 4.1057e-03, 6.7030e-03, 1.6866e-02,
        6.2101e-02, 1.6485e-02, 2.5149e-02, 4.1828e-02, 7.5231e-03, 2.4282e-03,
        3.4044e-03, 4.8306e-03, 2.4965e-02, 3.4466e-02, 1.4917e-02, 1.3371e-02,
        2.1824e-02, 1.1339e-03, 6.6276e-03, 1.2884e-02, 4.6933e-02, 4.4922e-02,
        5.5381e-03, 1.2710e-02, 4.4529e-02, 2.3726e-02, 3.8491e-02, 1.4928e-02,
        2.8750e-02, 2.4997e-03, 1.4293e-03, 1.4469e-02, 1.2889e-02, 1.2198e-02,
        3.1700e-03, 1.2895e-03, 6.8457e-02, 2.9942e-02, 1.3678e-02, 1.1382e-02,
        3.1289e-02, 1.6485e-03, 3.9063e-02, 8.6605e-01, 2.9481e-02, 8.5971e-03,
        2.4789e-01, 7.0986e-03, 5.5425e-03, 7.1797e-03, 8.5298e-02, 4.7958e-02,
        1.8968e-02, 8.1707e-03, 5.9681e-03, 2.8397e-02, 2.0292e-02, 5.9120e-03,
        1.1771e-03, 2.0855e-02, 2.0247e-02, 8.6334e-03, 7.0641e-03, 8.1918e-03,
        9.3954e-03, 8.2024e-02, 2.2804e-03, 1.1885e-01, 2.5871e-02, 5.7722e-03,
        1.5079e-02, 5.9215e-02, 3.9772e-03, 2.1827e-02, 1.2483e-02, 7.7384e-03,
        6.8899e-02, 1.0509e-02, 1.6041e-02, 1.4781e-03, 3.0236e-02, 3.8648e-02,
        2.0800e-03, 9.3104e-03, 1.4078e-02, 2.0105e-03, 6.4797e-03, 5.8823e-02,
        2.2588e-01, 1.3489e-02, 4.4152e-02, 6.9509e-02, 3.3535e-02, 8.8430e-03,
        5.5192e-03, 6.5108e-03, 6.0700e-03, 2.3320e-02, 3.8966e-02, 2.7285e-02,
        1.6611e-03, 7.4567e-03, 3.0014e-02, 1.0790e-03, 1.0670e-02, 2.3976e-02,
        7.9866e-03, 1.9947e-02, 3.6452e-02, 2.0288e-02, 8.5655e-03, 2.3905e-03,
        7.0432e-03, 4.7897e-02, 4.0366e-02, 9.3588e-03, 5.6001e-03, 5.6697e-03,
        2.7458e-02, 1.4447e-02, 5.3912e-03, 9.4531e-03, 1.7389e-02, 1.5690e-02,
        5.1908e-03, 2.7225e-02, 7.7876e-03, 6.9681e-03, 3.2093e-03, 3.8725e-02,
        4.3201e-03, 1.0264e-02, 1.1616e-02, 8.6118e-02, 6.7074e-03, 4.5637e-03,
        9.0264e-03, 7.3751e-03, 2.8856e-02, 1.7818e-02, 3.6863e-02, 1.2164e-02,
        6.8021e-03, 9.1666e-03, 3.8805e-03, 1.2022e-02, 5.3499e-03, 2.3527e-03,
        7.1392e-03, 2.2741e-03, 1.1849e-03, 1.8058e-02, 4.9544e-03, 1.0407e-02,
        6.3434e-02, 7.7950e-03, 5.3718e-02, 8.5043e-03, 1.8742e-02, 3.7175e-03,
        3.4874e-02, 1.5555e-02, 3.5836e-02, 7.2156e-03, 4.0224e-02, 1.1971e-02,
        1.5522e-01, 1.3716e-01, 2.7829e-02, 7.1587e-03, 5.9096e-03, 3.8481e-02,
        4.1408e-02, 1.1290e-01, 1.0321e-03, 8.1784e-02, 2.3073e-03, 3.0509e-02,
        2.5704e-02, 3.5071e-02, 1.5234e-03, 5.5836e-03, 3.0906e-02, 1.9596e-03,
        1.8160e-02, 1.7222e-02, 2.1589e-03, 4.1568e-03, 1.4273e-02, 7.8894e-02,
        3.0986e-03, 2.0555e-02, 7.5576e-03, 1.7354e-02, 6.7544e-03, 1.1260e-03,
        3.3693e-02, 1.6364e-03, 2.9528e-02, 2.8759e-02, 1.3075e-02, 3.5391e-03,
        7.9627e-03, 5.4334e-02, 1.2412e-02, 4.3285e-03, 9.7642e-02, 8.2015e-03,
        2.4233e-02, 2.2819e-02, 1.0958e-03, 2.8139e-03, 5.0963e-02, 3.4218e-03,
        9.4313e-03, 3.9315e-02, 4.9605e-02, 1.0425e-01, 2.3686e-03, 5.0026e-02,
        7.7612e-03, 5.4702e-03, 7.4165e-03, 1.3946e-03, 4.2023e-02, 1.6111e-02,
        2.4136e-02, 8.0160e-03, 1.0436e-03, 4.6804e-03, 2.0107e-03, 1.7419e-02,
        8.8171e-03, 6.5201e-03, 8.7014e-03, 7.4491e-02, 3.7844e-02, 5.1229e-02,
        6.2621e-02, 7.2930e-03, 6.8501e-02, 5.1532e-02, 1.8383e-02, 1.2483e-03,
        5.3616e-02, 1.3578e-02, 2.3480e-02, 2.9433e-02, 1.5497e-03, 1.9143e-02,
        4.4572e-02, 3.2648e-02, 6.3635e-03, 5.6019e-03, 7.2052e-02, 3.3516e-02,
        1.1562e-03, 1.0537e-03, 1.0457e-02, 2.8388e-03, 1.9056e-02, 6.6741e-03,
        6.4513e-03, 5.7186e-03, 1.0832e-02, 6.3372e-02, 1.3298e-01, 1.7387e-02,
        3.6845e-02, 1.3048e-03, 8.4211e-03, 4.9586e-02, 5.9509e-02, 1.6831e-02,
        4.2773e-03, 4.2043e-02, 5.1998e-03, 1.4899e-02, 8.1630e-03, 2.6650e-02,
        4.8746e-03, 6.2005e-03, 4.2179e-02, 6.4319e-03, 2.8676e-02, 6.1934e-03,
        5.1957e-03, 3.4885e-02, 3.0818e-02, 2.3451e-02, 1.1838e-02, 9.3603e-03,
        6.1641e-02, 6.0797e-03, 3.9582e-03, 5.2156e-02, 2.8057e-03, 2.6650e-02,
        1.9928e-03, 2.5077e-02, 7.7074e-04, 9.9252e-04, 1.8732e-02, 1.1438e-03,
        1.2189e-01, 5.4555e-03, 1.0569e-01, 2.4203e-03, 6.2680e-03, 1.5034e-02,
        4.0580e-01]), track_scores: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), pred_boxes: tensor([[-4.6363e-01,  1.5860e+01,  3.4261e-01,  ..., -7.5941e-01,
          4.1765e-03,  1.0221e-04],
        [-2.5999e+01,  2.0453e+01,  7.6815e-01,  ..., -2.4500e-01,
         -8.4732e-04,  1.3136e-03],
        [ 3.0206e+01,  6.1731e+00,  7.0703e-01,  ..., -3.4799e-01,
         -8.1343e-04, -5.1993e-04],
        ...,
        [-2.4058e+00,  2.4745e+01,  6.4873e-01,  ...,  4.1019e-01,
          8.7779e-04,  3.3310e-04],
        [ 2.8178e+01,  4.8182e+01,  7.3901e-01,  ..., -3.3784e-01,
          6.6150e-04, -5.3343e-03],
        [ 3.1940e-02, -6.2492e-02,  5.5435e-01,  ..., -9.9405e-01,
         -1.7210e-01,  6.9688e+00]]), pred_logits: tensor([[ -7.8005,  -9.2291, -10.3650,  ...,  -8.7853,  -7.8331,  -6.7131],
        [ -2.9977,  -2.7077,  -3.0400,  ...,  -5.0790,  -4.6607,  -4.8856],
        [ -3.2918,  -4.1178,  -4.6904,  ...,  -6.0773,  -4.4435,  -5.3701],
        ...,
        [ -5.0660,  -5.8891,  -6.7540,  ...,  -6.7987,  -6.7486,  -7.4395],
        [ -4.7776,  -5.3053,  -5.1385,  ...,  -6.4729,  -4.1823,  -6.3856],
        [ -0.3814,  -5.0945,  -5.5865,  ...,  -7.4560,  -4.3447,  -5.7929]]), mem_bank: tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.6297, -0.2389, -0.0543,  ..., -1.3331,  0.2644,  0.4980]],

        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [-0.4326,  0.2157,  0.5255,  ...,  0.7514, -0.2508,  0.5009]],

        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [-0.2984, -0.1301, -0.0033,  ..., -0.1002, -0.8986,  0.0752]],

        ...,

        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [-0.2986, -0.0864,  0.2624,  ...,  0.0460,  0.3023,  0.9572]],

        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [-0.7214,  0.1568,  0.6823,  ...,  0.5216, -0.5148,  0.0282]],

        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.3801,  0.3509,  1.8601,  ...,  0.6641,  0.3705,  0.6558]]]), mem_padding_mask: tensor([[ True,  True,  True, False],
        [ True,  True,  True, False],
        [ True,  True,  True, False],
        ...,
        [ True,  True,  True, False],
        [ True,  True,  True, False],
        [ True,  True,  True, False]]), save_period: tensor([3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,
        3.])])
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(89)forward()->Instances(num...        3.])])
-> return track_instances
(Pdb) --Return--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()->Instances(num...        3.])])
-> return forward_call(*input, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(703)_forward_single_frame_inference()
-> tmp = {}
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(704)_forward_single_frame_inference()
-> tmp["init_track_instances"] = self._generate_empty_tracks()
(Pdb) --Call--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(181)_generate_empty_tracks()
-> def _generate_empty_tracks(self):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(182)_generate_empty_tracks()
-> track_instances = Instances((1, 1))
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(183)_generate_empty_tracks()
-> num_queries, dim = self.query_embedding.weight.shape  # (300, 256 * 2)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(184)_generate_empty_tracks()
-> device = self.query_embedding.weight.device
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(185)_generate_empty_tracks()
-> query = self.query_embedding.weight
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(186)_generate_empty_tracks()
-> track_instances.ref_pts = self.reference_points(query[..., : dim // 2])
(Pdb) torch.Size([901, 512])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(189)_generate_empty_tracks()
-> pred_boxes_init = torch.zeros(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(190)_generate_empty_tracks()
-> (len(track_instances), 10), dtype=torch.float, device=device
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(189)_generate_empty_tracks()
-> pred_boxes_init = torch.zeros(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(192)_generate_empty_tracks()
-> track_instances.query = query
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(194)_generate_empty_tracks()
-> track_instances.output_embedding = torch.zeros(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(195)_generate_empty_tracks()
-> (num_queries, dim >> 1), device=device
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(194)_generate_empty_tracks()
-> track_instances.output_embedding = torch.zeros(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(198)_generate_empty_tracks()
-> track_instances.obj_idxes = torch.full(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(199)_generate_empty_tracks()
-> (len(track_instances),), -1, dtype=torch.long, device=device
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(198)_generate_empty_tracks()
-> track_instances.obj_idxes = torch.full(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(201)_generate_empty_tracks()
-> track_instances.matched_gt_idxes = torch.full(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(202)_generate_empty_tracks()
-> (len(track_instances),), -1, dtype=torch.long, device=device
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(201)_generate_empty_tracks()
-> track_instances.matched_gt_idxes = torch.full(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(204)_generate_empty_tracks()
-> track_instances.disappear_time = torch.zeros(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(205)_generate_empty_tracks()
-> (len(track_instances),), dtype=torch.long, device=device
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(204)_generate_empty_tracks()
-> track_instances.disappear_time = torch.zeros(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(208)_generate_empty_tracks()
-> track_instances.iou = torch.zeros(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(209)_generate_empty_tracks()
-> (len(track_instances),), dtype=torch.float, device=device
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(208)_generate_empty_tracks()
-> track_instances.iou = torch.zeros(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(211)_generate_empty_tracks()
-> track_instances.scores = torch.zeros(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(212)_generate_empty_tracks()
-> (len(track_instances),), dtype=torch.float, device=device
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(211)_generate_empty_tracks()
-> track_instances.scores = torch.zeros(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(214)_generate_empty_tracks()
-> track_instances.track_scores = torch.zeros(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(215)_generate_empty_tracks()
-> (len(track_instances),), dtype=torch.float, device=device
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(214)_generate_empty_tracks()
-> track_instances.track_scores = torch.zeros(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(218)_generate_empty_tracks()
-> track_instances.pred_boxes = pred_boxes_init
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(220)_generate_empty_tracks()
-> track_instances.pred_logits = torch.zeros(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(221)_generate_empty_tracks()
-> (len(track_instances), self.num_classes), dtype=torch.float, device=device
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(220)_generate_empty_tracks()
-> track_instances.pred_logits = torch.zeros(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(224)_generate_empty_tracks()
-> mem_bank_len = self.mem_bank_len
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(225)_generate_empty_tracks()
-> track_instances.mem_bank = torch.zeros(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(226)_generate_empty_tracks()
-> (len(track_instances), mem_bank_len, dim // 2),
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(227)_generate_empty_tracks()
-> dtype=torch.float32,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(228)_generate_empty_tracks()
-> device=device,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(225)_generate_empty_tracks()
-> track_instances.mem_bank = torch.zeros(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(230)_generate_empty_tracks()
-> track_instances.mem_padding_mask = torch.ones(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(231)_generate_empty_tracks()
-> (len(track_instances), mem_bank_len), dtype=torch.bool, device=device
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(230)_generate_empty_tracks()
-> track_instances.mem_padding_mask = torch.ones(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(233)_generate_empty_tracks()
-> track_instances.save_period = torch.zeros(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(234)_generate_empty_tracks()
-> (len(track_instances),), dtype=torch.float32, device=device
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(233)_generate_empty_tracks()
-> track_instances.save_period = torch.zeros(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(237)_generate_empty_tracks()
-> return track_instances.to(self.query_embedding.weight.device)
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(237)_generate_empty_tracks()->Instances(num...0., 0., 0.])])
-> return track_instances.to(self.query_embedding.weight.device)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(705)_forward_single_frame_inference()
-> tmp["track_instances"] = track_instances
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(706)_forward_single_frame_inference()
-> out_track_instances = self.query_interact(tmp)
(Pdb) --Call--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1117)__getattr__()
-> def __getattr__(self, name: str) -> Union[Tensor, 'Module']:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1118)__getattr__()
-> if '_parameters' in self.__dict__:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1119)__getattr__()
-> _parameters = self.__dict__['_parameters']
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1120)__getattr__()
-> if name in _parameters:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1122)__getattr__()
-> if '_buffers' in self.__dict__:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1123)__getattr__()
-> _buffers = self.__dict__['_buffers']
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1124)__getattr__()
-> if name in _buffers:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1126)__getattr__()
-> if '_modules' in self.__dict__:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1127)__getattr__()
-> modules = self.__dict__['_modules']
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1128)__getattr__()
-> if name in modules:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1129)__getattr__()
-> return modules[name]
(Pdb) --Return--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1129)__getattr__()->QueryInteract...place=False)
)
-> return modules[name]
(Pdb) --Call--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1045)_call_impl()
-> def _call_impl(self, *input, **kwargs):
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1046)_call_impl()
-> forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1049)_call_impl()
-> if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1050)_call_impl()
-> or _global_forward_hooks or _global_forward_pre_hooks):
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1049)_call_impl()
-> if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1050)_call_impl()
-> or _global_forward_hooks or _global_forward_pre_hooks):
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1049)_call_impl()
-> if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) --Call--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(247)forward()
-> def forward(self, data) -> Instances:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(248)forward()
-> import pdb; pdb.set_trace()
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(249)forward()
-> active_track_instances = self._select_active_tracks(data)
(Pdb) --Call--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(229)_select_active_tracks()
-> def _select_active_tracks(self, data: dict) -> Instances:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(230)_select_active_tracks()
-> track_instances: Instances = data["track_instances"]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(231)_select_active_tracks()
-> if self.training:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(242)_select_active_tracks()
-> active_track_instances = track_instances[
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(243)_select_active_tracks()
-> track_instances.obj_idxes >= 0]
(Pdb) tensor([False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False,  True, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False,  True, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False,  True, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(242)_select_active_tracks()
-> active_track_instances = track_instances[
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(245)_select_active_tracks()
-> return active_track_instances
(Pdb) Instances(num_instances=3, image_height=1, image_width=1, fields=[ref_pts: tensor([[-0.1737, -0.4143, -0.4898],
        [-0.0294, -1.0461, -0.6533],
        [-0.4897,  1.7039,  0.0219]]), query: tensor([[-0.5876,  1.6050, -2.0626,  ..., -0.8195,  2.2342,  1.4749],
        [-0.5229, -0.1827, -0.5886,  ..., -0.1446, -0.1752, -0.6182],
        [-0.1590,  0.5117,  0.0389,  ..., -0.4621, -1.1121, -0.6192]]), output_embedding: tensor([[ 7.2694e-01,  3.1695e-01, -3.3184e-01,  1.1483e+00,  1.5224e+00,
         -5.9879e-01,  7.3964e-02, -6.6572e-01, -3.5503e-01, -9.8590e-02,
          1.1947e+00, -2.6883e-01, -4.5232e-01,  5.6389e-01,  5.5222e-01,
         -3.0688e-01,  1.5410e-01,  1.3478e+00,  1.5335e-02, -3.0108e-01,
         -1.9055e-01,  6.2463e-02,  4.9710e-01,  6.6413e-01, -1.2193e+00,
         -7.5995e-01,  1.1914e+00, -5.0798e-03, -7.6076e-03, -5.7233e-01,
         -5.7443e-01,  4.2273e-01,  7.1485e-02, -4.1189e-01,  3.5460e-01,
         -6.7798e-01, -1.2680e+00, -6.5361e-01,  7.6835e-01,  8.9665e-01,
         -5.0035e-01,  1.0631e-01,  8.2272e-01,  1.8325e+00,  8.0111e-01,
         -2.4224e-01, -4.4750e-01, -4.3873e-01,  3.5413e-01,  9.6047e-01,
         -4.4461e-01, -4.5955e-01,  4.4361e-01,  7.5620e-01, -3.0593e-01,
         -2.6154e-01,  5.9923e-01,  2.2267e-01, -8.2645e-01,  2.1011e-01,
          9.4936e-01, -3.6755e-01, -1.9930e-01,  4.3162e-01, -1.0218e+00,
          9.6713e-01,  1.6494e+00,  2.4206e-01, -3.9320e-01, -3.2703e-01,
          8.2183e-01,  4.3381e-01,  3.0251e-02,  8.4313e-01, -1.7448e+00,
          3.9759e-01,  3.6216e-01, -1.1021e+00,  2.3605e-01, -3.1720e-01,
         -2.0022e+00, -2.3310e+00,  9.9953e-01, -8.5519e-01, -2.0730e+00,
         -1.6531e+00,  7.7287e-01,  3.1795e-01, -9.9628e-01,  6.0382e-01,
         -5.1290e-01, -4.6506e-01, -3.4594e-01, -7.4537e-01, -7.5967e-02,
         -3.6659e-01,  3.7468e-01,  8.3924e-01, -3.1773e-01, -1.0420e+00,
          4.9966e-01, -6.6588e-01, -2.5750e-01, -2.9709e+00,  1.2574e+00,
         -3.5731e-01, -4.8655e-01, -4.8249e-01, -1.1307e+00, -2.9750e-02,
         -1.5870e+00,  5.9637e-02, -1.7909e-01,  6.1030e-01,  6.5910e-01,
         -4.5963e-01, -5.2525e-02,  7.4953e-01, -7.2478e-03, -5.4064e-01,
          9.7136e-02,  8.7130e-01, -4.1656e-01, -7.1769e-01, -5.4666e-02,
          2.0464e+00,  1.4173e+00, -1.2885e-01,  1.1901e-01, -1.0678e+00,
          5.4064e-01,  4.9471e-01, -6.6469e-01, -2.9270e-01,  8.7610e-01,
         -4.9644e-01, -3.8483e-01, -5.8687e-02,  1.3609e-01, -1.5492e-01,
          1.0125e+00, -6.2630e-01,  7.6383e-01, -2.7331e-01, -7.6137e-01,
          9.1030e-01,  1.1017e+00,  3.3450e-01,  5.8950e-01,  6.5643e-01,
          4.0939e-01,  2.9196e-01, -2.5082e-01,  1.7208e+00,  3.1487e-01,
          3.2712e-01,  5.8992e-01,  8.9243e-01, -2.4007e-01,  4.6560e-01,
         -5.8472e-01, -1.2109e+00, -6.9805e-01, -2.2465e-01,  6.3318e-01,
          1.7725e-01, -7.3745e-01,  1.7641e+00,  9.6645e-02,  6.4937e-03,
         -1.1891e-01,  4.8954e-01,  1.5640e-01,  1.2476e-01,  1.5282e+00,
          3.8717e-01,  4.3892e-01, -5.4581e-02, -2.9244e-01, -1.5993e+00,
          8.2941e-01, -1.8160e+00, -8.9961e-01,  2.7121e-01, -5.5043e-01,
         -4.4373e-01, -5.4099e-01, -5.7787e-01,  1.8372e+00, -4.5881e-01,
         -7.1424e-01, -1.0763e+00, -3.4648e-01,  1.4664e-02,  4.0678e-01,
         -8.4456e-01, -2.5652e-01,  4.8034e-01, -6.5535e-01, -4.0944e-02,
          3.5053e-01,  1.4080e+00,  1.0351e+00, -1.8653e-01,  4.6054e-01,
         -3.7751e-02, -6.9706e-01, -1.7872e-01, -1.8899e+00,  7.4991e-01,
          1.4765e+00, -3.4634e-01,  8.3284e-02, -5.2292e-01, -4.0594e-01,
          5.8082e-01, -6.6763e-01,  4.4188e-01, -5.0132e-01,  1.2265e+00,
          1.4121e-01,  2.9375e-01,  1.8764e+00,  5.4309e-01, -1.4453e-01,
         -2.8427e-01, -1.9991e-01, -2.1194e-01,  5.5494e-01, -1.2598e+00,
         -8.5878e-01, -3.2394e-01, -4.0180e-01, -1.9010e-01,  4.2423e-01,
         -5.4407e-01, -1.1354e+00,  1.1981e+00,  9.5252e-01, -6.6023e-01,
          7.0692e-01, -1.2207e-01, -5.5430e-04,  1.5154e-01, -8.5813e-01,
         -1.4299e+00,  2.5044e-01, -4.7223e-01,  9.8596e-01,  8.9382e-01,
          1.7118e-01, -4.6269e-01,  4.0933e-01,  5.5163e-01, -5.7390e-01,
          2.6532e-01],
        [ 3.5175e-01, -3.5604e-01, -9.7067e-02,  1.8338e-01, -1.4753e-01,
          3.8563e-02, -1.3317e-01, -1.9324e-01, -5.1011e-02,  3.1631e-02,
          2.6219e-01,  2.0009e+00, -5.2623e-01,  6.2971e-01,  4.1340e-01,
         -1.5432e+00, -2.3047e-01, -3.2278e-01, -6.5874e-01, -1.4835e-01,
         -2.8788e-01,  1.3388e+00, -6.7844e-02, -3.3641e-01, -1.7970e+00,
         -7.2196e-02,  1.1957e+00,  5.1432e-01, -4.5699e-01, -4.7573e-01,
          5.3592e-01,  5.5428e-02, -2.0762e-01, -9.9296e-01, -4.0767e-01,
         -8.0755e-01, -4.1640e-01,  8.4198e-01,  5.7631e-02,  4.2826e-01,
         -1.1275e+00, -7.6334e-01,  3.1749e-01,  1.5643e+00,  6.7910e-01,
         -4.4407e-01, -4.6601e-02, -2.9539e-02, -2.1661e-01,  7.3895e-02,
         -4.0169e-01,  6.1455e-01,  3.0145e-01,  3.3305e-01,  3.2219e-02,
          2.2168e-01,  1.1359e+00,  3.0313e-01,  1.0197e+00, -7.0802e-01,
          1.8707e-01,  1.6715e-01, -1.4158e+00,  4.1564e-01, -7.4775e-01,
          5.1793e-01, -1.7391e-01,  7.0185e-01,  4.6832e-01, -6.5214e-01,
         -1.6975e-01, -9.6631e-02,  2.2942e-01,  1.1551e-01, -6.6892e-01,
          1.1216e-01,  4.1095e-01, -6.5190e-01, -7.0968e-01, -5.0184e-01,
         -5.7705e-01, -1.4119e+00, -7.5816e-02, -6.7831e-01, -5.5205e-01,
         -1.1766e+00,  3.5930e-01, -4.2231e-01, -1.4423e-01,  2.0953e-01,
         -3.6977e-01,  4.3729e-01,  3.0353e-01, -5.6471e-01,  4.9197e-02,
         -6.1611e-01, -3.1157e-01,  6.7363e-01, -4.2133e-01, -1.9759e-01,
         -9.6557e-01, -5.6422e-01, -1.6308e-02, -1.5764e+00,  3.1522e-01,
         -5.1310e-01, -7.4084e-01, -8.7649e-02, -2.4856e+00, -1.4375e+00,
         -2.0956e+00,  6.6836e-01, -1.7447e-01,  4.7054e-01,  4.5285e-01,
         -6.6924e-01,  1.3629e+00,  8.7438e-01, -7.0519e-01, -2.2270e-01,
         -6.2955e-01,  1.9143e+00,  2.5757e-02, -4.5248e-01, -3.0948e-01,
          4.8609e-01,  1.0507e+00, -5.8151e-01,  9.5013e-02, -6.8459e-01,
          1.5353e+00, -2.1641e-01, -1.0664e+00,  3.2953e-01, -6.4453e-01,
         -9.4313e-01, -2.5347e-01, -3.0617e-01, -9.2170e-01,  4.2102e-01,
          6.5960e-01, -2.1843e-02, -4.5122e-02, -6.4178e-01, -4.7056e-02,
          6.9719e-01, -1.1707e-01, -6.0526e-01, -2.7057e-01,  9.4504e-01,
          5.4400e-01,  1.1060e-01, -9.5943e-01,  4.7167e+00, -2.9433e-01,
          2.9216e-03, -5.2925e-01,  5.3396e-01,  1.3864e-01, -4.1258e-01,
         -6.4463e-01, -1.8232e-01, -1.2657e+00,  1.3741e+00, -1.5489e-01,
          6.2267e-01, -1.0823e+00,  2.0617e+00,  3.7629e-01, -6.1748e-01,
          1.0548e+00,  6.3961e-01,  2.6850e-01,  5.9275e-01,  1.3276e+00,
          1.0104e-01,  4.1088e-01, -3.1435e-01,  3.2603e-01, -3.3215e-01,
          3.2694e+00, -2.2436e-01,  6.9167e-02,  3.2299e-01, -9.0835e-01,
         -1.2739e+00, -1.0349e+00,  7.6532e-01,  7.2499e-01,  3.0560e-01,
         -3.1952e-01, -2.2890e-01,  6.3951e-01,  7.7289e-02, -1.0208e-01,
         -8.7849e-01,  1.1870e+00,  7.8377e-01, -1.8962e-01,  1.4940e-01,
         -3.0071e-01,  5.9864e-01, -1.5389e-01, -5.4247e-01, -3.4934e-01,
         -2.3072e-01,  6.7218e-01,  2.0196e-01, -1.3372e+00,  3.6862e-01,
         -4.0789e-01,  4.2295e-01, -3.4080e-01,  1.9408e-01,  4.9969e-01,
          5.5824e-01, -4.7848e-01,  2.0658e+00, -2.3163e-02, -2.8768e-01,
          1.1680e+00,  5.1867e-01,  9.1732e-01, -4.0654e-01,  1.3632e-01,
         -2.6899e-01, -3.9827e-02, -1.7757e-01, -2.6666e-01, -1.4132e+00,
         -7.0022e-01,  1.1015e-01,  6.1303e-01, -1.5691e-01,  3.0138e-01,
          3.2780e-01,  9.2632e-01,  8.9005e-01,  4.8667e-01,  6.1024e-01,
          2.7990e-01,  5.8607e-01,  3.9382e-01,  1.1831e-01, -3.3819e-01,
         -1.9828e+00, -8.4637e-02,  9.1902e-02,  7.1806e-01,  9.8545e-01,
          7.8920e-01, -2.2768e-01, -1.0494e+00,  4.9757e-01, -8.4579e-01,
          1.1767e-01],
        [-8.5410e-01, -1.3653e-02, -5.9994e-01, -3.9260e-01,  8.6629e-01,
          4.4574e-01,  5.8994e-02, -4.4518e-01,  1.6066e-01,  2.8581e-01,
          9.2751e-01,  3.6498e-01,  5.9080e-01,  5.9024e-01,  3.6513e-01,
         -1.9271e-01, -1.1359e+00,  1.1296e+00, -5.9634e-01,  1.7707e-01,
          7.0479e-01,  2.9443e-01,  3.4823e-01, -6.5629e-03, -7.7836e-01,
         -3.9578e-01,  4.6806e-01,  8.2694e-01, -3.4464e-01, -1.6236e+00,
         -6.6032e-02,  3.7356e-01,  1.7833e-01, -5.4454e-01, -8.0832e-01,
         -2.1392e-01, -2.9915e-01, -2.5563e-02,  8.7833e-01,  6.8929e-01,
         -1.0603e+00,  1.8376e+00,  4.4792e-01, -3.0744e+00,  9.8138e-01,
          5.3489e-01,  7.1467e-01, -1.1210e-01,  2.1846e-02,  4.5077e-01,
          2.4371e+00,  1.9617e-01, -1.5992e-01, -3.7091e-01, -8.4181e-01,
          5.7519e-01,  9.3733e-01,  7.2198e-01,  9.3957e-01, -6.8650e-01,
         -2.9789e-01, -2.9121e-01, -7.9973e-01,  5.4766e-01,  3.1243e-01,
          6.3460e-01,  4.6200e-01, -8.5588e-01, -2.1389e-01, -2.5874e-01,
         -1.9357e-02, -1.5903e-01, -7.0800e-01, -2.8155e-01,  6.7967e-01,
          2.8297e-03, -1.0327e+00, -4.2943e-01,  3.8248e-03, -1.8320e-01,
          1.4094e+00, -1.4613e+00, -7.3006e-01,  3.1213e-01, -8.8545e-01,
         -1.0902e+00, -5.3080e-01, -2.5854e-02,  4.6226e-01, -1.9763e-01,
         -1.4980e-02,  5.7094e-01,  3.7040e-01,  3.1193e-01,  5.4305e-01,
          5.5403e-02,  4.3927e-01, -8.1247e-01, -7.4195e-01,  6.5472e-01,
          3.2516e-01,  1.7001e-01,  5.6941e-01, -9.2623e-01, -1.5805e-01,
         -5.5653e-01, -1.5697e+00,  9.5220e-01, -1.7465e+00, -6.4865e-01,
          1.0835e+00,  1.2268e+00, -2.2257e-01, -1.3739e+00, -3.7203e-01,
         -4.4517e-02, -6.1463e-01, -5.5013e-01, -1.7509e-01,  1.3676e-01,
          6.9227e-01,  3.8846e-01,  2.7001e-01,  3.6216e-01,  6.5690e-01,
         -7.4120e-01,  8.6808e-02,  1.2616e+00,  7.2347e-01,  6.1484e-01,
          1.3079e+00, -1.5750e-01, -1.1012e+00,  4.8019e-01, -2.9056e-01,
         -6.3401e-02, -9.3528e-02, -1.4350e+00, -5.2750e-01,  8.2331e-01,
          2.1115e-01,  2.8066e-01,  6.4499e-01,  6.8153e-01,  7.0916e-01,
          1.3536e+00, -5.2912e-01, -2.0708e-01,  1.3237e+00,  3.5245e-01,
          3.7896e-01,  3.6975e-01, -8.0600e-01,  2.7276e+00,  1.6323e+00,
          9.3643e-01, -3.3077e-01,  3.8006e-02,  6.4981e-01,  5.3370e-02,
         -8.6701e-01, -4.8912e-01,  1.3947e-03, -7.4726e-01, -6.1742e-01,
         -9.0252e-02, -5.9249e-01,  1.1394e+00,  8.5479e-01, -5.6317e-01,
         -1.2356e-02, -7.6427e-01, -8.4514e-01, -1.0813e-01,  1.3639e+00,
          1.5924e-01, -8.5771e-01, -3.2660e-01,  7.5353e-01,  2.6395e-01,
         -3.4856e-01,  3.0623e-01, -1.4393e+00,  9.4517e-01,  7.5477e-02,
         -2.2159e+00,  6.3105e-02,  5.3979e-01,  2.9668e-01, -1.2851e+00,
          8.6741e-01, -7.2136e-01,  7.5946e-02, -1.0501e-01, -6.0374e-01,
         -8.5043e-01, -2.0430e-01,  1.4407e+00,  5.0037e-01, -4.4212e-01,
          4.4270e-01,  1.1354e+00, -4.4780e-01,  3.9675e-01,  6.3906e-02,
          2.4338e-01, -3.6491e-01,  5.4376e-01, -7.8556e-01,  2.9817e-01,
          3.8700e-01, -9.5799e-02, -1.2757e+00, -1.0806e+00,  6.3144e-01,
          1.3459e+00,  2.8040e-01, -1.4387e+00,  1.4059e-01, -2.2057e-01,
         -1.2620e+00, -9.9024e-02, -1.3269e+00, -8.0128e-01,  3.2628e-02,
         -3.9028e-02,  4.2750e-01,  2.6276e-01,  3.2275e-01, -3.5256e+00,
          6.9590e-01,  4.7463e-01,  3.0509e-02, -8.9338e-01,  1.1479e+00,
         -1.1521e+00, -1.6299e+00, -7.0114e-01, -8.3033e-01, -6.0437e-02,
         -1.2732e-01,  1.6454e-01,  4.5053e-01,  1.5171e+00, -4.1756e-02,
          2.3419e-01,  2.5649e-01, -5.3035e-01, -2.1445e-01,  9.9852e-01,
          4.9138e-01,  1.1628e-01, -7.1855e-01,  9.3856e-02, -4.6179e-01,
          3.9456e-01]]), obj_idxes: tensor([0, 1, 2]), matched_gt_idxes: tensor([-1, -1, -1]), disappear_time: tensor([0, 0, 0]), iou: tensor([0., 0., 0.]), scores: tensor([0.8401, 0.8999, 0.8660]), track_scores: tensor([0., 0., 0.]), pred_boxes: tensor([[-4.4362e+00, -1.0457e+01, -5.7496e-01,  5.4815e-01, -1.9604e+00,
          8.2555e-02,  2.6890e-02,  9.9731e-01, -1.2011e-03, -9.9059e-04],
        [-7.5262e-01, -2.4579e+01,  6.6525e-01,  1.5121e+00, -2.2621e+00,
          4.3397e-01, -1.8351e-02, -9.8844e-01,  8.8571e-02,  5.5166e+00],
        [-1.2292e+01,  3.5435e+01, -3.3134e-01, -2.0525e-01, -9.5619e-01,
          5.5846e-01,  1.1792e-01, -9.8280e-01, -2.9149e-01,  1.2141e+00]]), pred_logits: tensor([[ -6.4774,  -8.0849,  -7.5573, -11.3790,  -6.8656,  -5.9882,  -2.8453,
           1.6590,  -5.5511,  -4.6095],
        [  2.1958,  -6.2505,  -7.5734,  -7.5439, -13.5229,  -6.9315,  -7.1278,
          -9.5285,  -7.1040,  -6.2322],
        [ -8.0861,  -7.3184,  -7.0678,  -7.8600,  -6.4306,  -4.7218,  -6.1443,
          -5.5766,   1.8664,  -5.3478]]), mem_bank: tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [-0.5029, -0.2620,  0.0386,  ...,  0.0127,  0.3238, -0.2907]],

        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.2549,  0.2009,  0.5347,  ...,  0.1464,  0.1376,  0.4046]],

        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.2789, -0.3221,  0.8840,  ...,  0.7337,  0.1182,  0.8018]]]), mem_padding_mask: tensor([[ True,  True,  True, False],
        [ True,  True,  True, False],
        [ True,  True,  True, False]]), save_period: tensor([3., 3., 3.])])
(Pdb) tensor([[-0.5876,  1.6050, -2.0626,  ..., -0.8195,  2.2342,  1.4749],
        [-0.5229, -0.1827, -0.5886,  ..., -0.1446, -0.1752, -0.6182],
        [-0.1590,  0.5117,  0.0389,  ..., -0.4621, -1.1121, -0.6192]])
(Pdb) torch.Size([3, 512])
(Pdb) *** Newest frame
(Pdb) 240  	                    track_instances, active_track_instances)
241  	        else:
242  	            active_track_instances = track_instances[
243  	                track_instances.obj_idxes >= 0]
244  	
245  ->	        return active_track_instances
246  	
247  	    def forward(self, data) -> Instances:
248  	        import pdb; pdb.set_trace()
249  	        active_track_instances = self._select_active_tracks(data)
250  	        active_track_instances = self._update_track_embedding(
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(245)_select_active_tracks()->Instances(num...3., 3., 3.])])
-> return active_track_instances
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(250)forward()
-> active_track_instances = self._update_track_embedding(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(251)forward()
-> active_track_instances)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(250)forward()
-> active_track_instances = self._update_track_embedding(
(Pdb) --Call--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(151)_update_track_embedding()
-> def _update_track_embedding(self, track_instances: Instances) -> Instances:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(152)_update_track_embedding()
-> if len(track_instances) == 0:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(154)_update_track_embedding()
-> dim = track_instances.query.shape[1]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(155)_update_track_embedding()
-> out_embed = track_instances.output_embedding
(Pdb) 512
(Pdb) torch.Size([3, 256])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(156)_update_track_embedding()
-> query_pos = track_instances.query[:, :dim // 2]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(157)_update_track_embedding()
-> query_feat = track_instances.query[:, dim // 2:]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(158)_update_track_embedding()
-> q = k = query_pos + out_embed
(Pdb) torch.Size([3, 512])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(250)forward()
-> active_track_instances = self._update_track_embedding(
(Pdb) *** NameError: name 'track_instances' is not defined
(Pdb) *** NameError: name 'track_instances' is not defined
(Pdb) 245  	        return active_track_instances
246  	
247  	    def forward(self, data) -> Instances:
248  	        import pdb; pdb.set_trace()
249  	        active_track_instances = self._select_active_tracks(data)
250  ->	        active_track_instances = self._update_track_embedding(
251  	            active_track_instances)
252  	        init_track_instances: Instances = data["init_track_instances"]
253  	        merged_track_instances = Instances.cat(
254  	            [init_track_instances, active_track_instances])
255  	        return merged_track_instances
(Pdb) torch.Size([901, 512])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(158)_update_track_embedding()
-> q = k = query_pos + out_embed
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(161)_update_track_embedding()
-> tgt = out_embed
(Pdb) torch.Size([3, 256])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(162)_update_track_embedding()
-> tgt2 = self.self_attn(q[:, None], k[:, None], value=tgt[:, None])[0][:,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(163)_update_track_embedding()
-> 0]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(162)_update_track_embedding()
-> tgt2 = self.self_attn(q[:, None], k[:, None], value=tgt[:, None])[0][:,
(Pdb) *** NameError: name 'tgt2' is not defined
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(164)_update_track_embedding()
-> tgt = tgt + self.dropout1(tgt2)
(Pdb) torch.Size([3, 256])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(165)_update_track_embedding()
-> tgt = self.norm1(tgt)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(168)_update_track_embedding()
-> tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(169)_update_track_embedding()
-> tgt = tgt + self.dropout2(tgt2)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(170)_update_track_embedding()
-> tgt = self.norm2(tgt)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(172)_update_track_embedding()
-> if self.update_query_pos:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(174)_update_track_embedding()
-> query_pos2 = self.linear_pos2(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(175)_update_track_embedding()
-> self.dropout_pos1(self.activation(self.linear_pos1(tgt))))
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(174)_update_track_embedding()
-> query_pos2 = self.linear_pos2(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(176)_update_track_embedding()
-> query_pos = query_pos + self.dropout_pos2(query_pos2)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(177)_update_track_embedding()
-> query_pos = self.norm_pos(query_pos)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(178)_update_track_embedding()
-> track_instances.query[:, :dim // 2] = query_pos
(Pdb) torch.Size([3, 256])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(180)_update_track_embedding()
-> query_feat2 = self.linear_feat2(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(181)_update_track_embedding()
-> self.dropout_feat1(self.activation(self.linear_feat1(tgt))))
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(180)_update_track_embedding()
-> query_feat2 = self.linear_feat2(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(182)_update_track_embedding()
-> query_feat = query_feat + self.dropout_feat2(query_feat2)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(183)_update_track_embedding()
-> query_feat = self.norm_feat(query_feat)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(184)_update_track_embedding()
-> track_instances.query[:, dim // 2:] = query_feat
(Pdb) torch.Size([3, 256])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(187)_update_track_embedding()
-> return track_instances
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(187)_update_track_embedding()->Instances(num...3., 3., 3.])])
-> return track_instances
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(252)forward()
-> init_track_instances: Instances = data["init_track_instances"]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(253)forward()
-> merged_track_instances = Instances.cat(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(254)forward()
-> [init_track_instances, active_track_instances])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(253)forward()
-> merged_track_instances = Instances.cat(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(255)forward()
-> return merged_track_instances
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py(255)forward()->Instances(num...3., 3., 3.])])
-> return merged_track_instances
(Pdb) --Return--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()->Instances(num...3., 3., 3.])])
-> return forward_call(*input, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(707)_forward_single_frame_inference()
-> out["track_instances_fordet"] = track_instances
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(708)_forward_single_frame_inference()
-> out["track_instances"] = out_track_instances
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(709)_forward_single_frame_inference()
-> out["track_obj_idxes"] = track_instances.obj_idxes
(Pdb) torch.Size([901])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(710)_forward_single_frame_inference()
-> return out
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(710)_forward_single_frame_inference()->{'all_past_traj_preds': tensor([[[[[-...4107e+01]]]]]), 'bbox_index': tensor([1, 2, 0]), 'bev_embed': tensor([[[-0....6, -0.1223]]]), 'bev_pos': tensor([[[[ 1...,  0.1637]]]]), ...}
-> return out
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(765)simple_test_track()
-> self.prev_bev = frame_res["bev_embed"]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(766)simple_test_track()
-> track_instances = frame_res["track_instances"]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(767)simple_test_track()
-> track_instances_fordet = frame_res["track_instances_fordet"]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(769)simple_test_track()
-> self.test_track_instances = track_instances
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(770)simple_test_track()
-> results = [dict()]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(771)simple_test_track()
-> get_keys = ["bev_embed", "bev_pos",
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(772)simple_test_track()
-> "track_query_embeddings", "track_bbox_results",
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(773)simple_test_track()
-> "boxes_3d", "scores_3d", "labels_3d", "track_scores", "track_ids"]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(771)simple_test_track()
-> get_keys = ["bev_embed", "bev_pos",
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(774)simple_test_track()
-> if self.with_motion_head:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(775)simple_test_track()
-> get_keys += ["sdc_boxes_3d", "sdc_scores_3d", "sdc_track_scores", "sdc_track_bbox_results", "sdc_embedding"]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(776)simple_test_track()
-> results[0].update({k: frame_res[k] for k in get_keys})
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(777)simple_test_track()
-> results = self._det_instances2results(track_instances_fordet, results, img_metas)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(778)simple_test_track()
-> return results
(Pdb) *** AttributeError: 'list' object has no attribute 'keys'
(Pdb) *** AttributeError: 'dict' object has no attribute 'shape'
(Pdb) dict_keys(['bev_embed', 'bev_pos', 'track_query_embeddings', 'track_bbox_results', 'boxes_3d', 'scores_3d', 'labels_3d', 'track_scores', 'track_ids', 'sdc_boxes_3d', 'sdc_scores_3d', 'sdc_track_scores', 'sdc_track_bbox_results', 'sdc_embedding', 'boxes_3d_det', 'scores_3d_det', 'labels_3d_det'])
(Pdb) 17
(Pdb) *** NameError: name 'result' is not defined
(Pdb) 1
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(778)simple_test_track()->[{'bev_embed': tensor([[[-0....6, -0.1223]]]), 'bev_pos': tensor([[[[ 1...,  0.1637]]]]), 'boxes_3d': LiDARInstance...9.9059e-04]])), 'boxes_3d_det': LiDARInstance...2.4743e-03]])), ...}]
-> return results
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py(294)forward_test()
-> import pdb; pdb.set_trace()
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py(297)forward_test()
-> result_track[0] = self.upsample_bev_if_tiny(result_track[0])
(Pdb) --Call--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(586)upsample_bev_if_tiny()
-> def upsample_bev_if_tiny(self, outs_track):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(587)upsample_bev_if_tiny()
-> if outs_track["bev_embed"].size(0) == 100 * 100:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(620)upsample_bev_if_tiny()
-> return outs_track
(Pdb) <built-in method size of Tensor object at 0x2a4f0b200>
(Pdb) torch.Size([40000, 1, 256])
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(620)upsample_bev_if_tiny()->{'bev_embed': tensor([[[-0....6, -0.1223]]]), 'bev_pos': tensor([[[[ 1...,  0.1637]]]]), 'boxes_3d': LiDARInstance...9.9059e-04]])), 'boxes_3d_det': LiDARInstance...2.4743e-03]])), ...}
-> return outs_track
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py(299)forward_test()
-> bev_embed = result_track[0]["bev_embed"]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py(301)forward_test()
-> if self.with_seg_head:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py(302)forward_test()
-> result_seg =  self.seg_head.forward_test(bev_embed, gt_lane_labels, gt_lane_masks, img_metas, rescale)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/panseg_head.py(1012)forward_test()
-> bbox_list = [dict() for i in range(len(img_metas))]
(Pdb) 1007 	                    gt_lane_labels=None,
1008 	                    gt_lane_masks=None,
1009 	                    img_metas=None,
1010 	                    rescale=False):
1011 	        import pdb; pdb.set_trace()
1012 ->	        bbox_list = [dict() for i in range(len(img_metas))]
1013 	
1014 	        pred_seg_dict = self(pts_feats)
1015 	        results = self.get_bboxes(pred_seg_dict['outputs_classes'],
1016 	                                           pred_seg_dict['outputs_coords'],
1017 	                                           pred_seg_dict['enc_outputs_class'],
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/panseg_head.py(1014)forward_test()
-> pred_seg_dict = self(pts_feats)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/seg_head_plugin/seg_deformable_transformer.py(319)forward()
-> memory = self.encoder(query=feat_flatten,
(Pdb) 314  	
315  	        feat_flatten = feat_flatten.permute(1, 0, 2)  # (H*W, bs, embed_dims)
316  	        lvl_pos_embed_flatten = lvl_pos_embed_flatten.permute(
317  	            1, 0, 2)  # (H*W, bs, embed_dims)
318  	        import pdb; pdb.set_trace()
319  ->	        memory = self.encoder(query=feat_flatten,
320  	                              key=None,
321  	                              value=None,
322  	                              query_pos=lvl_pos_embed_flatten,
323  	                              query_key_padding_mask=mask_flatten,
324  	                              spatial_shapes=spatial_shapes,
(Pdb) --Call--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1117)__getattr__()
-> def __getattr__(self, name: str) -> Union[Tensor, 'Module']:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1118)__getattr__()
-> if '_parameters' in self.__dict__:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1119)__getattr__()
-> _parameters = self.__dict__['_parameters']
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1120)__getattr__()
-> if name in _parameters:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1122)__getattr__()
-> if '_buffers' in self.__dict__:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1123)__getattr__()
-> _buffers = self.__dict__['_buffers']
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1124)__getattr__()
-> if name in _buffers:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1126)__getattr__()
-> if '_modules' in self.__dict__:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1127)__getattr__()
-> modules = self.__dict__['_modules']
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1128)__getattr__()
-> if name in modules:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1129)__getattr__()
-> return modules[name]
(Pdb) --Return--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1129)__getattr__()->DetrTransform... )
    )
  )
)
-> return modules[name]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/seg_head_plugin/seg_deformable_transformer.py(320)forward()
-> key=None,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/seg_head_plugin/seg_deformable_transformer.py(321)forward()
-> value=None,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/seg_head_plugin/seg_deformable_transformer.py(322)forward()
-> query_pos=lvl_pos_embed_flatten,
(Pdb) 317  	            1, 0, 2)  # (H*W, bs, embed_dims)
318  	        import pdb; pdb.set_trace()
319  	        memory = self.encoder(query=feat_flatten,
320  	                              key=None,
321  	                              value=None,
322  ->	                              query_pos=lvl_pos_embed_flatten,
323  	                              query_key_padding_mask=mask_flatten,
324  	                              spatial_shapes=spatial_shapes,
325  	                              reference_points=reference_points,
326  	                              level_start_index=level_start_index,
327  	                              valid_ratios=valid_ratios,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/seg_head_plugin/seg_deformable_transformer.py(323)forward()
-> query_key_padding_mask=mask_flatten,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/seg_head_plugin/seg_deformable_transformer.py(324)forward()
-> spatial_shapes=spatial_shapes,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/seg_head_plugin/seg_deformable_transformer.py(325)forward()
-> reference_points=reference_points,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/seg_head_plugin/seg_deformable_transformer.py(326)forward()
-> level_start_index=level_start_index,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/seg_head_plugin/seg_deformable_transformer.py(327)forward()
-> valid_ratios=valid_ratios,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/seg_head_plugin/seg_deformable_transformer.py(319)forward()
-> memory = self.encoder(query=feat_flatten,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/seg_head_plugin/seg_deformable_transformer.py(328)forward()
-> **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/seg_head_plugin/seg_deformable_transformer.py(319)forward()
-> memory = self.encoder(query=feat_flatten,
(Pdb) --Call--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1045)_call_impl()
-> def _call_impl(self, *input, **kwargs):
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1046)_call_impl()
-> forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1049)_call_impl()
-> if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1050)_call_impl()
-> or _global_forward_hooks or _global_forward_pre_hooks):
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1049)_call_impl()
-> if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1050)_call_impl()
-> or _global_forward_hooks or _global_forward_pre_hooks):
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1049)_call_impl()
-> if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) --Call--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmdet/models/utils/transformer.py(112)forward()
-> def forward(self, *args, **kwargs):
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmdet/models/utils/transformer.py(118)forward()
-> x = super(DetrTransformerEncoder, self).forward(*args, **kwargs)
(Pdb) self = DetrTransformerEncoder(
  (layers): ModuleList(
    (0): BaseTransformerLayer(
      (attentions): ModuleList(
        (0): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (ffns): ModuleList(
        (0): FFN(
          (activate): ReLU(inplace=True)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): Linear(in_features=512, out_features=256, bias=True)
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (norms): ModuleList(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BaseTransformerLayer(
      (attentions): ModuleList(
        (0): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (ffns): ModuleList(
        (0): FFN(
          (activate): ReLU(inplace=True)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): Linear(in_features=512, out_features=256, bias=True)
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (norms): ModuleList(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BaseTransformerLayer(
      (attentions): ModuleList(
        (0): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (ffns): ModuleList(
        (0): FFN(
          (activate): ReLU(inplace=True)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): Linear(in_features=512, out_features=256, bias=True)
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (norms): ModuleList(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BaseTransformerLayer(
      (attentions): ModuleList(
        (0): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (ffns): ModuleList(
        (0): FFN(
          (activate): ReLU(inplace=True)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): Linear(in_features=512, out_features=256, bias=True)
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (norms): ModuleList(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (4): BaseTransformerLayer(
      (attentions): ModuleList(
        (0): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (ffns): ModuleList(
        (0): FFN(
          (activate): ReLU(inplace=True)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): Linear(in_features=512, out_features=256, bias=True)
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (norms): ModuleList(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (5): BaseTransformerLayer(
      (attentions): ModuleList(
        (0): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (ffns): ModuleList(
        (0): FFN(
          (activate): ReLU(inplace=True)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): Linear(in_features=512, out_features=256, bias=True)
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (norms): ModuleList(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
args = ()
kwargs = {'query': tensor([[[-0.6875, -0.5693, -1.9356,  ...,  1.0307,  0.3035, -1.0652]],

        [[-0.6930, -0.4575, -1.9585,  ...,  1.0475,  0.3035, -1.0094]],

        [[-0.5533, -0.5116, -2.0531,  ...,  0.9059,  0.4282, -0.9072]],

        ...,

        [[-1.1203, -0.1974, -0.8452,  ..., -0.1053,  0.4028, -0.1727]],

        [[-1.1513, -0.1712, -0.8412,  ..., -0.0746,  0.4519, -0.1574]],

        [[-1.1760, -0.1482, -0.8719,  ..., -0.0818,  0.4776, -0.1223]]]), 'key': None, 'value': None, 'query_pos': tensor([[[ 0.5089, -0.2952, -2.2817,  ...,  1.2452, -1.1452,  0.6498]],

        [[ 0.5089, -0.2952, -2.2817,  ...,  1.2452, -1.1452,  0.6498]],

        [[ 0.5089, -0.2952, -2.2817,  ...,  1.2452, -1.1452,  0.6498]],

        ...,

        [[ 0.4775, -0.2952, -3.0504,  ...,  1.2452, -1.1445,  0.6498]],

        [[ 0.4775, -0.2952, -3.0504,  ...,  1.2452, -1.1445,  0.6498]],

        [[ 0.4775, -0.2952, -3.0504,  ...,  1.2452, -1.1445,  0.6498]]]), 'query_key_padding_mask': tensor([[False, False, False,  ..., False, False, False]]), 'spatial_shapes': tensor([[200, 200]]), 'reference_points': tensor([[[[0.0025, 0.0025]],

         [[0.0075, 0.0025]],

         [[0.0125, 0.0025]],

         ...,

         [[0.9875, 0.9975]],

         [[0.9925, 0.9975]],

         [[0.9975, 0.9975]]]]), 'level_start_index': tensor([0]), 'valid_ratios': tensor([[[1., 1.]]])}
(Pdb) self = DetrTransformerEncoder(
  (layers): ModuleList(
    (0): BaseTransformerLayer(
      (attentions): ModuleList(
        (0): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (ffns): ModuleList(
        (0): FFN(
          (activate): ReLU(inplace=True)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): Linear(in_features=512, out_features=256, bias=True)
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (norms): ModuleList(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BaseTransformerLayer(
      (attentions): ModuleList(
        (0): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (ffns): ModuleList(
        (0): FFN(
          (activate): ReLU(inplace=True)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): Linear(in_features=512, out_features=256, bias=True)
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (norms): ModuleList(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BaseTransformerLayer(
      (attentions): ModuleList(
        (0): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (ffns): ModuleList(
        (0): FFN(
          (activate): ReLU(inplace=True)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): Linear(in_features=512, out_features=256, bias=True)
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (norms): ModuleList(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BaseTransformerLayer(
      (attentions): ModuleList(
        (0): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (ffns): ModuleList(
        (0): FFN(
          (activate): ReLU(inplace=True)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): Linear(in_features=512, out_features=256, bias=True)
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (norms): ModuleList(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (4): BaseTransformerLayer(
      (attentions): ModuleList(
        (0): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (ffns): ModuleList(
        (0): FFN(
          (activate): ReLU(inplace=True)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): Linear(in_features=512, out_features=256, bias=True)
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (norms): ModuleList(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (5): BaseTransformerLayer(
      (attentions): ModuleList(
        (0): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (ffns): ModuleList(
        (0): FFN(
          (activate): ReLU(inplace=True)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): Linear(in_features=512, out_features=256, bias=True)
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (norms): ModuleList(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
args = ()
kwargs = {'query': tensor([[[-0.6875, -0.5693, -1.9356,  ...,  1.0307,  0.3035, -1.0652]],

        [[-0.6930, -0.4575, -1.9585,  ...,  1.0475,  0.3035, -1.0094]],

        [[-0.5533, -0.5116, -2.0531,  ...,  0.9059,  0.4282, -0.9072]],

        ...,

        [[-1.1203, -0.1974, -0.8452,  ..., -0.1053,  0.4028, -0.1727]],

        [[-1.1513, -0.1712, -0.8412,  ..., -0.0746,  0.4519, -0.1574]],

        [[-1.1760, -0.1482, -0.8719,  ..., -0.0818,  0.4776, -0.1223]]]), 'key': None, 'value': None, 'query_pos': tensor([[[ 0.5089, -0.2952, -2.2817,  ...,  1.2452, -1.1452,  0.6498]],

        [[ 0.5089, -0.2952, -2.2817,  ...,  1.2452, -1.1452,  0.6498]],

        [[ 0.5089, -0.2952, -2.2817,  ...,  1.2452, -1.1452,  0.6498]],

        ...,

        [[ 0.4775, -0.2952, -3.0504,  ...,  1.2452, -1.1445,  0.6498]],

        [[ 0.4775, -0.2952, -3.0504,  ...,  1.2452, -1.1445,  0.6498]],

        [[ 0.4775, -0.2952, -3.0504,  ...,  1.2452, -1.1445,  0.6498]]]), 'query_key_padding_mask': tensor([[False, False, False,  ..., False, False, False]]), 'spatial_shapes': tensor([[200, 200]]), 'reference_points': tensor([[[[0.0025, 0.0025]],

         [[0.0075, 0.0025]],

         [[0.0125, 0.0025]],

         ...,

         [[0.9875, 0.9975]],

         [[0.9925, 0.9975]],

         [[0.9975, 0.9975]]]]), 'level_start_index': tensor([0]), 'valid_ratios': tensor([[[1., 1.]]])}
(Pdb) self = DetrTransformerEncoder(
  (layers): ModuleList(
    (0): BaseTransformerLayer(
      (attentions): ModuleList(
        (0): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (ffns): ModuleList(
        (0): FFN(
          (activate): ReLU(inplace=True)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): Linear(in_features=512, out_features=256, bias=True)
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (norms): ModuleList(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BaseTransformerLayer(
      (attentions): ModuleList(
        (0): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (ffns): ModuleList(
        (0): FFN(
          (activate): ReLU(inplace=True)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): Linear(in_features=512, out_features=256, bias=True)
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (norms): ModuleList(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BaseTransformerLayer(
      (attentions): ModuleList(
        (0): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (ffns): ModuleList(
        (0): FFN(
          (activate): ReLU(inplace=True)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): Linear(in_features=512, out_features=256, bias=True)
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (norms): ModuleList(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BaseTransformerLayer(
      (attentions): ModuleList(
        (0): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (ffns): ModuleList(
        (0): FFN(
          (activate): ReLU(inplace=True)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): Linear(in_features=512, out_features=256, bias=True)
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (norms): ModuleList(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (4): BaseTransformerLayer(
      (attentions): ModuleList(
        (0): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (ffns): ModuleList(
        (0): FFN(
          (activate): ReLU(inplace=True)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): Linear(in_features=512, out_features=256, bias=True)
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (norms): ModuleList(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (5): BaseTransformerLayer(
      (attentions): ModuleList(
        (0): MultiScaleDeformableAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
          (attention_weights): Linear(in_features=256, out_features=128, bias=True)
          (value_proj): Linear(in_features=256, out_features=256, bias=True)
          (output_proj): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (ffns): ModuleList(
        (0): FFN(
          (activate): ReLU(inplace=True)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): Linear(in_features=512, out_features=256, bias=True)
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (norms): ModuleList(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
args = ()
kwargs = {'query': tensor([[[-0.6875, -0.5693, -1.9356,  ...,  1.0307,  0.3035, -1.0652]],

        [[-0.6930, -0.4575, -1.9585,  ...,  1.0475,  0.3035, -1.0094]],

        [[-0.5533, -0.5116, -2.0531,  ...,  0.9059,  0.4282, -0.9072]],

        ...,

        [[-1.1203, -0.1974, -0.8452,  ..., -0.1053,  0.4028, -0.1727]],

        [[-1.1513, -0.1712, -0.8412,  ..., -0.0746,  0.4519, -0.1574]],

        [[-1.1760, -0.1482, -0.8719,  ..., -0.0818,  0.4776, -0.1223]]]), 'key': None, 'value': None, 'query_pos': tensor([[[ 0.5089, -0.2952, -2.2817,  ...,  1.2452, -1.1452,  0.6498]],

        [[ 0.5089, -0.2952, -2.2817,  ...,  1.2452, -1.1452,  0.6498]],

        [[ 0.5089, -0.2952, -2.2817,  ...,  1.2452, -1.1452,  0.6498]],

        ...,

        [[ 0.4775, -0.2952, -3.0504,  ...,  1.2452, -1.1445,  0.6498]],

        [[ 0.4775, -0.2952, -3.0504,  ...,  1.2452, -1.1445,  0.6498]],

        [[ 0.4775, -0.2952, -3.0504,  ...,  1.2452, -1.1445,  0.6498]]]), 'query_key_padding_mask': tensor([[False, False, False,  ..., False, False, False]]), 'spatial_shapes': tensor([[200, 200]]), 'reference_points': tensor([[[[0.0025, 0.0025]],

         [[0.0075, 0.0025]],

         [[0.0125, 0.0025]],

         ...,

         [[0.9875, 0.9975]],

         [[0.9925, 0.9975]],

         [[0.9975, 0.9975]]]]), 'level_start_index': tensor([0]), 'valid_ratios': tensor([[[1., 1.]]])}
(Pdb) 113  	        """Forward function for `TransformerCoder`.
114  	
115  	        Returns:
116  	            Tensor: forwarded results with shape [num_query, bs, embed_dims].
117  	        """
118  ->	        x = super(DetrTransformerEncoder, self).forward(*args, **kwargs)
119  	        if self.post_norm is not None:
120  	            x = self.post_norm(x)
121  	        return x
122  	
123  	
(Pdb) RuntimeError: The size of tensor a (4) must match the size of tensor b (16) at non-singleton dimension 3
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmdet/models/utils/transformer.py(118)forward()
-> x = super(DetrTransformerEncoder, self).forward(*args, **kwargs)
(Pdb) --Return--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmdet/models/utils/transformer.py(118)forward()->None
-> x = super(DetrTransformerEncoder, self).forward(*args, **kwargs)
(Pdb) RuntimeError: The size of tensor a (4) must match the size of tensor b (16) at non-singleton dimension 3
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) --Return--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()->None
-> return forward_call(*input, **kwargs)
(Pdb) RuntimeError: The size of tensor a (4) must match the size of tensor b (16) at non-singleton dimension 3
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/seg_head_plugin/seg_deformable_transformer.py(319)forward()
-> memory = self.encoder(query=feat_flatten,
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/seg_head_plugin/seg_deformable_transformer.py(319)forward()->None
-> memory = self.encoder(query=feat_flatten,
(Pdb) RuntimeError: The size of tensor a (4) must match the size of tensor b (16) at non-singleton dimension 3
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(186)new_func()
-> return old_func(*args, **kwargs)
(Pdb) *** NameError: name 'memory' is not defined
(Pdb) --Return--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(186)new_func()->None
-> return old_func(*args, **kwargs)
(Pdb) RuntimeError: The size of tensor a (4) must match the size of tensor b (16) at non-singleton dimension 3
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) --Return--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()->None
-> return forward_call(*input, **kwargs)
(Pdb) RuntimeError: The size of tensor a (4) must match the size of tensor b (16) at non-singleton dimension 3
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/panseg_head.py(226)forward()
-> enc_outputs_class, enc_outputs_coord = self.transformer(
(Pdb)   /Users/liangming.xu/code/UniAD/tools/test.py(274)<module>()
-> main()
  /Users/liangming.xu/code/UniAD/tools/test.py(230)main()
-> result = model(return_loss=False, rescale=True, **data)
  /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
  /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py(83)forward()
-> return self.forward_test(**kwargs)
  /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py(302)forward_test()
-> result_seg =  self.seg_head.forward_test(bev_embed, gt_lane_labels, gt_lane_masks, img_metas, rescale)
  /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/panseg_head.py(1014)forward_test()
-> pred_seg_dict = self(pts_feats)
  /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
  /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(186)new_func()
-> return old_func(*args, **kwargs)
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/panseg_head.py(226)forward()
-> enc_outputs_class, enc_outputs_coord = self.transformer(
  /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()->None
-> return forward_call(*input, **kwargs)
  /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(186)new_func()->None
-> return old_func(*args, **kwargs)
  /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/seg_head_plugin/seg_deformable_transformer.py(319)forward()->None
-> memory = self.encoder(query=feat_flatten,
  /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()->None
-> return forward_call(*input, **kwargs)
  /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmdet/models/utils/transformer.py(118)forward()->None
-> x = super(DetrTransformerEncoder, self).forward(*args, **kwargs)
  /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/cnn/bricks/transformer.py(585)forward()
-> query = layer(
  /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
  /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/cnn/bricks/transformer.py(474)forward()
-> query = self.attentions[attn_index](
  /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
  /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/utils/misc.py(340)new_func()
-> output = old_func(*args, **kwargs)
  /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(350)forward()
-> output = multi_scale_deformable_attn_pytorch(
  /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) *** AttributeError: 'list' object has no attribute 'shape'
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/panseg_head.py(226)forward()->None
-> enc_outputs_class, enc_outputs_coord = self.transformer(
(Pdb) RuntimeError: The size of tensor a (4) must match the size of tensor b (16) at non-singleton dimension 3
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(186)new_func()
-> return old_func(*args, **kwargs)
(Pdb) --Return--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(186)new_func()->None
-> return old_func(*args, **kwargs)
(Pdb) 