NOTE: Redirects are currently not supported in Windows or MacOs.
/opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : ./tools/test.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 1
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:28596
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /var/folders/bl/c09z1sm91mxfb8z7mn7v4rhh0000gq/T/torchelastic_tmcsux66/none_z9_jpi43
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=28596
  group_rank=0
  group_world_size=1
  local_ranks=[0]
  role_ranks=[0]
  global_ranks=[0]
  role_world_sizes=[1]
  global_world_sizes=[1]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /var/folders/bl/c09z1sm91mxfb8z7mn7v4rhh0000gq/T/torchelastic_tmcsux66/none_z9_jpi43/attempt_0/0/error.json
projects.mmdet3d_plugin
======
Loading NuScenes tables for version v1.0-trainval...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 22.656 seconds.
======
Reverse indexing ...
Done reverse indexing in 6.4 seconds.
======
load checkpoint from local path: ./ckpts/uniad_base_e2e.pth
2023-07-27 19:19:53,666 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2023-07-27 19:19:53,669 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2023-07-27 19:19:53,671 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2023-07-27 19:19:53,674 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2023-07-27 19:19:53,676 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2023-07-27 19:19:53,678 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2023-07-27 19:19:53,681 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2023-07-27 19:19:53,683 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2023-07-27 19:19:53,685 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2023-07-27 19:19:53,688 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2023-07-27 19:19:53,691 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2023-07-27 19:19:53,693 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2023-07-27 19:19:53,696 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2023-07-27 19:19:53,698 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
2023-07-27 19:19:53,700 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2023-07-27 19:19:53,703 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2023-07-27 19:19:53,705 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2023-07-27 19:19:53,707 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2023-07-27 19:19:53,710 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2023-07-27 19:19:53,712 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2023-07-27 19:19:53,715 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2023-07-27 19:19:53,717 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2023-07-27 19:19:53,719 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2023-07-27 19:19:53,722 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2023-07-27 19:19:53,726 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2023-07-27 19:19:53,730 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
The model and loaded state dict do not match exactly

size mismatch for seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 256]).
size mismatch for seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).
size mismatch for seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 256]).
size mismatch for seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).
size mismatch for seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 256]).
size mismatch for seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).
size mismatch for seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 256]).
size mismatch for seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).
size mismatch for seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 256]).
size mismatch for seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).
size mismatch for seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 256]).
size mismatch for seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).
size mismatch for seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 256]).
size mismatch for seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).
size mismatch for seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 256]).
size mismatch for seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).
size mismatch for seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 256]).
size mismatch for seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).
size mismatch for seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 256]).
size mismatch for seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).
size mismatch for seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 256]).
size mismatch for seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).
size mismatch for seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 256]).
size mismatch for seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).
size mismatch for seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 256]).
size mismatch for seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).
size mismatch for seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 256]).
size mismatch for seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).
size mismatch for seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 256]).
size mismatch for seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).
size mismatch for seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 256]).
size mismatch for seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).
size mismatch for seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 256]).
size mismatch for seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).
size mismatch for seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 256]).
size mismatch for seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).
size mismatch for seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 256]).
size mismatch for seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).
size mismatch for seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 256]).
size mismatch for seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).
size mismatch for seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 256]).
size mismatch for seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).
size mismatch for seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 256]).
size mismatch for seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).
size mismatch for seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 256]).
size mismatch for seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).
size mismatch for seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 256]).
size mismatch for seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).
unexpected key in source state_dict: bbox_size_fc.weight, bbox_size_fc.bias, pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/datasets/pipelines/occflow_label.py(164)__call__()
-> segmentations = torch.from_numpy(
(Pdb) 159  	
160  	            segmentations.append(segmentation)
161  	            instances.append(instance)
162  	        import pdb; pdb.set_trace()
163  	        # segmentation = 1 where objects are located
164  ->	        segmentations = torch.from_numpy(
165  	            np.stack(segmentations, axis=0)).long()
166  	        instances = torch.from_numpy(np.stack(instances, axis=0)).long()
167  	
168  	        # generate heatmap & offset from segmentation & instance
169  	        instance_centerness, instance_offset, instance_flow, instance_backward_flow = self.center_offset_flow(
(Pdb) 7
(Pdb) 7
(Pdb) 7
(Pdb) [LiDARInstance3DBoxes(
    tensor([[-6.4132e-01, -2.4022e+01, -2.2782e+00,  1.8380e+00,  4.3970e+00,
          1.5590e+00, -3.1750e+00, -1.5941e-01,  6.4455e+00],
        [-1.3249e+01, -7.0688e-01, -1.3545e+00,  3.0800e-01,  1.5790e+00,
          1.1350e+00, -1.3977e-01, -4.4199e-03,  2.2335e-02],
        [-3.7800e+00, -9.2550e+00, -1.7569e+00,  5.4300e-01,  2.1850e+00,
          1.1750e+00, -3.1804e+00,  0.0000e+00,  0.0000e+00],
        [-2.3555e+01,  3.8098e+01, -3.5558e-01,  5.8500e-01,  4.1100e-01,
          1.6470e+00,  1.4675e+00, -3.1336e-01, -2.5979e-01],
        [-1.2371e+01,  3.5773e+01, -9.0977e-01,  7.0300e-01,  9.3800e-01,
          1.6590e+00, -3.4477e+00,  1.1399e-01,  5.1186e-01],
        [-1.7343e+01,  4.4967e+01, -5.8426e-01,  3.2180e+00,  1.3483e+01,
          3.6170e+00, -3.5564e+00, -7.0843e-02,  1.5920e-01],
        [-2.4299e+01,  3.2866e+01, -4.3007e-01,  1.3690e+00,  1.4590e+00,
          1.6430e+00, -3.5192e+00,  3.1057e-01,  1.5911e-01],
        [-2.3438e+01,  3.7369e+01, -4.9221e-01,  5.6400e-01,  5.2800e-01,
          1.7080e+00, -3.2798e+00, -3.6307e-01, -4.7841e-01],
        [-2.2838e+01,  3.9179e+01, -3.0369e-01,  4.4400e-01,  6.0900e-01,
          1.6910e+00,  1.8883e-02, -1.3342e-01,  1.9484e-03]])), LiDARInstance3DBoxes(
    tensor([[-7.0898e-01, -2.0792e+01, -2.1769e+00,  1.8380e+00,  4.3970e+00,
          1.5590e+00, -3.1691e+00, -1.5831e-01,  6.4441e+00],
        [-1.3244e+01, -6.9819e-01, -1.2540e+00,  3.0800e-01,  1.5790e+00,
          1.1350e+00, -1.2227e-01, -4.4518e-03,  3.2934e-02],
        [-3.7714e+00, -9.2530e+00, -1.7568e+00,  5.4300e-01,  2.1850e+00,
          1.1750e+00, -3.1803e+00,  0.0000e+00,  0.0000e+00],
        [-2.3700e+01,  3.7967e+01, -2.8685e-01,  5.8500e-01,  4.1100e-01,
          1.6470e+00,  1.4676e+00, -3.1388e-01, -2.6064e-01],
        [-1.2302e+01,  3.6028e+01, -8.3141e-01,  7.0300e-01,  9.3800e-01,
          1.6590e+00, -2.7495e+00, -2.1577e-02,  4.7599e-01],
        [-1.7351e+01,  4.5055e+01, -6.3071e-01,  3.2180e+00,  1.3483e+01,
          3.6170e+00, -3.5563e+00, -3.5377e-02,  7.9556e-02],
        [-2.4132e+01,  3.2946e+01, -3.7592e-01,  1.3690e+00,  1.4590e+00,
          1.6430e+00, -3.5191e+00,  3.1042e-01,  1.5923e-01],
        [-1.2046e+01,  3.6135e+01, -8.3058e-01,  2.5300e-01,  2.2800e-01,
          7.5200e-01, -1.3478e+00, -2.1444e-01,  2.8885e-01],
        [-2.3561e+01,  3.6079e+01, -3.9800e-01,  4.5300e-01,  3.8500e-01,
          1.6950e+00, -3.6463e+00, -8.0193e-02,  3.2501e-02],
        [-2.3607e+01,  3.7126e+01, -3.5339e-01,  5.6400e-01,  5.2800e-01,
          1.7080e+00, -3.2797e+00, -3.9198e-01, -2.6830e-01],
        [-2.2893e+01,  3.9177e+01, -1.5398e-01,  4.4400e-01,  6.0900e-01,
          1.6910e+00,  1.8929e-02, -6.6233e-02,  3.4924e-02]])), LiDARInstance3DBoxes(
    tensor([[-7.9602e-01, -1.7568e+01, -2.0746e+00,  1.8380e+00,  4.3970e+00,
          1.5590e+00, -3.1634e+00, -1.6375e-01,  6.6700e+00],
        [-1.3251e+01, -6.8117e-01, -1.2032e+00,  3.0800e-01,  1.5790e+00,
          1.1350e+00, -1.2230e-01,  1.1062e-02, -8.6215e-02],
        [-3.7767e+00, -9.2558e+00, -1.7569e+00,  5.4300e-01,  2.1850e+00,
          1.1750e+00, -3.1803e+00,  0.0000e+00,  0.0000e+00],
        [-2.3865e+01,  3.7830e+01, -2.1834e-01,  5.8500e-01,  4.1100e-01,
          1.6470e+00, -4.8157e+00, -2.6991e-01, -1.4137e-01],
        [-1.2389e+01,  3.6242e+01, -7.5535e-01,  7.0300e-01,  9.3800e-01,
          1.6590e+00, -2.4005e+00, -7.6745e-02,  4.5361e-01],
        [-1.7369e+01,  4.5036e+01, -3.8112e-01,  3.2180e+00,  1.3483e+01,
          3.6170e+00, -3.5563e+00, -9.5735e-03, -1.9517e-01],
        [-2.3985e+01,  3.3019e+01, -3.2193e-01,  1.3690e+00,  1.4590e+00,
          1.6430e+00, -3.5192e+00,  3.1047e-01,  1.5908e-01],
        [-1.2157e+01,  3.6272e+01, -6.7496e-01,  2.5300e-01,  2.2800e-01,
          7.5200e-01, -1.3479e+00, -3.3923e-02,  4.6936e-01],
        [-2.3609e+01,  3.6087e+01, -3.0963e-01,  4.5300e-01,  3.8500e-01,
          1.6950e+00, -3.7336e+00, -8.1365e-02,  2.8216e-02],
        [-2.3826e+01,  3.7088e+01, -2.5573e-01,  5.6400e-01,  5.2800e-01,
          1.7080e+00, -3.2798e+00, -2.1054e-01, -2.8979e-02],
        [-2.2901e+01,  3.9206e+01, -1.2761e-01,  4.4400e-01,  6.0900e-01,
          1.6910e+00,  1.8896e-02,  1.6149e-03,  6.7304e-02]])), LiDARInstance3DBoxes(
    tensor([[-2.3244e+01,  4.0133e+01, -1.6752e-01,  5.3900e-01,  7.6600e-01,
          1.9040e+00, -4.6892e+00, -7.8969e-01,  1.7873e-02],
        [-8.7734e-01, -1.4114e+01, -1.9006e+00,  1.8380e+00,  4.3970e+00,
          1.5590e+00, -3.1575e+00, -2.6674e-02,  7.2814e+00],
        [-1.3237e+01, -7.9381e-01, -1.0579e+00,  3.0800e-01,  1.5790e+00,
          1.1350e+00, -1.2231e-01,  1.3875e-02, -2.0641e-01],
        [-2.3975e+01,  3.7820e+01, -1.6931e-01,  5.8500e-01,  4.1100e-01,
          1.6470e+00, -4.8157e+00, -2.2526e-01, -2.2978e-02],
        [-1.2384e+01,  3.6476e+01, -6.7927e-01,  7.0300e-01,  9.3800e-01,
          1.6590e+00, -2.0514e+00, -1.7621e-02,  4.7015e-01],
        [-1.7371e+01,  4.4847e+01, -3.8956e-01,  3.2180e+00,  1.3483e+01,
          3.6170e+00, -3.5564e+00, -4.4344e-02, -1.1496e-01],
        [-2.3827e+01,  3.3100e+01, -2.6784e-01,  1.3690e+00,  1.4590e+00,
          1.6430e+00, -3.5192e+00,  3.1064e-01,  1.5924e-01],
        [-1.2083e+01,  3.6596e+01, -6.1060e-01,  2.5300e-01,  2.2800e-01,
          7.5200e-01, -1.3479e+00,  4.9322e-02,  3.8569e-01],
        [-2.3647e+01,  3.6099e+01, -2.2235e-01,  4.5300e-01,  3.8500e-01,
          1.6950e+00, -3.8208e+00, -8.3073e-02,  2.1741e-02],
        [-2.3823e+01,  3.7091e+01, -2.5570e-01,  5.6400e-01,  5.2800e-01,
          1.7080e+00, -3.5416e+00,  0.0000e+00,  0.0000e+00],
        [-2.2896e+01,  3.9241e+01, -1.0116e-01,  4.4400e-01,  6.0900e-01,
          1.6910e+00,  1.8885e-02, -3.9274e-02,  3.4034e-02]])), LiDARInstance3DBoxes(
    tensor([[-2.3631e+01,  4.0145e+01, -1.1873e-01,  5.3900e-01,  7.6600e-01,
          1.9040e+00, -4.6892e+00, -1.5835e-01,  3.6419e-03],
        [-8.1417e-01, -1.0280e+01, -1.6844e+00,  1.8380e+00,  4.3970e+00,
          1.5590e+00, -3.1663e+00,  1.1408e-01,  7.7228e+00],
        [-1.3231e+01, -8.9404e-01, -9.6221e-01,  3.0800e-01,  1.5790e+00,
          1.1350e+00, -1.2232e-01,  1.0879e-01, -8.5031e-02],
        [-2.4080e+01,  3.7809e+01, -1.2025e-01,  5.8500e-01,  4.1100e-01,
          1.6470e+00, -4.8157e+00, -1.1431e-01,  2.5156e-03],
        [-1.2396e+01,  3.6713e+01, -6.0205e-01,  7.0300e-01,  9.3800e-01,
          1.6590e+00, -2.0514e+00, -9.5195e-02,  2.7431e-01],
        [-1.7390e+01,  4.4933e+01, -3.3602e-01,  3.2180e+00,  1.3483e+01,
          3.6170e+00, -3.5564e+00, -3.4732e-02,  8.0370e-02],
        [-2.3664e+01,  3.3181e+01, -2.1366e-01,  1.3690e+00,  1.4590e+00,
          1.6430e+00, -3.5192e+00,  3.1061e-01,  1.5936e-01],
        [-1.2104e+01,  3.6656e+01, -5.5808e-01,  2.5300e-01,  2.2800e-01,
          7.5200e-01, -1.3479e+00, -4.7588e-02,  1.2194e-01],
        [-2.3682e+01,  3.6109e+01, -1.5904e-01,  4.5300e-01,  3.8500e-01,
          1.6950e+00, -3.9081e+00, -8.4726e-02,  1.5233e-02],
        [-2.3816e+01,  3.7091e+01, -1.5567e-01,  5.6400e-01,  5.2800e-01,
          1.7080e+00, -3.8034e+00, -5.8061e-02, -7.1373e-02],
        [-2.2929e+01,  3.9245e+01, -1.0118e-01,  4.4400e-01,  6.0900e-01,
          1.6910e+00,  1.8880e-02, -1.2401e-01, -1.7162e-01],
        [-4.4297e+01,  4.9275e+01,  1.6982e+00,  1.9260e+00,  4.8300e+00,
          1.6560e+00, -4.3634e-01,  0.0000e+00,  0.0000e+00]])), LiDARInstance3DBoxes(
    tensor([[-2.3392e+01,  4.0135e+01, -6.7952e-02,  5.3900e-01,  7.6600e-01,
          1.9040e+00, -4.6892e+00,  7.2261e-02, -1.3027e-03],
        [-7.5643e-01, -6.3914e+00, -1.4657e+00,  1.8380e+00,  4.3970e+00,
          1.5590e+00, -3.1750e+00,  7.9031e-03,  7.8977e+00],
        [-1.3122e+01, -8.8651e-01, -8.6124e-01,  3.0800e-01,  1.5790e+00,
          1.1350e+00, -1.2231e-01, -1.1384e-02,  9.7640e-02],
        [-2.4079e+01,  3.7820e+01, -8.2611e-02,  5.8500e-01,  4.1100e-01,
          1.6470e+00, -4.6411e+00, -5.6951e-02,  9.5047e-02],
        [-1.2470e+01,  3.6748e+01, -5.8372e-01,  7.0300e-01,  9.3800e-01,
          1.6590e+00, -2.0514e+00, -3.0982e-01, -2.0072e-02],
        [-1.7385e+01,  4.4928e+01, -3.3593e-01,  3.2180e+00,  1.3483e+01,
          3.6170e+00, -3.5564e+00,  0.0000e+00,  0.0000e+00],
        [-2.3507e+01,  3.3256e+01, -1.5956e-01,  1.3690e+00,  1.4590e+00,
          1.6430e+00, -3.5192e+00,  2.8235e-01,  1.4488e-01],
        [-1.2126e+01,  3.6715e+01, -5.0555e-01,  2.5300e-01,  2.2800e-01,
          7.5200e-01, -1.3479e+00,  2.5339e-02,  1.3435e-01],
        [-3.6108e+01,  5.4119e+01,  1.5392e+00,  1.8060e+00,  4.0530e+00,
          1.6490e+00, -3.5704e+00,  0.0000e+00,  0.0000e+00],
        [-2.3722e+01,  3.6112e+01, -1.4696e-01,  4.5300e-01,  3.8500e-01,
          1.6950e+00, -3.9954e+00,  8.4331e-03,  1.2736e-01],
        [-2.3871e+01,  3.7015e+01, -1.0902e-01,  5.6400e-01,  5.2800e-01,
          1.7080e+00, -3.8092e+00,  5.1209e-03,  7.3717e-02],
        [-2.3010e+01,  3.9069e+01, -5.9002e-02,  4.4400e-01,  6.0900e-01,
          1.6910e+00,  1.8886e-02,  2.9957e-02, -3.1574e-01],
        [-4.4294e+01,  4.9273e+01,  1.6983e+00,  1.9260e+00,  4.8300e+00,
          1.6560e+00, -4.3633e-01,  0.0000e+00,  0.0000e+00]])), LiDARInstance3DBoxes(
    tensor([[-2.3551e+01,  4.0143e+01, -6.8490e-02,  5.3900e-01,  7.6600e-01,
          1.9040e+00, -4.6892e+00, -1.4142e-01,  6.0097e-02],
        [-8.0918e-01, -1.5884e+00, -1.2582e+00,  1.8380e+00,  4.3970e+00,
          1.5590e+00, -3.2012e+00, -1.7227e-01,  8.2781e+00],
        [-3.3950e+01,  4.3110e+01,  8.2361e-01,  1.7850e+00,  4.2850e+00,
          1.7570e+00, -1.7979e+00,  1.0879e+00, -8.8755e-01],
        [-1.3245e+01, -7.9200e-01, -8.0775e-01,  3.0800e-01,  1.5790e+00,
          1.1350e+00, -1.2234e-01, -2.2013e-01,  1.7159e-01],
        [-2.4143e+01,  3.7912e+01, -4.1045e-02,  5.8500e-01,  4.1100e-01,
          1.6470e+00, -4.6993e+00, -3.1520e-02,  1.7708e-02],
        [-1.2738e+01,  3.6687e+01, -4.7157e-01,  7.0300e-01,  9.3800e-01,
          1.6590e+00, -2.0515e+00, -3.2628e-01, -4.0480e-02],
        [-1.7390e+01,  4.4934e+01, -2.8604e-01,  3.2180e+00,  1.3483e+01,
          3.6170e+00, -3.5564e+00, -3.4116e-02,  7.2844e-02],
        [-2.3354e+01,  3.3337e+01, -1.0552e-01,  1.3690e+00,  1.4590e+00,
          1.6430e+00, -3.5192e+00,  5.0433e-02,  1.6181e-01],
        [-1.2077e+01,  3.6799e+01, -4.2667e-01,  2.5300e-01,  2.2800e-01,
          7.5200e-01, -1.3479e+00,  9.3958e-02,  1.5653e-01],
        [-3.6111e+01,  5.4121e+01,  1.5891e+00,  1.8060e+00,  4.0530e+00,
          1.6490e+00, -3.5704e+00, -3.4982e-02,  7.5999e-02],
        [-2.3673e+01,  3.6248e+01, -9.0982e-02,  4.5300e-01,  3.8500e-01,
          1.6950e+00, -4.1699e+00,  7.2499e-02,  2.5086e-01],
        [-2.3810e+01,  3.7169e+01, -5.2220e-02,  5.6400e-01,  5.2800e-01,
          1.7080e+00, -4.5132e+00,  4.3759e-02,  2.5654e-01],
        [-2.2897e+01,  3.8893e+01,  3.3793e-02,  4.4400e-01,  6.0900e-01,
          1.6910e+00,  1.8859e-02,  2.1819e-01, -1.6064e-02],
        [-4.4297e+01,  4.9273e+01,  1.7821e+00,  1.9260e+00,  4.8300e+00,
          1.6560e+00, -4.3636e-01,  0.0000e+00,  0.0000e+00]]))]
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmdet/datasets/pipelines/compose.py(40)__call__()
-> data = t(data)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/datasets/nuscenes_e2e_dataset.py(254)prepare_test_data()
-> example = self.pipeline(input_dict)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/datasets/nuscenes_e2e_dataset.py(726)__getitem__()
-> return self.prepare_test_data(idx)
(Pdb) 0
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py(44)<listcomp>()
-> data = [self.dataset[idx] for idx in possibly_batched_index]
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py(44)fetch()
-> data = [self.dataset[idx] for idx in possibly_batched_index]
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/utils/data/dataloader.py(561)_next_data()
-> data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py(44)fetch()
-> data = [self.dataset[idx] for idx in possibly_batched_index]
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py(44)<listcomp>()
-> data = [self.dataset[idx] for idx in possibly_batched_index]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/datasets/nuscenes_e2e_dataset.py(726)__getitem__()
-> return self.prepare_test_data(idx)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/datasets/nuscenes_e2e_dataset.py(254)prepare_test_data()
-> example = self.pipeline(input_dict)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmdet/datasets/pipelines/compose.py(40)__call__()
-> data = t(data)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/datasets/pipelines/occflow_label.py(164)__call__()
-> segmentations = torch.from_numpy(
(Pdb) 