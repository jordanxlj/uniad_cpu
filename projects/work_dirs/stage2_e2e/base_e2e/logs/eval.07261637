NOTE: Redirects are currently not supported in Windows or MacOs.
/opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : ./tools/test.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 1
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:28596
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /var/folders/bl/c09z1sm91mxfb8z7mn7v4rhh0000gq/T/torchelastic_q85my0ae/none__uqs_0gt
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=28596
  group_rank=0
  group_world_size=1
  local_ranks=[0]
  role_ranks=[0]
  global_ranks=[0]
  role_world_sizes=[1]
  global_world_sizes=[1]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /var/folders/bl/c09z1sm91mxfb8z7mn7v4rhh0000gq/T/torchelastic_q85my0ae/none__uqs_0gt/attempt_0/0/error.json
projects.mmdet3d_plugin
======
Loading NuScenes tables for version v1.0-trainval...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 21.154 seconds.
======
Reverse indexing ...
Done reverse indexing in 6.4 seconds.
======
load checkpoint from local path: ./ckpts/uniad_base_e2e.pth
2023-07-26 16:38:14,514 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2023-07-26 16:38:14,517 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2023-07-26 16:38:14,520 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2023-07-26 16:38:14,522 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2023-07-26 16:38:14,525 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2023-07-26 16:38:14,527 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2023-07-26 16:38:14,530 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2023-07-26 16:38:14,532 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2023-07-26 16:38:14,535 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2023-07-26 16:38:14,537 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2023-07-26 16:38:14,540 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2023-07-26 16:38:14,543 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2023-07-26 16:38:14,545 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2023-07-26 16:38:14,548 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
2023-07-26 16:38:14,551 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2023-07-26 16:38:14,553 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2023-07-26 16:38:14,556 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2023-07-26 16:38:14,558 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2023-07-26 16:38:14,561 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2023-07-26 16:38:14,563 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2023-07-26 16:38:14,566 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2023-07-26 16:38:14,568 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2023-07-26 16:38:14,570 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2023-07-26 16:38:14,573 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2023-07-26 16:38:14,577 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2023-07-26 16:38:14,580 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
The model and loaded state dict do not match exactly

size mismatch for seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 256]).
size mismatch for seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).
size mismatch for seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 256]).
size mismatch for seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).
size mismatch for seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 256]).
size mismatch for seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).
size mismatch for seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 256]).
size mismatch for seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).
size mismatch for seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 256]).
size mismatch for seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).
size mismatch for seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 256]).
size mismatch for seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).
size mismatch for seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 256]).
size mismatch for seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).
size mismatch for seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 256]).
size mismatch for seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).
size mismatch for seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 256]).
size mismatch for seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).
size mismatch for seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 256]).
size mismatch for seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).
size mismatch for seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 256]).
size mismatch for seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).
size mismatch for seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 256]).
size mismatch for seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).
unexpected key in source state_dict: bbox_size_fc.weight, bbox_size_fc.bias, pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

> /Users/liangming.xu/code/UniAD/tools/test.py(230)main()
-> result = model(return_loss=False, rescale=True, **data)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(723)simple_test_track()
-> bs = img.size(0)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(658)_forward_single_frame_inference()
-> bev_embed, bev_pos = self.get_bevs(img, img_metas, prev_bev=prev_bev)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) 144  	    # (bs, num_heads, num_queries, num_levels, num_points) ->
145  	    # (bs, num_heads, 1, num_queries, num_levels*num_points)
146  	    attention_weights = attention_weights.transpose(1, 2).reshape(
147  	        bs * num_heads, 1, num_queries, num_levels * num_points)
148  	    import pdb; pdb.set_trace()
149  ->	    output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
150  	              attention_weights).sum(-1).view(bs, num_heads * embed_dims,
151  	                                              num_queries)
152  	    return output.transpose(1, 2).contiguous()
153  	
154  	
(Pdb) torch.Size([16, 1, 40000, 4])
(Pdb) torch.Size([16, 32, 40000, 4])
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(150)multi_scale_deformable_attn_pytorch()
-> attention_weights).sum(-1).view(bs, num_heads * embed_dims,
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(150)multi_scale_deformable_attn_pytorch()
-> attention_weights).sum(-1).view(bs, num_heads * embed_dims,
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(150)multi_scale_deformable_attn_pytorch()
-> attention_weights).sum(-1).view(bs, num_heads * embed_dims,
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(151)multi_scale_deformable_attn_pytorch()
-> num_queries)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(152)multi_scale_deformable_attn_pytorch()
-> return output.transpose(1, 2).contiguous()
(Pdb) --Return--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(152)multi_scale_deformable_attn_pytorch()->tensor([[[0.,...0., 0., 0.]]])
-> return output.transpose(1, 2).contiguous()
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(254)forward()
-> output = output.permute(1, 2, 0)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(258)forward()
-> output = output.view(num_query, embed_dims, bs, self.num_bev_queue)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) torch.Size([48, 32, 9502, 32])
(Pdb) 144  	    # (bs, num_heads, num_queries, num_levels, num_points) ->
145  	    # (bs, num_heads, 1, num_queries, num_levels*num_points)
146  	    attention_weights = attention_weights.transpose(1, 2).reshape(
147  	        bs * num_heads, 1, num_queries, num_levels * num_points)
148  	    import pdb; pdb.set_trace()
149  ->	    output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
150  	              attention_weights).sum(-1).view(bs, num_heads * embed_dims,
151  	                                              num_queries)
152  	    return output.transpose(1, 2).contiguous()
153  	
154  	
(Pdb) torch.Size([48, 1, 9502, 32])
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(150)multi_scale_deformable_attn_pytorch()
-> attention_weights).sum(-1).view(bs, num_heads * embed_dims,
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(150)multi_scale_deformable_attn_pytorch()
-> attention_weights).sum(-1).view(bs, num_heads * embed_dims,
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(150)multi_scale_deformable_attn_pytorch()
-> attention_weights).sum(-1).view(bs, num_heads * embed_dims,
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(151)multi_scale_deformable_attn_pytorch()
-> num_queries)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(152)multi_scale_deformable_attn_pytorch()
-> return output.transpose(1, 2).contiguous()
(Pdb) --Return--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(152)multi_scale_deformable_attn_pytorch()->tensor([[[-6....0.0000e+00]]])
-> return output.transpose(1, 2).contiguous()
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(150)multi_scale_deformable_attn_pytorch()
-> attention_weights).sum(-1).view(bs, num_heads * embed_dims,
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(150)multi_scale_deformable_attn_pytorch()
-> attention_weights).sum(-1).view(bs, num_heads * embed_dims,
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(150)multi_scale_deformable_attn_pytorch()
-> attention_weights).sum(-1).view(bs, num_heads * embed_dims,
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(151)multi_scale_deformable_attn_pytorch()
-> num_queries)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(152)multi_scale_deformable_attn_pytorch()
-> return output.transpose(1, 2).contiguous()
(Pdb) --Return--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(152)multi_scale_deformable_attn_pytorch()->tensor([[[0.,...0., 0., 0.]]])
-> return output.transpose(1, 2).contiguous()
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(150)multi_scale_deformable_attn_pytorch()
-> attention_weights).sum(-1).view(bs, num_heads * embed_dims,
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(150)multi_scale_deformable_attn_pytorch()
-> attention_weights).sum(-1).view(bs, num_heads * embed_dims,
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(150)multi_scale_deformable_attn_pytorch()
-> attention_weights).sum(-1).view(bs, num_heads * embed_dims,
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(151)multi_scale_deformable_attn_pytorch()
-> num_queries)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(152)multi_scale_deformable_attn_pytorch()
-> return output.transpose(1, 2).contiguous()
(Pdb) --Return--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(152)multi_scale_deformable_attn_pytorch()->tensor([[[ 5....0.0000e+00]]])
-> return output.transpose(1, 2).contiguous()
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(395)forward()
-> if not self.batch_first:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(398)forward()
-> return output
(Pdb) torch.Size([6, 9502, 256])
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(398)forward()->tensor([[[ 5....0.0000e+00]]])
-> return output
(Pdb) --Return--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()->tensor([[[ 5....0.0000e+00]]])
-> return forward_call(*input, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(163)forward()
-> level_start_index=level_start_index).view(bs, self.num_cams, max_len, self.embed_dims)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(161)forward()
-> queries = self.deformable_attention(query=queries_rebatch.view(bs*self.num_cams, max_len, self.embed_dims), key=key, value=value,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(164)forward()
-> for j in range(bs):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(165)forward()
-> for i, index_query_per_img in enumerate(indexes):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(166)forward()
-> slots[j, index_query_per_img] += queries[j, i, :len(index_query_per_img)]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(165)forward()
-> for i, index_query_per_img in enumerate(indexes):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(166)forward()
-> slots[j, index_query_per_img] += queries[j, i, :len(index_query_per_img)]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(165)forward()
-> for i, index_query_per_img in enumerate(indexes):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(166)forward()
-> slots[j, index_query_per_img] += queries[j, i, :len(index_query_per_img)]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(165)forward()
-> for i, index_query_per_img in enumerate(indexes):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(166)forward()
-> slots[j, index_query_per_img] += queries[j, i, :len(index_query_per_img)]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(165)forward()
-> for i, index_query_per_img in enumerate(indexes):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(166)forward()
-> slots[j, index_query_per_img] += queries[j, i, :len(index_query_per_img)]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(165)forward()
-> for i, index_query_per_img in enumerate(indexes):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(166)forward()
-> slots[j, index_query_per_img] += queries[j, i, :len(index_query_per_img)]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(165)forward()
-> for i, index_query_per_img in enumerate(indexes):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(164)forward()
-> for j in range(bs):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(168)forward()
-> count = bev_mask.sum(-1) > 0
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(169)forward()
-> count = count.permute(1, 2, 0).sum(-1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(170)forward()
-> count = torch.clamp(count, min=1.0)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(171)forward()
-> slots = slots / count[..., None]
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) torch.Size([16, 1, 40000, 4])
(Pdb) torch.Size([16, 32, 40000, 4])
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) torch.Size([48, 32, 9502, 32])
(Pdb) torch.Size([48, 1, 9502, 32])
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) torch.Size([16, 1, 40000, 4])
(Pdb) torch.Size([16, 32, 40000, 4])
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py(297)forward_test()
-> result_track[0] = self.upsample_bev_if_tiny(result_track[0])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/panseg_head.py(1013)forward_test()
-> bbox_list = [dict() for i in range(len(img_metas))]
(Pdb) 1008 	                    gt_lane_labels=None,
1009 	                    gt_lane_masks=None,
1010 	                    img_metas=None,
1011 	                    rescale=False):
1012 	        import pdb; pdb.set_trace()
1013 ->	        bbox_list = [dict() for i in range(len(img_metas))]
1014 	
1015 	        pred_seg_dict = self(pts_feats)
1016 	        results = self.get_bboxes(pred_seg_dict['outputs_classes'],
1017 	                                           pred_seg_dict['outputs_coords'],
1018 	                                           pred_seg_dict['enc_outputs_class'],
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/panseg_head.py(1015)forward_test()
-> pred_seg_dict = self(pts_feats)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/panseg_head.py(227)forward()
-> enc_outputs_class, enc_outputs_coord = self.transformer(
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(149)multi_scale_deformable_attn_pytorch()
-> output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
(Pdb) torch.Size([8, 32, 40000, 4])
(Pdb) 144  	    # (bs, num_heads, num_queries, num_levels, num_points) ->
145  	    # (bs, num_heads, 1, num_queries, num_levels*num_points)
146  	    attention_weights = attention_weights.transpose(1, 2).reshape(
147  	        bs * num_heads, 1, num_queries, num_levels * num_points)
148  	    import pdb; pdb.set_trace()
149  ->	    output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
150  	              attention_weights).sum(-1).view(bs, num_heads * embed_dims,
151  	                                              num_queries)
152  	    return output.transpose(1, 2).contiguous()
153  	
154  	
(Pdb) torch.Size([8, 1, 40000, 16])
(Pdb) 