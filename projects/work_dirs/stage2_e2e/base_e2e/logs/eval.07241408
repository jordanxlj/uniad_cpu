NOTE: Redirects are currently not supported in Windows or MacOs.
/opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : ./tools/test.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 1
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:28596
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /var/folders/bl/c09z1sm91mxfb8z7mn7v4rhh0000gq/T/torchelastic_hpcptdh1/none_ezth943o
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=28596
  group_rank=0
  group_world_size=1
  local_ranks=[0]
  role_ranks=[0]
  global_ranks=[0]
  role_world_sizes=[1]
  global_world_sizes=[1]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /var/folders/bl/c09z1sm91mxfb8z7mn7v4rhh0000gq/T/torchelastic_hpcptdh1/none_ezth943o/attempt_0/0/error.json
projects.mmdet3d_plugin
======
Loading NuScenes tables for version v1.0-trainval...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 22.982 seconds.
======
Reverse indexing ...
Done reverse indexing in 6.3 seconds.
======
load checkpoint from local path: ./ckpts/uniad_base_e2e.pth
2023-07-24 14:09:12,447 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2023-07-24 14:09:12,451 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2023-07-24 14:09:12,453 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2023-07-24 14:09:12,456 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2023-07-24 14:09:12,458 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2023-07-24 14:09:12,460 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2023-07-24 14:09:12,463 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2023-07-24 14:09:12,465 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2023-07-24 14:09:12,468 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2023-07-24 14:09:12,470 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2023-07-24 14:09:12,473 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2023-07-24 14:09:12,475 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2023-07-24 14:09:12,478 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2023-07-24 14:09:12,480 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
2023-07-24 14:09:12,482 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2023-07-24 14:09:12,485 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2023-07-24 14:09:12,487 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2023-07-24 14:09:12,489 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2023-07-24 14:09:12,492 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2023-07-24 14:09:12,494 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2023-07-24 14:09:12,496 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2023-07-24 14:09:12,499 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2023-07-24 14:09:12,501 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2023-07-24 14:09:12,504 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2023-07-24 14:09:12,508 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2023-07-24 14:09:12,511 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
The model and loaded state dict do not match exactly

unexpected key in source state_dict: bbox_size_fc.weight, bbox_size_fc.bias, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias, pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

> /Users/liangming.xu/code/UniAD/tools/test.py(229)main()
-> result = model(return_loss=False, rescale=True, **data)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(723)simple_test_track()
-> bs = img.size(0)
(Pdb) 718  	        timestamp=None,
719  	    ):
720  	        """only support bs=1 and sequential input"""
721  	
722  	        import pdb; pdb.set_trace()
723  ->	        bs = img.size(0)
724  	        # img_metas = img_metas[0]
725  	
726  	        """ init track instances for first frame """
727  	        if (
728  	            self.test_track_instances is None
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(641)_forward_single_frame_inference()
-> active_inst = track_instances[track_instances.obj_idxes >= 0]
(Pdb) 636  	        img: B, num_cam, C, H, W = img.shape
637  	        """
638  	
639  	        """ velo update """
640  	        import pdb; pdb.set_trace()
641  ->	        active_inst = track_instances[track_instances.obj_idxes >= 0]
642  	        other_inst = track_instances[track_instances.obj_idxes < 0]
643  	
644  	        if l2g_r2 is not None and len(active_inst) > 0 and l2g_r1 is not None:
645  	            ref_pts = active_inst.ref_pts
646  	            velo = active_inst.pred_boxes[:, -2:]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(113)get_bev_features()
-> bs = mlvl_feats[0].size(0)
(Pdb) 108  	            img_metas=None):
109  	        """
110  	        obtain bev features.
111  	        """
112  	        import pdb; pdb.set_trace()
113  ->	        bs = mlvl_feats[0].size(0)
114  	        bev_queries = bev_queries.unsqueeze(1).repeat(1, bs, 1)
115  	        bev_pos = bev_pos.flatten(2).permute(2, 0, 1)
116  	        # obtain rotation angle and shift with ego motion
117  	        delta_x = np.array([each['can_bus'][0]
118  	                           for each in img_metas])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(114)get_bev_features()
-> bev_queries = bev_queries.unsqueeze(1).repeat(1, bs, 1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(115)get_bev_features()
-> bev_pos = bev_pos.flatten(2).permute(2, 0, 1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(117)get_bev_features()
-> delta_x = np.array([each['can_bus'][0]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(118)get_bev_features()
-> for each in img_metas])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(117)get_bev_features()
-> delta_x = np.array([each['can_bus'][0]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(119)get_bev_features()
-> delta_y = np.array([each['can_bus'][1]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(120)get_bev_features()
-> for each in img_metas])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(119)get_bev_features()
-> delta_y = np.array([each['can_bus'][1]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(121)get_bev_features()
-> ego_angle = np.array(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(122)get_bev_features()
-> [each['can_bus'][-2] / np.pi * 180 for each in img_metas])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(121)get_bev_features()
-> ego_angle = np.array(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(123)get_bev_features()
-> grid_length_y = grid_length[0]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(124)get_bev_features()
-> grid_length_x = grid_length[1]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(125)get_bev_features()
-> translation_length = np.sqrt(delta_x ** 2 + delta_y ** 2)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(126)get_bev_features()
-> translation_angle = np.arctan2(delta_y, delta_x) / np.pi * 180
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(127)get_bev_features()
-> bev_angle = ego_angle - translation_angle
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(128)get_bev_features()
-> shift_y = translation_length * \
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(129)get_bev_features()
-> np.cos(bev_angle / 180 * np.pi) / grid_length_y / bev_h
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(128)get_bev_features()
-> shift_y = translation_length * \
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(129)get_bev_features()
-> np.cos(bev_angle / 180 * np.pi) / grid_length_y / bev_h
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(128)get_bev_features()
-> shift_y = translation_length * \
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(129)get_bev_features()
-> np.cos(bev_angle / 180 * np.pi) / grid_length_y / bev_h
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(128)get_bev_features()
-> shift_y = translation_length * \
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(130)get_bev_features()
-> shift_x = translation_length * \
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(131)get_bev_features()
-> np.sin(bev_angle / 180 * np.pi) / grid_length_x / bev_w
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(130)get_bev_features()
-> shift_x = translation_length * \
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(131)get_bev_features()
-> np.sin(bev_angle / 180 * np.pi) / grid_length_x / bev_w
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(130)get_bev_features()
-> shift_x = translation_length * \
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(131)get_bev_features()
-> np.sin(bev_angle / 180 * np.pi) / grid_length_x / bev_w
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(130)get_bev_features()
-> shift_x = translation_length * \
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(132)get_bev_features()
-> shift_y = shift_y * self.use_shift
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(133)get_bev_features()
-> shift_x = shift_x * self.use_shift
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(134)get_bev_features()
-> shift = bev_queries.new_tensor(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(135)get_bev_features()
-> [shift_x, shift_y]).permute(1, 0)  # xy, bs -> bs, xy
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(134)get_bev_features()
-> shift = bev_queries.new_tensor(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(135)get_bev_features()
-> [shift_x, shift_y]).permute(1, 0)  # xy, bs -> bs, xy
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(134)get_bev_features()
-> shift = bev_queries.new_tensor(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(137)get_bev_features()
-> if prev_bev is not None:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(152)get_bev_features()
-> can_bus = bev_queries.new_tensor(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(153)get_bev_features()
-> [each['can_bus'] for each in img_metas])  # [:, :]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(152)get_bev_features()
-> can_bus = bev_queries.new_tensor(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(154)get_bev_features()
-> can_bus = self.can_bus_mlp(can_bus)[None, :, :]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(155)get_bev_features()
-> bev_queries = bev_queries + can_bus * self.use_can_bus
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(157)get_bev_features()
-> feat_flatten = []
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(158)get_bev_features()
-> spatial_shapes = []
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(159)get_bev_features()
-> for lvl, feat in enumerate(mlvl_feats):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(160)get_bev_features()
-> bs, num_cam, c, h, w = feat.shape
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(161)get_bev_features()
-> spatial_shape = (h, w)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(162)get_bev_features()
-> feat = feat.flatten(3).permute(1, 0, 3, 2)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(163)get_bev_features()
-> if self.use_cams_embeds:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(164)get_bev_features()
-> feat = feat + self.cams_embeds[:, None, None, :].to(feat.dtype)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(165)get_bev_features()
-> feat = feat + self.level_embeds[None,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(166)get_bev_features()
-> None, lvl:lvl + 1, :].to(feat.dtype)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(165)get_bev_features()
-> feat = feat + self.level_embeds[None,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(166)get_bev_features()
-> None, lvl:lvl + 1, :].to(feat.dtype)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(165)get_bev_features()
-> feat = feat + self.level_embeds[None,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(167)get_bev_features()
-> spatial_shapes.append(spatial_shape)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(168)get_bev_features()
-> feat_flatten.append(feat)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(159)get_bev_features()
-> for lvl, feat in enumerate(mlvl_feats):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(160)get_bev_features()
-> bs, num_cam, c, h, w = feat.shape
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(161)get_bev_features()
-> spatial_shape = (h, w)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(162)get_bev_features()
-> feat = feat.flatten(3).permute(1, 0, 3, 2)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(163)get_bev_features()
-> if self.use_cams_embeds:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(164)get_bev_features()
-> feat = feat + self.cams_embeds[:, None, None, :].to(feat.dtype)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(165)get_bev_features()
-> feat = feat + self.level_embeds[None,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(166)get_bev_features()
-> None, lvl:lvl + 1, :].to(feat.dtype)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(165)get_bev_features()
-> feat = feat + self.level_embeds[None,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(166)get_bev_features()
-> None, lvl:lvl + 1, :].to(feat.dtype)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(165)get_bev_features()
-> feat = feat + self.level_embeds[None,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(167)get_bev_features()
-> spatial_shapes.append(spatial_shape)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(168)get_bev_features()
-> feat_flatten.append(feat)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(159)get_bev_features()
-> for lvl, feat in enumerate(mlvl_feats):
(Pdb) 154  	        can_bus = self.can_bus_mlp(can_bus)[None, :, :]
155  	        bev_queries = bev_queries + can_bus * self.use_can_bus
156  	
157  	        feat_flatten = []
158  	        spatial_shapes = []
159  ->	        for lvl, feat in enumerate(mlvl_feats):
160  	            bs, num_cam, c, h, w = feat.shape
161  	            spatial_shape = (h, w)
162  	            feat = feat.flatten(3).permute(1, 0, 3, 2)
163  	            if self.use_cams_embeds:
164  	                feat = feat + self.cams_embeds[:, None, None, :].to(feat.dtype)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(181)forward()
-> output = bev_query
(Pdb) 176  	            Tensor: Results with shape [1, num_query, bs, embed_dims] when
177  	                return_intermediate is `False`, otherwise it has shape
178  	                [num_layers, num_query, bs, embed_dims].
179  	        """
180  	        import pdb; pdb.set_trace()
181  ->	        output = bev_query
182  	        intermediate = []
183  	
184  	        ref_3d = self.get_reference_points(
185  	            bev_h, bev_w, self.pc_range[5]-self.pc_range[2], self.num_points_in_pillar, dim='3d', bs=bev_query.size(1),  device=bev_query.device, dtype=bev_query.dtype)
186  	        ref_2d = self.get_reference_points(
(Pdb) 200
(Pdb) 200
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(182)forward()
-> intermediate = []
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(184)forward()
-> ref_3d = self.get_reference_points(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(185)forward()
-> bev_h, bev_w, self.pc_range[5]-self.pc_range[2], self.num_points_in_pillar, dim='3d', bs=bev_query.size(1),  device=bev_query.device, dtype=bev_query.dtype)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(184)forward()
-> ref_3d = self.get_reference_points(
(Pdb) --Call--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(48)get_reference_points()
-> @staticmethod
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(63)get_reference_points()
-> if dim == '3d':
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(64)get_reference_points()
-> zs = torch.linspace(0.5, Z - 0.5, num_points_in_pillar, dtype=dtype,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(65)get_reference_points()
-> device=device).view(-1, 1, 1).expand(num_points_in_pillar, H, W) / Z
(Pdb) *** NameError: name 'zs' is not defined
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(64)get_reference_points()
-> zs = torch.linspace(0.5, Z - 0.5, num_points_in_pillar, dtype=dtype,
(Pdb) *** NameError: name 'zs' is not defined
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(65)get_reference_points()
-> device=device).view(-1, 1, 1).expand(num_points_in_pillar, H, W) / Z
(Pdb) *** NameError: name 'zs' is not defined
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(64)get_reference_points()
-> zs = torch.linspace(0.5, Z - 0.5, num_points_in_pillar, dtype=dtype,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(65)get_reference_points()
-> device=device).view(-1, 1, 1).expand(num_points_in_pillar, H, W) / Z
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(64)get_reference_points()
-> zs = torch.linspace(0.5, Z - 0.5, num_points_in_pillar, dtype=dtype,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(65)get_reference_points()
-> device=device).view(-1, 1, 1).expand(num_points_in_pillar, H, W) / Z
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(64)get_reference_points()
-> zs = torch.linspace(0.5, Z - 0.5, num_points_in_pillar, dtype=dtype,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(66)get_reference_points()
-> xs = torch.linspace(0.5, W - 0.5, W, dtype=dtype,
(Pdb) torch.Size([4, 200, 200])
(Pdb) 8.0
(Pdb) tensor(0.0625)
(Pdb) [1, 0, 0]
(Pdb) tensor(0.3542)
(Pdb) tensor(0.9375)
(Pdb) tensor(0.0625)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(67)get_reference_points()
-> device=device).view(1, 1, W).expand(num_points_in_pillar, H, W) / W
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(66)get_reference_points()
-> xs = torch.linspace(0.5, W - 0.5, W, dtype=dtype,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(67)get_reference_points()
-> device=device).view(1, 1, W).expand(num_points_in_pillar, H, W) / W
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(66)get_reference_points()
-> xs = torch.linspace(0.5, W - 0.5, W, dtype=dtype,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(67)get_reference_points()
-> device=device).view(1, 1, W).expand(num_points_in_pillar, H, W) / W
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(66)get_reference_points()
-> xs = torch.linspace(0.5, W - 0.5, W, dtype=dtype,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(67)get_reference_points()
-> device=device).view(1, 1, W).expand(num_points_in_pillar, H, W) / W
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(66)get_reference_points()
-> xs = torch.linspace(0.5, W - 0.5, W, dtype=dtype,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(68)get_reference_points()
-> ys = torch.linspace(0.5, H - 0.5, H, dtype=dtype,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(69)get_reference_points()
-> device=device).view(1, H, 1).expand(num_points_in_pillar, H, W) / H
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(68)get_reference_points()
-> ys = torch.linspace(0.5, H - 0.5, H, dtype=dtype,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(69)get_reference_points()
-> device=device).view(1, H, 1).expand(num_points_in_pillar, H, W) / H
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(68)get_reference_points()
-> ys = torch.linspace(0.5, H - 0.5, H, dtype=dtype,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(69)get_reference_points()
-> device=device).view(1, H, 1).expand(num_points_in_pillar, H, W) / H
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(68)get_reference_points()
-> ys = torch.linspace(0.5, H - 0.5, H, dtype=dtype,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(69)get_reference_points()
-> device=device).view(1, H, 1).expand(num_points_in_pillar, H, W) / H
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(68)get_reference_points()
-> ys = torch.linspace(0.5, H - 0.5, H, dtype=dtype,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(70)get_reference_points()
-> ref_3d = torch.stack((xs, ys, zs), -1)
(Pdb) torch.Size([4, 200, 200])
(Pdb) torch.Size([4, 200, 200])
(Pdb) (Pdb) torch.Size([1, 1, 200])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(71)get_reference_points()
-> ref_3d = ref_3d.permute(0, 3, 1, 2).flatten(2).permute(0, 2, 1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(72)get_reference_points()
-> ref_3d = ref_3d[None].repeat(bs, 1, 1, 1)
(Pdb) torch.Size([4, 40000, 3])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(73)get_reference_points()
-> return ref_3d
(Pdb) torch.Size([1, 4, 40000, 3])
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(73)get_reference_points()->tensor([[[[0....5, 0.9375]]]])
-> return ref_3d
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(186)forward()
-> ref_2d = self.get_reference_points(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(187)forward()
-> bev_h, bev_w, dim='2d', bs=bev_query.size(1), device=bev_query.device, dtype=bev_query.dtype)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(186)forward()
-> ref_2d = self.get_reference_points(
(Pdb) --Call--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(48)get_reference_points()
-> @staticmethod
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(63)get_reference_points()
-> if dim == '3d':
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(76)get_reference_points()
-> elif dim == '2d':
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(77)get_reference_points()
-> ref_y, ref_x = torch.meshgrid(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(78)get_reference_points()
-> torch.linspace(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(79)get_reference_points()
-> 0.5, H - 0.5, H, dtype=dtype, device=device),
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(78)get_reference_points()
-> torch.linspace(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(80)get_reference_points()
-> torch.linspace(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(81)get_reference_points()
-> 0.5, W - 0.5, W, dtype=dtype, device=device)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(80)get_reference_points()
-> torch.linspace(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(77)get_reference_points()
-> ref_y, ref_x = torch.meshgrid(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(83)get_reference_points()
-> ref_y = ref_y.reshape(-1)[None] / H
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(84)get_reference_points()
-> ref_x = ref_x.reshape(-1)[None] / W
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(85)get_reference_points()
-> ref_2d = torch.stack((ref_x, ref_y), -1)
(Pdb) torch.Size([1, 40000])
(Pdb) torch.Size([1, 40000])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(86)get_reference_points()
-> ref_2d = ref_2d.repeat(bs, 1, 1).unsqueeze(2)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(87)get_reference_points()
-> return ref_2d
(Pdb) torch.Size([1, 40000, 1, 2])
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(87)get_reference_points()->tensor([[[[0....5, 0.9975]]]])
-> return ref_2d
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(189)forward()
-> reference_points_cam, bev_mask = self.point_sampling(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(190)forward()
-> ref_3d, self.pc_range, img_metas)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(189)forward()
-> reference_points_cam, bev_mask = self.point_sampling(
(Pdb) --Call--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(178)new_func()
-> @functools.wraps(old_func)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(182)new_func()
-> if not isinstance(args[0], torch.nn.Module):
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(185)new_func()
-> if not (hasattr(args[0], 'fp16_enabled') and args[0].fp16_enabled):
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(186)new_func()
-> return old_func(*args, **kwargs)
(Pdb) --Call--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(90)point_sampling()
-> @force_fp32(apply_to=('reference_points', 'img_metas'))
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(93)point_sampling()
-> lidar2img = []
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(94)point_sampling()
-> for img_meta in img_metas:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(95)point_sampling()
-> lidar2img.append(img_meta['lidar2img'])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(94)point_sampling()
-> for img_meta in img_metas:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(96)point_sampling()
-> lidar2img = np.asarray(lidar2img)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(97)point_sampling()
-> lidar2img = reference_points.new_tensor(lidar2img)  # (B, N, 4, 4)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(98)point_sampling()
-> reference_points = reference_points.clone()
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(100)point_sampling()
-> reference_points[..., 0:1] = reference_points[..., 0:1] * \
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(101)point_sampling()
-> (pc_range[3] - pc_range[0]) + pc_range[0]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(100)point_sampling()
-> reference_points[..., 0:1] = reference_points[..., 0:1] * \
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(101)point_sampling()
-> (pc_range[3] - pc_range[0]) + pc_range[0]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(100)point_sampling()
-> reference_points[..., 0:1] = reference_points[..., 0:1] * \
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(102)point_sampling()
-> reference_points[..., 1:2] = reference_points[..., 1:2] * \
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(103)point_sampling()
-> (pc_range[4] - pc_range[1]) + pc_range[1]
(Pdb) torch.Size([1, 4, 40000, 3])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(102)point_sampling()
-> reference_points[..., 1:2] = reference_points[..., 1:2] * \
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(103)point_sampling()
-> (pc_range[4] - pc_range[1]) + pc_range[1]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(102)point_sampling()
-> reference_points[..., 1:2] = reference_points[..., 1:2] * \
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(104)point_sampling()
-> reference_points[..., 2:3] = reference_points[..., 2:3] * \
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(105)point_sampling()
-> (pc_range[5] - pc_range[2]) + pc_range[2]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(104)point_sampling()
-> reference_points[..., 2:3] = reference_points[..., 2:3] * \
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(105)point_sampling()
-> (pc_range[5] - pc_range[2]) + pc_range[2]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(104)point_sampling()
-> reference_points[..., 2:3] = reference_points[..., 2:3] * \
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(107)point_sampling()
-> reference_points = torch.cat(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(108)point_sampling()
-> (reference_points, torch.ones_like(reference_points[..., :1])), -1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(107)point_sampling()
-> reference_points = torch.cat(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(110)point_sampling()
-> reference_points = reference_points.permute(1, 0, 2, 3)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(111)point_sampling()
-> D, B, num_query = reference_points.size()[:3]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(112)point_sampling()
-> num_cam = lidar2img.size(1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(114)point_sampling()
-> reference_points = reference_points.view(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(115)point_sampling()
-> D, B, 1, num_query, 4).repeat(1, 1, num_cam, 1, 1).unsqueeze(-1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(114)point_sampling()
-> reference_points = reference_points.view(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(115)point_sampling()
-> D, B, 1, num_query, 4).repeat(1, 1, num_cam, 1, 1).unsqueeze(-1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(114)point_sampling()
-> reference_points = reference_points.view(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(115)point_sampling()
-> D, B, 1, num_query, 4).repeat(1, 1, num_cam, 1, 1).unsqueeze(-1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(114)point_sampling()
-> reference_points = reference_points.view(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(117)point_sampling()
-> lidar2img = lidar2img.view(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(118)point_sampling()
-> 1, B, num_cam, 1, 4, 4).repeat(D, 1, 1, num_query, 1, 1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(117)point_sampling()
-> lidar2img = lidar2img.view(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(118)point_sampling()
-> 1, B, num_cam, 1, 4, 4).repeat(D, 1, 1, num_query, 1, 1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(117)point_sampling()
-> lidar2img = lidar2img.view(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(120)point_sampling()
-> reference_points_cam = torch.matmul(lidar2img.to(torch.float32),
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(121)point_sampling()
-> reference_points.to(torch.float32)).squeeze(-1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(120)point_sampling()
-> reference_points_cam = torch.matmul(lidar2img.to(torch.float32),
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(121)point_sampling()
-> reference_points.to(torch.float32)).squeeze(-1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(120)point_sampling()
-> reference_points_cam = torch.matmul(lidar2img.to(torch.float32),
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(122)point_sampling()
-> eps = 1e-5
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(124)point_sampling()
-> bev_mask = (reference_points_cam[..., 2:3] > eps)
(Pdb) torch.Size([4, 1, 6, 40000, 4])
(Pdb) 4
(Pdb) 1
(Pdb) 40000
(Pdb) torch.Size([4, 1, 6, 40000, 4, 1])
(Pdb) torch.Size([4, 1, 6, 40000, 4, 4])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(125)point_sampling()
-> reference_points_cam = reference_points_cam[..., 0:2] / torch.maximum(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(126)point_sampling()
-> reference_points_cam[..., 2:3], torch.ones_like(reference_points_cam[..., 2:3]) * eps)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(125)point_sampling()
-> reference_points_cam = reference_points_cam[..., 0:2] / torch.maximum(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(128)point_sampling()
-> reference_points_cam[..., 0] /= img_metas[0]['img_shape'][0][1]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(129)point_sampling()
-> reference_points_cam[..., 1] /= img_metas[0]['img_shape'][0][0]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(131)point_sampling()
-> bev_mask = (bev_mask & (reference_points_cam[..., 1:2] > 0.0)
(Pdb) torch.Size([4, 1, 6, 40000, 2])
(Pdb) torch.Size([4, 1, 6, 40000, 1])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(132)point_sampling()
-> & (reference_points_cam[..., 1:2] < 1.0)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(131)point_sampling()
-> bev_mask = (bev_mask & (reference_points_cam[..., 1:2] > 0.0)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(133)point_sampling()
-> & (reference_points_cam[..., 0:1] < 1.0)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(131)point_sampling()
-> bev_mask = (bev_mask & (reference_points_cam[..., 1:2] > 0.0)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(134)point_sampling()
-> & (reference_points_cam[..., 0:1] > 0.0))
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(131)point_sampling()
-> bev_mask = (bev_mask & (reference_points_cam[..., 1:2] > 0.0)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(135)point_sampling()
-> if digit_version(TORCH_VERSION) >= digit_version('1.8'):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(136)point_sampling()
-> bev_mask = torch.nan_to_num(bev_mask)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(141)point_sampling()
-> reference_points_cam = reference_points_cam.permute(2, 1, 3, 0, 4)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(142)point_sampling()
-> bev_mask = bev_mask.permute(2, 1, 3, 0, 4).squeeze(-1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(144)point_sampling()
-> return reference_points_cam, bev_mask
(Pdb) torch.Size([6, 1, 40000, 4])
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(144)point_sampling()->(tensor([[[[[-...9260e-01]]]]]), tensor([[[[Fa...se, False]]]]))
-> return reference_points_cam, bev_mask
(Pdb) --Return--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(186)new_func()->(tensor([[[[[-...9260e-01]]]]]), tensor([[[[Fa...se, False]]]]))
-> return old_func(*args, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(193)forward()
-> shift_ref_2d = ref_2d  # .clone()
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(194)forward()
-> shift_ref_2d += shift[:, None, None, :]
(Pdb) torch.Size([1, 40000, 1, 2])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(197)forward()
-> bev_query = bev_query.permute(1, 0, 2)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(198)forward()
-> bev_pos = bev_pos.permute(1, 0, 2)
(Pdb) torch.Size([1, 40000, 256])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(199)forward()
-> bs, len_bev, num_bev_level, _ = ref_2d.shape
(Pdb) torch.Size([1, 40000, 256])
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(98)new_func()
-> return old_func(*args, **kwargs)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(159)get_bev_features()
-> for lvl, feat in enumerate(mlvl_feats):
(Pdb) 154  	        can_bus = self.can_bus_mlp(can_bus)[None, :, :]
155  	        bev_queries = bev_queries + can_bus * self.use_can_bus
156  	
157  	        feat_flatten = []
158  	        spatial_shapes = []
159  ->	        for lvl, feat in enumerate(mlvl_feats):
160  	            bs, num_cam, c, h, w = feat.shape
161  	            spatial_shape = (h, w)
162  	            feat = feat.flatten(3).permute(1, 0, 3, 2)
163  	            if self.use_cams_embeds:
164  	                feat = feat + self.cams_embeds[:, None, None, :].to(feat.dtype)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(98)new_func()
-> return old_func(*args, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head.py(150)get_bev_features()
-> bev_embed = self.transformer.get_bev_features(
(Pdb) 145  	        bev_queries = self.bev_embedding.weight.to(dtype)
146  	
147  	        bev_mask = torch.zeros((bs, self.bev_h, self.bev_w),
148  	                               device=bev_queries.device).to(dtype)
149  	        bev_pos = self.positional_encoding(bev_mask).to(dtype)
150  ->	        bev_embed = self.transformer.get_bev_features(
151  	            mlvl_feats,
152  	            bev_queries,
153  	            self.bev_h,
154  	            self.bev_w,
155  	            grid_length=(self.real_h / self.bev_h,
(Pdb) LearnedPositionalEncoding(num_feats=128, row_num_embed=200, col_num_embed=200)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(98)new_func()
-> return old_func(*args, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(159)get_bev_features()
-> for lvl, feat in enumerate(mlvl_feats):
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(98)new_func()
-> return old_func(*args, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(199)forward()
-> bs, len_bev, num_bev_level, _ = ref_2d.shape
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(200)forward()
-> if prev_bev is not None:
(Pdb) 40000
(Pdb) 1
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(207)forward()
-> hybird_ref_2d = torch.stack([ref_2d, ref_2d], 1).reshape(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(208)forward()
-> bs*2, len_bev, num_bev_level, 2)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(207)forward()
-> hybird_ref_2d = torch.stack([ref_2d, ref_2d], 1).reshape(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(210)forward()
-> for lid, layer in enumerate(self.layers):
(Pdb) torch.Size([2, 40000, 1, 2])
(Pdb) torch.Size([1, 40000, 1, 2])
(Pdb) 205  	                bs*2, len_bev, num_bev_level, 2)
206  	        else:
207  	            hybird_ref_2d = torch.stack([ref_2d, ref_2d], 1).reshape(
208  	                bs*2, len_bev, num_bev_level, 2)
209  	
210  ->	        for lid, layer in enumerate(self.layers):
211  	            output = layer(
212  	                bev_query,
213  	                key,
214  	                value,
215  	                *args,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(211)forward()
-> output = layer(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(334)forward()
-> norm_index = 0
(Pdb) 329  	
330  	        Returns:
331  	            Tensor: forwarded results with shape [num_queries, bs, embed_dims].
332  	        """
333  	        import pdb; pdb.set_trace()
334  ->	        norm_index = 0
335  	        attn_index = 0
336  	        ffn_index = 0
337  	        identity = query
338  	        if attn_masks is None:
339  	            attn_masks = [None for _ in range(self.num_attn)]
(Pdb) torch.Size([2, 40000, 1, 2])
(Pdb) torch.Size([1, 4, 40000, 3])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(335)forward()
-> attn_index = 0
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(336)forward()
-> ffn_index = 0
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(337)forward()
-> identity = query
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(338)forward()
-> if attn_masks is None:
(Pdb) tensor([[[[  3.1990,   6.4286,   3.5336,  ...,   8.3173,  -0.5325,   2.3804]],

         [[  6.4288,   8.9205,   4.3692,  ...,   6.2627,   1.9760,   6.1840]],

         [[  3.0975,   7.3454,   2.7196,  ...,  -1.3326,   5.5200,   3.7259]],

         ...,

         [[ -3.8663,  11.8676,  10.4982,  ...,  11.2748, -14.8558,  -9.5456]],

         [[ -1.7726,   9.9349,  10.4040,  ...,  13.8707, -14.5492,  -8.7125]],

         [[ -7.7436,   5.9941,  21.6763,  ...,  18.0718, -16.3144, -16.2851]]],


        [[[ -4.5418,   4.4145,   3.8405,  ...,   8.3078,   1.3050,  -4.2951]],

         [[ -3.4916,   9.8733,   6.7721,  ...,  11.2092,   4.3993,  -0.0681]],

         [[ -1.9377,   8.3983,   6.9626,  ...,  10.5152,   4.3201,   0.7689]],

         ...,

         [[ -8.9876,   6.8889,   5.1479,  ...,  13.7610, -12.3399, -13.9070]],

         [[-10.9650,   9.2815,   3.8063,  ...,  16.6990, -10.3758, -14.4184]],

         [[ -9.9806,   6.1271,   5.9371,  ...,  20.3728,  -6.8561, -19.7159]]],


        [[[  2.1969,   6.0079,   5.0824,  ...,   6.3029,   0.0929,  -5.2978]],

         [[ 11.7498,  12.2752,   6.1298,  ...,   8.4064,   0.5133,  -1.9068]],

         [[ 14.2645,  13.5757,   9.1516,  ...,   7.9495,  -0.7175,  -2.4951]],

         ...,

         [[  0.8495,   5.1743,   9.4434,  ...,   7.0961, -10.5130,  -5.7193]],

         [[  1.8320,   4.8487,   7.3360,  ...,   6.0082, -10.0290,  -8.3471]],

         [[  0.4157,   5.8008,   9.4776,  ...,   5.5317,  -8.0861, -10.7007]]],


        [[[ -6.6188,   2.3106,   9.6888,  ...,   1.3805,  -0.8795,  -9.2438]],

         [[ -4.5702,   4.8406,  10.2454,  ...,  -0.3396,   0.5274,  -8.8564]],

         [[ -2.9052,   4.0843,   8.3252,  ...,  -1.7976,  -0.3526, -10.0691]],

         ...,

         [[ -3.9013,   3.3275,  36.3824,  ...,   4.2961,  -9.5027,   6.3013]],

         [[ -4.4623,   0.1012,  35.8446,  ...,  -5.1390,  -0.4700,   8.6162]],

         [[ -7.4175,   2.2471,  25.2476,  ...,  -1.6352,   8.0412,  -3.1918]]],


        [[[ -0.9880,  -5.8253,   0.5559,  ...,   8.0490,   0.0420,  -6.5475]],

         [[  3.1637,  -5.5724,  -2.1822,  ...,  10.3989,   2.8952,  -6.8383]],

         [[  3.2908,  -5.7172,  -3.6086,  ...,  10.3329,   3.5472,  -7.4258]],

         ...,

         [[  3.5622, -21.1074,   6.3215,  ...,  19.8945, -20.0743, -11.4611]],

         [[  6.4270, -19.8431,  10.4512,  ...,  18.3839, -23.1454,  -8.9378]],

         [[  7.9200, -17.8035,  13.6424,  ...,  14.7619, -26.2136, -10.8642]]],


        [[[  4.3615,   1.9385,   1.1825,  ...,   7.2843,  -3.5502, -11.1966]],

         [[  8.2695,   5.8804,  -1.1755,  ...,   9.0412,  -5.9583, -10.1197]],

         [[  6.1884,   8.3551,   0.2251,  ...,   7.6944,  -6.3587,  -8.7386]],

         ...,

         [[ -6.2765, -10.4733,  13.0415,  ...,   8.5676, -24.3095,  -6.9338]],

         [[ -4.9148, -16.4974,  14.2527,  ...,  14.7695, -23.7201,  -7.1145]],

         [[  3.1160, -19.5491,  27.6913,  ...,   9.3798, -20.5175, -14.2333]]]])
(Pdb) torch.Size([6, 30825, 1, 256])
(Pdb) torch.Size([6, 30825, 1, 256])
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(211)forward()
-> output = layer(
(Pdb) 206  	        else:
207  	            hybird_ref_2d = torch.stack([ref_2d, ref_2d], 1).reshape(
208  	                bs*2, len_bev, num_bev_level, 2)
209  	
210  	        for lid, layer in enumerate(self.layers):
211  ->	            output = layer(
212  	                bev_query,
213  	                key,
214  	                value,
215  	                *args,
216  	                bev_pos=bev_pos,
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(98)new_func()
-> return old_func(*args, **kwargs)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(179)get_bev_features()
-> bev_embed = self.encoder(
(Pdb) 174  	            (1,)), spatial_shapes.prod(1).cumsum(0)[:-1]))
175  	
176  	        feat_flatten = feat_flatten.permute(
177  	            0, 2, 1, 3)  # (num_cam, H*W, bs, embed_dims)
178  	
179  ->	        bev_embed = self.encoder(
180  	            bev_queries,
181  	            feat_flatten,
182  	            feat_flatten,
183  	            bev_h=bev_h,
184  	            bev_w=bev_w,
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(98)new_func()
-> return old_func(*args, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(211)forward()
-> output = layer(
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(338)forward()
-> if attn_masks is None:
(Pdb) 333  	        import pdb; pdb.set_trace()
334  	        norm_index = 0
335  	        attn_index = 0
336  	        ffn_index = 0
337  	        identity = query
338  ->	        if attn_masks is None:
339  	            attn_masks = [None for _ in range(self.num_attn)]
340  	        elif isinstance(attn_masks, torch.Tensor):
341  	            attn_masks = [
342  	                copy.deepcopy(attn_masks) for _ in range(self.num_attn)
343  	            ]
(Pdb) torch.Size([6, 30825, 1, 256])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(339)forward()
-> attn_masks = [None for _ in range(self.num_attn)]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(352)forward()
-> for layer in self.operation_order:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(354)forward()
-> if layer == 'self_attn':
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(356)forward()
-> query = self.attentions[attn_index](
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(357)forward()
-> query,
(Pdb) 352  	        for layer in self.operation_order:
353  	            # temporal self attention
354  	            if layer == 'self_attn':
355  	
356  	                query = self.attentions[attn_index](
357  ->	                    query,
358  	                    prev_bev,
359  	                    prev_bev,
360  	                    identity if self.pre_norm else None,
361  	                    query_pos=bev_pos,
362  	                    key_pos=bev_pos,
(Pdb) torch.Size([1, 40000, 256])
(Pdb) *** AttributeError: 'NoneType' object has no attribute 'shape'
(Pdb) (Pdb) torch.Size([1, 40000, 256])
(Pdb) tensor([[[-0.3692, -0.1346, -1.7494,  ..., -0.6419, -0.9804, -1.5040],
         [-0.3229, -0.5238, -0.0736,  ...,  1.4400,  0.3592,  0.0848],
         [ 0.9678, -0.5568, -0.5044,  ..., -0.3400, -0.9468, -0.5633],
         ...,
         [ 0.3519,  0.7324,  0.2420,  ..., -0.6904, -0.5197,  0.0501],
         [-0.0446, -0.6355,  1.0753,  ..., -0.6749, -1.5575, -0.1796],
         [ 0.7491,  0.0168, -0.1023,  ...,  0.0354,  0.1010, -0.6031]]])
(Pdb) torch.Size([1, 40000, 256])
(Pdb) torch.Size([1, 40000, 256])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(176)forward()
-> if value is None:
(Pdb) 171  	
172  	        Returns:
173  	             Tensor: forwarded results with shape [num_query, bs, embed_dims].
174  	        """
175  	        import pdb; pdb.set_trace()
176  ->	        if value is None:
177  	            assert self.batch_first
178  	            bs, len_bev, c = query.shape
179  	            value = torch.stack([query, query], 1).reshape(bs*2, len_bev, c)
180  	
181  	        if identity is None:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(357)forward()
-> query,
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(357)forward()
-> query,
(Pdb) 352  	        for layer in self.operation_order:
353  	            # temporal self attention
354  	            if layer == 'self_attn':
355  	
356  	                query = self.attentions[attn_index](
357  ->	                    query,
358  	                    prev_bev,
359  	                    prev_bev,
360  	                    identity if self.pre_norm else None,
361  	                    query_pos=bev_pos,
362  	                    key_pos=bev_pos,
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(176)forward()
-> if value is None:
(Pdb) 171  	
172  	        Returns:
173  	             Tensor: forwarded results with shape [num_query, bs, embed_dims].
174  	        """
175  	        import pdb; pdb.set_trace()
176  ->	        if value is None:
177  	            assert self.batch_first
178  	            bs, len_bev, c = query.shape
179  	            value = torch.stack([query, query], 1).reshape(bs*2, len_bev, c)
180  	
181  	        if identity is None:
(Pdb) *** AttributeError: 'NoneType' object has no attribute 'shape'
(Pdb) *** AttributeError: 'NoneType' object has no attribute 'shape'
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(177)forward()
-> assert self.batch_first
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(178)forward()
-> bs, len_bev, c = query.shape
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(179)forward()
-> value = torch.stack([query, query], 1).reshape(bs*2, len_bev, c)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(181)forward()
-> if identity is None:
(Pdb) torch.Size([1, 40000, 256])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(182)forward()
-> identity = query
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(183)forward()
-> if query_pos is not None:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(184)forward()
-> query = query + query_pos
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(185)forward()
-> if not self.batch_first:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(189)forward()
-> bs,  num_query, embed_dims = query.shape
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(190)forward()
-> _, num_value, _ = value.shape
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(191)forward()
-> assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_value
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(192)forward()
-> assert self.num_bev_queue == 2
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(194)forward()
-> query = torch.cat([value[:bs], query], -1)
(Pdb) torch.Size([2, 40000, 256])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(195)forward()
-> value = self.value_proj(value)
(Pdb) torch.Size([1, 40000, 512])
(Pdb) torch.Size([2, 40000, 256])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(197)forward()
-> if key_padding_mask is not None:
(Pdb) torch.Size([2, 40000, 256])
(Pdb) 1
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(200)forward()
-> value = value.reshape(bs*self.num_bev_queue,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(201)forward()
-> num_value, self.num_heads, -1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(200)forward()
-> value = value.reshape(bs*self.num_bev_queue,
(Pdb) torch.Size([2, 40000, 256])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(203)forward()
-> sampling_offsets = self.sampling_offsets(query)
(Pdb) torch.Size([1, 40000, 512])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(204)forward()
-> sampling_offsets = sampling_offsets.view(
(Pdb) torch.Size([1, 40000, 128])
(Pdb) 199  	
200  	        value = value.reshape(bs*self.num_bev_queue,
201  	                              num_value, self.num_heads, -1)
202  	
203  	        sampling_offsets = self.sampling_offsets(query)
204  ->	        sampling_offsets = sampling_offsets.view(
205  	            bs, num_query, self.num_heads,  self.num_bev_queue, self.num_levels, self.num_points, 2)
206  	        attention_weights = self.attention_weights(query).view(
207  	            bs, num_query,  self.num_heads, self.num_bev_queue, self.num_levels * self.num_points)
208  	        attention_weights = attention_weights.softmax(-1)
209  	
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(205)forward()
-> bs, num_query, self.num_heads,  self.num_bev_queue, self.num_levels, self.num_points, 2)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(204)forward()
-> sampling_offsets = sampling_offsets.view(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(206)forward()
-> attention_weights = self.attention_weights(query).view(
(Pdb) torch.Size([1, 40000, 8, 2, 1, 4, 2])
(Pdb) *** NameError: name 'N' is not defined
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(207)forward()
-> bs, num_query,  self.num_heads, self.num_bev_queue, self.num_levels * self.num_points)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(206)forward()
-> attention_weights = self.attention_weights(query).view(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(208)forward()
-> attention_weights = attention_weights.softmax(-1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(210)forward()
-> attention_weights = attention_weights.view(bs, num_query,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(211)forward()
-> self.num_heads,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(212)forward()
-> self.num_bev_queue,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(213)forward()
-> self.num_levels,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(214)forward()
-> self.num_points)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(210)forward()
-> attention_weights = attention_weights.view(bs, num_query,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(216)forward()
-> attention_weights = attention_weights.permute(0, 3, 1, 2, 4, 5)\
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(217)forward()
-> .reshape(bs*self.num_bev_queue, num_query, self.num_heads, self.num_levels, self.num_points).contiguous()
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(216)forward()
-> attention_weights = attention_weights.permute(0, 3, 1, 2, 4, 5)\
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(218)forward()
-> sampling_offsets = sampling_offsets.permute(0, 3, 1, 2, 4, 5, 6)\
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(219)forward()
-> .reshape(bs*self.num_bev_queue, num_query, self.num_heads, self.num_levels, self.num_points, 2)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(218)forward()
-> sampling_offsets = sampling_offsets.permute(0, 3, 1, 2, 4, 5, 6)\
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(221)forward()
-> if reference_points.shape[-1] == 2:
(Pdb) torch.Size([2, 40000, 8, 1, 4, 2])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(222)forward()
-> offset_normalizer = torch.stack(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(223)forward()
-> [spatial_shapes[..., 1], spatial_shapes[..., 0]], -1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(222)forward()
-> offset_normalizer = torch.stack(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(224)forward()
-> sampling_locations = reference_points[:, :, None, :, None, :] \
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(225)forward()
-> + sampling_offsets \
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(226)forward()
-> / offset_normalizer[None, None, None, :, None, :]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(225)forward()
-> + sampling_offsets \
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(224)forward()
-> sampling_locations = reference_points[:, :, None, :, None, :] \
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(237)forward()
-> if torch.cuda.is_available() and value.is_cuda:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(249)forward()
-> output = multi_scale_deformable_attn_pytorch(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(250)forward()
-> value, spatial_shapes, sampling_locations, attention_weights)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(249)forward()
-> output = multi_scale_deformable_attn_pytorch(
(Pdb) --Call--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(94)multi_scale_deformable_attn_pytorch()
-> def multi_scale_deformable_attn_pytorch(value, value_spatial_shapes,
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(116)multi_scale_deformable_attn_pytorch()
-> bs, _, num_heads, embed_dims = value.shape
(Pdb) *** NameError: name 'num_heads' is not defined
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/ops/multi_scale_deform_attn.py(118)multi_scale_deformable_attn_pytorch()
-> sampling_locations.shape
(Pdb) 113  	        Tensor: has shape (bs, num_queries, embed_dims)
114  	    """
115  	
116  	    bs, _, num_heads, embed_dims = value.shape
117  	    _, num_queries, num_heads, num_levels, num_points, _ =\
118  ->	        sampling_locations.shape
119  	    value_list = value.split([H_ * W_ for H_, W_ in value_spatial_shapes],
120  	                             dim=1)
121  	    sampling_grids = 2 * sampling_locations - 1
122  	    sampling_value_list = []
123  	    for level, (H_, W_) in enumerate(value_spatial_shapes):
(Pdb) 8
(Pdb) 32
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(249)forward()
-> output = multi_scale_deformable_attn_pytorch(
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(249)forward()
-> output = multi_scale_deformable_attn_pytorch(
(Pdb) 244  	            output = MultiScaleDeformableAttnFunction.apply(
245  	                value, spatial_shapes, level_start_index, sampling_locations,
246  	                attention_weights, self.im2col_step)
247  	        else:
248  	
249  ->	            output = multi_scale_deformable_attn_pytorch(
250  	                value, spatial_shapes, sampling_locations, attention_weights)
251  	
252  	        # output shape (bs*num_bev_queue, num_query, embed_dims)
253  	        # (bs*num_bev_queue, num_query, embed_dims)-> (num_query, embed_dims, bs*num_bev_queue)
254  	        output = output.permute(1, 2, 0)
(Pdb) torch.Size([2, 40000, 8, 1, 4, 2])
(Pdb) torch.Size([2, 40000, 8, 32])
(Pdb) torch.Size([2, 40000, 8, 1, 4])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(254)forward()
-> output = output.permute(1, 2, 0)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(258)forward()
-> output = output.view(num_query, embed_dims, bs, self.num_bev_queue)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(259)forward()
-> output = output.mean(-1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(262)forward()
-> output = output.permute(2, 0, 1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(264)forward()
-> output = self.output_proj(output)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(357)forward()
-> query,
(Pdb) 352  	        for layer in self.operation_order:
353  	            # temporal self attention
354  	            if layer == 'self_attn':
355  	
356  	                query = self.attentions[attn_index](
357  ->	                    query,
358  	                    prev_bev,
359  	                    prev_bev,
360  	                    identity if self.pre_norm else None,
361  	                    query_pos=bev_pos,
362  	                    key_pos=bev_pos,
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(264)forward()
-> output = self.output_proj(output)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(266)forward()
-> if not self.batch_first:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(269)forward()
-> return self.dropout(output) + identity
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(269)forward()->tensor([[[-3....6.0102e-01]]])
-> return self.dropout(output) + identity
(Pdb) --Return--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()->tensor([[[-3....6.0102e-01]]])
-> return forward_call(*input, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(370)forward()
-> attn_index += 1
(Pdb) 365  	                    reference_points=ref_2d,
366  	                    spatial_shapes=torch.tensor(
367  	                        [[bev_h, bev_w]], device=query.device),
368  	                    level_start_index=torch.tensor([0], device=query.device),
369  	                    **kwargs)
370  ->	                attn_index += 1
371  	                identity = query
372  	
373  	            elif layer == 'norm':
374  	                query = self.norms[norm_index](query)
375  	                norm_index += 1
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(371)forward()
-> identity = query
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(352)forward()
-> for layer in self.operation_order:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(354)forward()
-> if layer == 'self_attn':
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(373)forward()
-> elif layer == 'norm':
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(374)forward()
-> query = self.norms[norm_index](query)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(211)forward()
-> output = layer(
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(98)new_func()
-> return old_func(*args, **kwargs)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(179)get_bev_features()
-> bev_embed = self.encoder(
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(98)new_func()
-> return old_func(*args, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head.py(150)get_bev_features()
-> bev_embed = self.transformer.get_bev_features(
(Pdb) 145  	        bev_queries = self.bev_embedding.weight.to(dtype)
146  	
147  	        bev_mask = torch.zeros((bs, self.bev_h, self.bev_w),
148  	                               device=bev_queries.device).to(dtype)
149  	        bev_pos = self.positional_encoding(bev_mask).to(dtype)
150  ->	        bev_embed = self.transformer.get_bev_features(
151  	            mlvl_feats,
152  	            bev_queries,
153  	            self.bev_h,
154  	            self.bev_w,
155  	            grid_length=(self.real_h / self.bev_h,
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(98)new_func()
-> return old_func(*args, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(179)get_bev_features()
-> bev_embed = self.encoder(
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(98)new_func()
-> return old_func(*args, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(211)forward()
-> output = layer(
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(374)forward()
-> query = self.norms[norm_index](query)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(375)forward()
-> norm_index += 1
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(352)forward()
-> for layer in self.operation_order:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(354)forward()
-> if layer == 'self_attn':
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(373)forward()
-> elif layer == 'norm':
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(378)forward()
-> elif layer == 'cross_attn':
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(379)forward()
-> query = self.attentions[attn_index](
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(122)forward()
-> if key is None:
(Pdb) 117  	                as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].
118  	        Returns:
119  	             Tensor: forwarded results with shape [num_query, bs, embed_dims].
120  	        """
121  	        import pdb; pdb.set_trace()
122  ->	        if key is None:
123  	            key = query
124  	        if value is None:
125  	            value = key
126  	
127  	        if residual is None:
(Pdb) torch.Size([6, 30825, 1, 256])
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(186)new_func()
-> return old_func(*args, **kwargs)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(379)forward()
-> query = self.attentions[attn_index](
(Pdb) 374  	                query = self.norms[norm_index](query)
375  	                norm_index += 1
376  	
377  	            # spaital cross attention
378  	            elif layer == 'cross_attn':
379  ->	                query = self.attentions[attn_index](
380  	                    query,
381  	                    key,
382  	                    value,
383  	                    identity if self.pre_norm else None,
384  	                    query_pos=query_pos,
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(186)new_func()
-> return old_func(*args, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(122)forward()
-> if key is None:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(124)forward()
-> if value is None:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(127)forward()
-> if residual is None:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(128)forward()
-> inp_residual = query
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(129)forward()
-> slots = torch.zeros_like(query)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(130)forward()
-> if query_pos is not None:
(Pdb) torch.Size([1, 40000, 256])
(Pdb) torch.Size([6, 30825, 1, 256])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(133)forward()
-> bs, num_query, _ = query.size()
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(135)forward()
-> D = reference_points_cam.size(3)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(136)forward()
-> indexes = []
(Pdb) 4
(Pdb) torch.Size([6, 1, 40000, 4, 2])
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(186)new_func()
-> return old_func(*args, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(136)forward()
-> indexes = []
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(137)forward()
-> for i, mask_per_img in enumerate(bev_mask):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(138)forward()
-> index_query_per_img = mask_per_img[0].sum(-1).nonzero().squeeze(-1)
(Pdb) torch.Size([6, 1, 40000, 4])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(139)forward()
-> indexes.append(index_query_per_img)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(137)forward()
-> for i, mask_per_img in enumerate(bev_mask):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(138)forward()
-> index_query_per_img = mask_per_img[0].sum(-1).nonzero().squeeze(-1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(139)forward()
-> indexes.append(index_query_per_img)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(137)forward()
-> for i, mask_per_img in enumerate(bev_mask):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(138)forward()
-> index_query_per_img = mask_per_img[0].sum(-1).nonzero().squeeze(-1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(139)forward()
-> indexes.append(index_query_per_img)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(137)forward()
-> for i, mask_per_img in enumerate(bev_mask):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(138)forward()
-> index_query_per_img = mask_per_img[0].sum(-1).nonzero().squeeze(-1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(139)forward()
-> indexes.append(index_query_per_img)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(137)forward()
-> for i, mask_per_img in enumerate(bev_mask):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(138)forward()
-> index_query_per_img = mask_per_img[0].sum(-1).nonzero().squeeze(-1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(139)forward()
-> indexes.append(index_query_per_img)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(137)forward()
-> for i, mask_per_img in enumerate(bev_mask):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(138)forward()
-> index_query_per_img = mask_per_img[0].sum(-1).nonzero().squeeze(-1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(139)forward()
-> indexes.append(index_query_per_img)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(137)forward()
-> for i, mask_per_img in enumerate(bev_mask):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(140)forward()
-> max_len = max([len(each) for each in indexes])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(143)forward()
-> queries_rebatch = query.new_zeros(
(Pdb) 9502
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(144)forward()
-> [bs, self.num_cams, max_len, self.embed_dims])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(143)forward()
-> queries_rebatch = query.new_zeros(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(145)forward()
-> reference_points_rebatch = reference_points_cam.new_zeros(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(146)forward()
-> [bs, self.num_cams, max_len, D, 2])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(145)forward()
-> reference_points_rebatch = reference_points_cam.new_zeros(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(148)forward()
-> for j in range(bs):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(149)forward()
-> for i, reference_points_per_img in enumerate(reference_points_cam):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(150)forward()
-> index_query_per_img = indexes[i]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(151)forward()
-> queries_rebatch[j, i, :len(index_query_per_img)] = query[j, index_query_per_img]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(152)forward()
-> reference_points_rebatch[j, i, :len(index_query_per_img)] = reference_points_per_img[j, index_query_per_img]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(149)forward()
-> for i, reference_points_per_img in enumerate(reference_points_cam):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(150)forward()
-> index_query_per_img = indexes[i]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(151)forward()
-> queries_rebatch[j, i, :len(index_query_per_img)] = query[j, index_query_per_img]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(152)forward()
-> reference_points_rebatch[j, i, :len(index_query_per_img)] = reference_points_per_img[j, index_query_per_img]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(149)forward()
-> for i, reference_points_per_img in enumerate(reference_points_cam):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(150)forward()
-> index_query_per_img = indexes[i]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(151)forward()
-> queries_rebatch[j, i, :len(index_query_per_img)] = query[j, index_query_per_img]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(152)forward()
-> reference_points_rebatch[j, i, :len(index_query_per_img)] = reference_points_per_img[j, index_query_per_img]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(149)forward()
-> for i, reference_points_per_img in enumerate(reference_points_cam):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(150)forward()
-> index_query_per_img = indexes[i]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(151)forward()
-> queries_rebatch[j, i, :len(index_query_per_img)] = query[j, index_query_per_img]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(152)forward()
-> reference_points_rebatch[j, i, :len(index_query_per_img)] = reference_points_per_img[j, index_query_per_img]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(149)forward()
-> for i, reference_points_per_img in enumerate(reference_points_cam):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(150)forward()
-> index_query_per_img = indexes[i]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(151)forward()
-> queries_rebatch[j, i, :len(index_query_per_img)] = query[j, index_query_per_img]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(152)forward()
-> reference_points_rebatch[j, i, :len(index_query_per_img)] = reference_points_per_img[j, index_query_per_img]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(149)forward()
-> for i, reference_points_per_img in enumerate(reference_points_cam):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(150)forward()
-> index_query_per_img = indexes[i]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(151)forward()
-> queries_rebatch[j, i, :len(index_query_per_img)] = query[j, index_query_per_img]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(152)forward()
-> reference_points_rebatch[j, i, :len(index_query_per_img)] = reference_points_per_img[j, index_query_per_img]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(149)forward()
-> for i, reference_points_per_img in enumerate(reference_points_cam):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(148)forward()
-> for j in range(bs):
(Pdb) 1
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(154)forward()
-> num_cams, l, bs, embed_dims = key.shape
(Pdb) torch.Size([1, 6, 9502, 4, 2])
(Pdb) 9502
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(156)forward()
-> key = key.permute(2, 0, 1, 3).reshape(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(157)forward()
-> bs * self.num_cams, l, self.embed_dims)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(156)forward()
-> key = key.permute(2, 0, 1, 3).reshape(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(158)forward()
-> value = value.permute(2, 0, 1, 3).reshape(
(Pdb) torch.Size([6, 30825, 256])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(159)forward()
-> bs * self.num_cams, l, self.embed_dims)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(158)forward()
-> value = value.permute(2, 0, 1, 3).reshape(
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(161)forward()
-> queries = self.deformable_attention(query=queries_rebatch.view(bs*self.num_cams, max_len, self.embed_dims), key=key, value=value,
(Pdb) torch.Size([6, 30825, 256])
(Pdb) torch.Size([1, 6, 9502, 4, 2])
(Pdb) *** AttributeError: 'list' object has no attribute 'shape'
(Pdb) 6
(Pdb) *** AttributeError: 'NoneType' object has no attribute 'shape'
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(186)new_func()
-> return old_func(*args, **kwargs)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(379)forward()
-> query = self.attentions[attn_index](
(Pdb) *** AttributeError: 'NoneType' object has no attribute 'shape'
(Pdb) (Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(211)forward()
-> output = layer(
(Pdb) 206  	        else:
207  	            hybird_ref_2d = torch.stack([ref_2d, ref_2d], 1).reshape(
208  	                bs*2, len_bev, num_bev_level, 2)
209  	
210  	        for lid, layer in enumerate(self.layers):
211  ->	            output = layer(
212  	                bev_query,
213  	                key,
214  	                value,
215  	                *args,
216  	                bev_pos=bev_pos,
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(379)forward()
-> query = self.attentions[attn_index](
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(186)new_func()
-> return old_func(*args, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(161)forward()
-> queries = self.deformable_attention(query=queries_rebatch.view(bs*self.num_cams, max_len, self.embed_dims), key=key, value=value,
(Pdb) torch.Size([1, 6, 9502, 256])
(Pdb) torch.Size([1, 6, 9502, 4, 2])
(Pdb) 156  	        key = key.permute(2, 0, 1, 3).reshape(
157  	            bs * self.num_cams, l, self.embed_dims)
158  	        value = value.permute(2, 0, 1, 3).reshape(
159  	            bs * self.num_cams, l, self.embed_dims)
160  	
161  ->	        queries = self.deformable_attention(query=queries_rebatch.view(bs*self.num_cams, max_len, self.embed_dims), key=key, value=value,
162  	                                            reference_points=reference_points_rebatch.view(bs*self.num_cams, max_len, D, 2), spatial_shapes=spatial_shapes,
163  	                                            level_start_index=level_start_index).view(bs, self.num_cams, max_len, self.embed_dims)
164  	        for j in range(bs):
165  	            for i, index_query_per_img in enumerate(indexes):
166  	                slots[j, index_query_per_img] += queries[j, i, :len(index_query_per_img)]
(Pdb) 30825
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(162)forward()
-> reference_points=reference_points_rebatch.view(bs*self.num_cams, max_len, D, 2), spatial_shapes=spatial_shapes,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(163)forward()
-> level_start_index=level_start_index).view(bs, self.num_cams, max_len, self.embed_dims)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(161)forward()
-> queries = self.deformable_attention(query=queries_rebatch.view(bs*self.num_cams, max_len, self.embed_dims), key=key, value=value,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(163)forward()
-> level_start_index=level_start_index).view(bs, self.num_cams, max_len, self.embed_dims)
(Pdb) *** NameError: name 'queries' is not defined
(Pdb) *** NameError: name 'queries' is not defined
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(161)forward()
-> queries = self.deformable_attention(query=queries_rebatch.view(bs*self.num_cams, max_len, self.embed_dims), key=key, value=value,
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(164)forward()
-> for j in range(bs):
(Pdb) tensor([[[[-7.0429e-01,  2.5034e-01, -1.0141e+00,  ..., -9.0292e-01,
           -3.4877e-01,  8.7809e-01],
          [ 2.8180e-01, -2.1293e+00, -3.0065e-01,  ..., -2.6573e-01,
           -4.2596e-01, -6.0133e-02],
          [-4.3118e-01, -3.5719e+00,  4.6930e-01,  ..., -4.7710e-01,
           -2.3459e-01,  4.4025e-01],
          ...,
          [-1.7949e+00, -5.4718e+00,  8.1861e-01,  ...,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [-1.7949e+00, -5.4718e+00,  8.1861e-01,  ...,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [-1.7949e+00, -5.4718e+00,  8.1861e-01,  ...,  0.0000e+00,
            0.0000e+00,  0.0000e+00]],

         [[ 2.0840e+00, -4.5902e-01,  3.1519e-01,  ...,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 8.9014e-01, -7.6695e-01,  1.2820e+00,  ..., -2.9597e-01,
           -3.4449e-02, -7.8788e-02],
          [ 2.8377e-02, -2.6008e+00, -7.1845e-01,  ..., -1.1598e+01,
            9.2374e+00, -4.7609e+00],
          ...,
          [-1.1671e+00, -8.2531e-01,  9.3288e-02,  ...,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [-1.1671e+00, -8.2531e-01,  9.3288e-02,  ...,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [-1.1671e+00, -8.2531e-01,  9.3288e-02,  ...,  0.0000e+00,
            0.0000e+00,  0.0000e+00]],

         [[ 1.1986e+00, -9.2782e-01,  4.0572e-01,  ..., -4.6140e-01,
           -6.2168e-02, -1.3475e+00],
          [ 8.9273e-01,  1.3483e+00, -9.1452e-01,  ..., -8.7765e-02,
           -2.0175e-01,  3.1288e-01],
          [-1.3780e+00, -1.7461e+00,  2.0812e+00,  ..., -5.7179e-01,
           -8.5804e+00, -4.9219e+00],
          ...,
          [-2.4530e-01,  3.0373e-01, -3.5621e-02,  ...,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [-2.4530e-01,  3.0373e-01, -3.5621e-02,  ...,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [-2.4530e-01,  3.0373e-01, -3.5621e-02,  ...,  0.0000e+00,
            0.0000e+00,  0.0000e+00]],

         [[-2.8029e+00, -3.5197e-01,  2.0986e+00,  ..., -7.3786e-01,
           -1.9785e+00,  5.2536e-01],
          [-7.3875e-01,  4.3469e-03,  6.1572e-01,  ..., -1.4267e+00,
           -7.7210e-01,  1.5775e+00],
          [-3.4774e+00, -3.3320e-01,  2.3169e+00,  ..., -8.8341e-01,
           -3.7429e-01,  9.8922e-01],
          ...,
          [ 4.1104e-01, -1.0716e+00,  8.6595e-01,  ..., -2.0068e+00,
            8.3698e-01, -1.0928e-02],
          [ 1.1665e-01, -2.3870e-01,  1.4642e+00,  ...,  1.4747e-01,
            5.7517e-01, -5.4109e-01],
          [ 7.3305e-01, -1.0872e+00,  1.1524e+00,  ..., -5.1573e-01,
            1.4649e-01, -3.8450e-01]],

         [[ 2.2591e+00,  1.2815e+00, -1.4161e+00,  ..., -1.5255e+00,
           -2.0796e+00,  5.4683e-01],
          [ 2.3799e+00,  1.1875e+00,  3.1981e-02,  ..., -8.2790e+00,
            1.2455e+01, -7.1696e+00],
          [-1.2194e-01, -8.8690e-01,  2.9709e+00,  ..., -5.9817e+00,
           -1.8904e+00, -3.4465e+00],
          ...,
          [-7.1113e-01, -2.5035e+00,  1.0418e-01,  ...,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [-7.1113e-01, -2.5035e+00,  1.0418e-01,  ...,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [-7.1113e-01, -2.5035e+00,  1.0418e-01,  ...,  0.0000e+00,
            0.0000e+00,  0.0000e+00]],

         [[-1.4130e+00, -2.8700e+00,  3.7611e+00,  ...,  2.3668e-01,
           -3.4213e-01,  4.1421e-01],
          [-7.6786e-01, -9.2544e-01,  1.7417e+00,  ...,  5.2007e-01,
           -1.0750e+00,  8.7048e-01],
          [-1.6803e+00, -6.8472e-01,  1.0993e+00,  ...,  8.1432e-01,
           -9.9818e-01,  4.3274e-01],
          ...,
          [-1.3174e+00,  4.1480e-01,  4.5449e-01,  ...,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [-1.3174e+00,  4.1480e-01,  4.5449e-01,  ...,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [-1.3174e+00,  4.1480e-01,  4.5449e-01,  ...,  0.0000e+00,
            0.0000e+00,  0.0000e+00]]]])
(Pdb) torch.Size([1, 6, 9502, 256])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(165)forward()
-> for i, index_query_per_img in enumerate(indexes):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(166)forward()
-> slots[j, index_query_per_img] += queries[j, i, :len(index_query_per_img)]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(165)forward()
-> for i, index_query_per_img in enumerate(indexes):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(166)forward()
-> slots[j, index_query_per_img] += queries[j, i, :len(index_query_per_img)]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(165)forward()
-> for i, index_query_per_img in enumerate(indexes):
(Pdb) *** AttributeError: 'list' object has no attribute 'shape'
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(166)forward()
-> slots[j, index_query_per_img] += queries[j, i, :len(index_query_per_img)]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(165)forward()
-> for i, index_query_per_img in enumerate(indexes):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(166)forward()
-> slots[j, index_query_per_img] += queries[j, i, :len(index_query_per_img)]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(165)forward()
-> for i, index_query_per_img in enumerate(indexes):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(166)forward()
-> slots[j, index_query_per_img] += queries[j, i, :len(index_query_per_img)]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(165)forward()
-> for i, index_query_per_img in enumerate(indexes):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(166)forward()
-> slots[j, index_query_per_img] += queries[j, i, :len(index_query_per_img)]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(165)forward()
-> for i, index_query_per_img in enumerate(indexes):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(164)forward()
-> for j in range(bs):
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(168)forward()
-> count = bev_mask.sum(-1) > 0
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(169)forward()
-> count = count.permute(1, 2, 0).sum(-1)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(170)forward()
-> count = torch.clamp(count, min=1.0)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(171)forward()
-> slots = slots / count[..., None]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(172)forward()
-> slots = self.output_proj(slots)
(Pdb) torch.Size([1, 40000, 256])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(174)forward()
-> return self.dropout(slots) + inp_residual
(Pdb) torch.Size([1, 40000, 256])
(Pdb) torch.Size([6, 30825, 256])
(Pdb) torch.Size([1, 40000, 256])
(Pdb) --Return--
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(174)forward()->tensor([[[ 16..., -10.7230]]])
-> return self.dropout(slots) + inp_residual
(Pdb) --Return--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py(186)new_func()->tensor([[[ 16..., -10.7230]]])
-> return old_func(*args, **kwargs)
(Pdb) --Return--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()->tensor([[[ 16..., -10.7230]]])
-> return forward_call(*input, **kwargs)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(394)forward()
-> attn_index += 1
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(395)forward()
-> identity = query
(Pdb) torch.Size([1, 40000, 256])
(Pdb) torch.Size([1, 40000, 256])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(352)forward()
-> for layer in self.operation_order:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(354)forward()
-> if layer == 'self_attn':
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(373)forward()
-> elif layer == 'norm':
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(374)forward()
-> query = self.norms[norm_index](query)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(375)forward()
-> norm_index += 1
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(352)forward()
-> for layer in self.operation_order:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(354)forward()
-> if layer == 'self_attn':
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(373)forward()
-> elif layer == 'norm':
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(378)forward()
-> elif layer == 'cross_attn':
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(397)forward()
-> elif layer == 'ffn':
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(398)forward()
-> query = self.ffns[ffn_index](
(Pdb) --Call--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1117)__getattr__()
-> def __getattr__(self, name: str) -> Union[Tensor, 'Module']:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1118)__getattr__()
-> if '_parameters' in self.__dict__:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1119)__getattr__()
-> _parameters = self.__dict__['_parameters']
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1120)__getattr__()
-> if name in _parameters:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1122)__getattr__()
-> if '_buffers' in self.__dict__:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1123)__getattr__()
-> _buffers = self.__dict__['_buffers']
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1124)__getattr__()
-> if name in _buffers:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1126)__getattr__()
-> if '_modules' in self.__dict__:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1127)__getattr__()
-> modules = self.__dict__['_modules']
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1128)__getattr__()
-> if name in modules:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1129)__getattr__()
-> return modules[name]
(Pdb) --Return--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1129)__getattr__()->ModuleList(
 ...entity()
  )
)
-> return modules[name]
(Pdb) --Call--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/container.py(181)__getitem__()
-> @_copy_to_script_wrapper
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/container.py(183)__getitem__()
-> if isinstance(idx, slice):
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/container.py(186)__getitem__()
-> return self._modules[self._get_abs_string_index(idx)]
(Pdb) --Call--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/container.py(172)_get_abs_string_index()
-> def _get_abs_string_index(self, idx):
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/container.py(174)_get_abs_string_index()
-> idx = operator.index(idx)
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/container.py(175)_get_abs_string_index()
-> if not (-len(self) <= idx < len(self)):
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/container.py(177)_get_abs_string_index()
-> if idx < 0:
(Pdb) > /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/container.py(179)_get_abs_string_index()
-> return str(idx)
(Pdb) --Return--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/container.py(179)_get_abs_string_index()->'0'
-> return str(idx)
(Pdb) --Return--
> /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/container.py(186)__getitem__()->FFN(
  (activ...: Identity()
)
-> return self._modules[self._get_abs_string_index(idx)]
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(399)forward()
-> query, identity if self.pre_norm else None)
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(398)forward()
-> query = self.ffns[ffn_index](
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(400)forward()
-> ffn_index += 1
(Pdb) torch.Size([1, 40000, 256])
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(352)forward()
-> for layer in self.operation_order:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(354)forward()
-> if layer == 'self_attn':
(Pdb) 6
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(334)forward()
-> norm_index = 0
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(176)forward()
-> if value is None:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(122)forward()
-> if key is None:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(334)forward()
-> norm_index = 0
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(176)forward()
-> if value is None:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(122)forward()
-> if key is None:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(334)forward()
-> norm_index = 0
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(176)forward()
-> if value is None:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(122)forward()
-> if key is None:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(334)forward()
-> norm_index = 0
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(176)forward()
-> if value is None:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(122)forward()
-> if key is None:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py(334)forward()
-> norm_index = 0
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py(176)forward()
-> if value is None:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py(122)forward()
-> if key is None:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/decoder.py(279)forward()
-> if value is None:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/decoder.py(279)forward()
-> if value is None:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/decoder.py(279)forward()
-> if value is None:
(Pdb) > /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/decoder.py(279)forward()
-> if value is None:
(Pdb) 274  	
275  	        Returns:
276  	             Tensor: forwarded results with shape [num_query, bs, embed_dims].
277  	        """
278  	        import pdb; pdb.set_trace()
279  ->	        if value is None:
280  	            value = query
281  	
282  	        if identity is None:
283  	            identity = query
284  	        if query_pos is not None:
(Pdb)   /Users/liangming.xu/code/UniAD/tools/test.py(273)<module>()
-> main()
  /Users/liangming.xu/code/UniAD/tools/test.py(229)main()
-> result = model(return_loss=False, rescale=True, **data)
  /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
  /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py(83)forward()
-> return self.forward_test(**kwargs)
  /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py(292)forward_test()
-> result_track = self.simple_test_track(img, l2g_t, l2g_r_mat, img_metas, timestamp)
  /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(753)simple_test_track()
-> frame_res = self._forward_single_frame_inference(
  /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py(659)_forward_single_frame_inference()
-> det_output = self.pts_bbox_head.get_detections(
  /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head.py(171)get_detections()
-> hs, init_reference, inter_references = self.transformer.get_states_and_refs(
  /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py(218)get_states_and_refs()
-> inter_states, inter_references = self.decoder(
  /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
  /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/decoder.py(97)forward()
-> output = layer(
  /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
  /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/cnn/bricks/transformer.py(492)forward()
-> query = self.attentions[attn_index](
  /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/torch/nn/modules/module.py(1051)_call_impl()
-> return forward_call(*input, **kwargs)
  /opt/homebrew/anaconda3/envs/uniad/lib/python3.8/site-packages/mmcv/utils/misc.py(340)new_func()
-> output = old_func(*args, **kwargs)
> /Users/liangming.xu/code/UniAD/projects/mmdet3d_plugin/uniad/modules/decoder.py(279)forward()
-> if value is None:
(Pdb) 285  	            query = query + query_pos
286  	        if not self.batch_first:
287  	            # change to (bs, num_query ,embed_dims)
288  	            query = query.permute(1, 0, 2)
289  	            value = value.permute(1, 0, 2)
290  	
291  	        bs, num_query, _ = query.shape
292  	        bs, num_value, _ = value.shape
293  	        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_value
294  	
295  	        value = self.value_proj(value)
(Pdb) 